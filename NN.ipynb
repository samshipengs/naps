{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time \n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "from functools import partial\n",
    "\n",
    "from utils import ignore_warnings, load_data\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.5 s, sys: 1.37 s, total: 19.9 s\n",
      "Wall time: 19.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# nrows = 10000\n",
    "nrows = None\n",
    "train = load_data('train', nrows=nrows)#, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the rows that is clickout\n",
    "is_clickout = train.action_type == 'clickout item'\n",
    "# # and it is not nan\n",
    "# not_na = train.re.notna()\n",
    "# and the impressions are not nans\n",
    "imp_not_na = train.impressions.notna()\n",
    "# only select the ones with 25 lens \n",
    "train['nimp'] = train.impressions.str.split('|').str.len()\n",
    "twenty_five = train['nimp'] == 25\n",
    "\n",
    "select_mask = is_clickout & imp_not_na & twenty_five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[select_mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['current_filters'].isna(), 'current_filters'] = 'no_filter'\n",
    "train.loc[train['reference'].isna(), 'reference'] = 'no_reference'\n",
    "\n",
    "train['cfs'] = train['current_filters'].str.split('|')\n",
    "train['imps'] = train['impressions'].str.split('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['session_id', 'timestamp', 'reference', 'imps']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1232016, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: (1232016, 4)\n",
      "after: (1232011, 4)\n",
      "before: (1232011, 5)\n",
      "after: (1231380, 5)\n"
     ]
    }
   ],
   "source": [
    "all_imps = train.imps.values \n",
    "all_imps = list(set([j for i in all_imps for j in i]))\n",
    "# we only embed the ones appeared in the impression list, otherwise uncomment below\n",
    "# all_imps = list(set(all_imps + list(train['reference'].unique())))\n",
    "imp2natural = {v: k for k, v in enumerate(all_imps)}\n",
    "# only select 25 length impressions \n",
    "train['reference'] = train.reference.map(imp2natural)\n",
    "# drop the one reference does not have a mapping\n",
    "print('before:', train.shape)\n",
    "train = train[train.reference.notna()].reset_index(drop=True)\n",
    "print('after:', train.shape)\n",
    "train = train[train.imps.str.len()==25].reset_index(drop=True)\n",
    "train['imps'] = train.imps.apply(lambda x: [imp2natural[i] for i in x])\n",
    "def assign_target(row):\n",
    "    ref = row.reference\n",
    "    imp = row.imps\n",
    "    if ref in imp:\n",
    "        return imp.index(ref)\n",
    "    else:\n",
    "        return 25\n",
    "#         return -1\n",
    "train['target'] = train.apply(assign_target, axis=1)\n",
    "# remove the target 25 (i.e. not appearing in the list)\n",
    "print('before:', train.shape)\n",
    "train = train[train.target != 25].reset_index(drop=True)\n",
    "print('after:', train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "743076"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imp2natural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['session_id', 'imps', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and val\n",
    "unique_sids = train['session_id'].unique()\n",
    "train_sids = unique_sids[:-int(len(unique_sids)*0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_mask = train.session_id.isin(train_sids)\n",
    "xtrain = train.loc[trn_mask, ['imps', 'target']].reset_index(drop=True)\n",
    "xval = train.loc[~trn_mask, ['imps', 'target']].reset_index(drop=True)\n",
    "del train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1108640, 2) (122740, 2)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape, xval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = xtrain['target'].values\n",
    "yval = xval['target'].values\n",
    "del xtrain['target'], xval['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.layers import Activation, concatenate, Dense, Dropout, Embedding, Input, Reshape, Flatten, Conv1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "n_item_ids = len(imp2natural)\n",
    "n_embed = 30\n",
    "input_layer = Input(shape = (25, ), dtype = \"int32\")\n",
    "impression_embedding = Embedding(n_item_ids, n_embed, input_length=25)(input_layer)\n",
    "impression_embedding = Dropout(0.5)(impression_embedding)\n",
    "# h1 = Conv1D(16, 3)(impression_embedding)\n",
    "h0 = Flatten()(impression_embedding)\n",
    "h1 = Dense(units=10, activation='relu')(h0)\n",
    "h1 = Dropout(0.4)(h1)\n",
    "# h1 = Flatten()(h1)\n",
    "output_layer = Dense(25, activation='softmax')(h1)\n",
    "\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "# opt = optimizers.SGD(lr = 0.001, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "opt = optimizers.Adam(lr=0.002)\n",
    "model.compile(optimizer = opt, loss = \"categorical_crossentropy\", metrics=['accuracy'])\n",
    "# model.compile(optimizer = opt, loss = \"sparse_categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 25, 30)            22292280  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 25, 30)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                7510      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 25)                275       \n",
      "=================================================================\n",
      "Total params: 22,300,065\n",
      "Trainable params: 22,300,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from clr import CyclicLR\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from datetime import datetime as dt\n",
    "model_file = 'test.model'\n",
    "\n",
    "callbacks = [ModelCheckpoint(model_file, save_best_only=True, verbose=1)]\n",
    "# callbacks.append(EarlyStopping(patience=150, verbose=1))\n",
    "# callbacks.append(ReduceLROnPlateau(factor=0.5, patience=20, min_lr=5e-4, verbose=1))\n",
    "log_dir = \"logs/{}\".format(dt.now().strftime('%m-%d-%H-%M'))\n",
    "# tb = TensorBoard(log_dir=log_dir, histogram_freq=2, write_graph=True, write_grads=True, write_images=True,\n",
    "#                  embeddings_freq=10, embeddings_layer_names=['embedding_1'], embeddings_data=next(val_gen))\n",
    "tb = TensorBoard(log_dir=log_dir, write_graph=True, write_grads=True)\n",
    "callbacks.append(tb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1108640 samples, validate on 122740 samples\n",
      "Epoch 1/3000\n",
      " - 141s - loss: 2.5620 - acc: 0.3254 - val_loss: 2.5312 - val_acc: 0.3261\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.53123, saving model to test.model\n",
      "Epoch 2/3000\n",
      " - 138s - loss: 2.4482 - acc: 0.3276 - val_loss: 2.5390 - val_acc: 0.3261\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.53123\n",
      "Epoch 3/3000\n",
      " - 134s - loss: 2.3664 - acc: 0.3309 - val_loss: 2.5675 - val_acc: 0.3258\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.53123\n",
      "Epoch 4/3000\n",
      " - 133s - loss: 2.3115 - acc: 0.3358 - val_loss: 2.5928 - val_acc: 0.3253\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.53123\n",
      "Epoch 5/3000\n",
      " - 133s - loss: 2.2740 - acc: 0.3407 - val_loss: 2.6232 - val_acc: 0.3250\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.53123\n",
      "Epoch 6/3000\n",
      " - 133s - loss: 2.2462 - acc: 0.3448 - val_loss: 2.6389 - val_acc: 0.3233\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.53123\n",
      "Epoch 7/3000\n",
      " - 134s - loss: 2.2251 - acc: 0.3486 - val_loss: 2.6495 - val_acc: 0.3229\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.53123\n",
      "Epoch 8/3000\n",
      " - 133s - loss: 2.2080 - acc: 0.3514 - val_loss: 2.6544 - val_acc: 0.3228\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.53123\n",
      "Epoch 9/3000\n",
      " - 133s - loss: 2.1943 - acc: 0.3543 - val_loss: 2.6700 - val_acc: 0.3230\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.53123\n",
      "Epoch 10/3000\n",
      " - 133s - loss: 2.1817 - acc: 0.3564 - val_loss: 2.6820 - val_acc: 0.3227\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.53123\n",
      "Epoch 11/3000\n",
      " - 133s - loss: 2.1714 - acc: 0.3586 - val_loss: 2.6883 - val_acc: 0.3207\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.53123\n",
      "Epoch 12/3000\n",
      " - 133s - loss: 2.1620 - acc: 0.3609 - val_loss: 2.6874 - val_acc: 0.3200\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.53123\n",
      "Epoch 13/3000\n",
      " - 133s - loss: 2.1555 - acc: 0.3622 - val_loss: 2.6921 - val_acc: 0.3202\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.53123\n",
      "Epoch 14/3000\n",
      " - 133s - loss: 2.1489 - acc: 0.3638 - val_loss: 2.7006 - val_acc: 0.3183\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.53123\n",
      "Epoch 15/3000\n",
      " - 133s - loss: 2.1429 - acc: 0.3648 - val_loss: 2.7086 - val_acc: 0.3169\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.53123\n",
      "Epoch 16/3000\n",
      " - 133s - loss: 2.1379 - acc: 0.3659 - val_loss: 2.7011 - val_acc: 0.3185\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2.53123\n",
      "Epoch 17/3000\n",
      " - 133s - loss: 2.1329 - acc: 0.3671 - val_loss: 2.7068 - val_acc: 0.3198\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.53123\n",
      "Epoch 18/3000\n",
      " - 133s - loss: 2.1282 - acc: 0.3680 - val_loss: 2.7139 - val_acc: 0.3190\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.53123\n",
      "Epoch 19/3000\n",
      " - 133s - loss: 2.1236 - acc: 0.3691 - val_loss: 2.7130 - val_acc: 0.3168\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.53123\n",
      "Epoch 20/3000\n",
      " - 133s - loss: 2.1213 - acc: 0.3693 - val_loss: 2.7215 - val_acc: 0.3175\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.53123\n",
      "Epoch 21/3000\n",
      " - 133s - loss: 2.1177 - acc: 0.3706 - val_loss: 2.7226 - val_acc: 0.3203\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.53123\n",
      "Epoch 22/3000\n",
      " - 133s - loss: 2.1147 - acc: 0.3707 - val_loss: 2.7332 - val_acc: 0.3192\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.53123\n",
      "Epoch 23/3000\n",
      " - 133s - loss: 2.1112 - acc: 0.3717 - val_loss: 2.7212 - val_acc: 0.3186\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.53123\n",
      "Epoch 24/3000\n",
      " - 133s - loss: 2.1092 - acc: 0.3722 - val_loss: 2.7278 - val_acc: 0.3168\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.53123\n",
      "Epoch 25/3000\n",
      " - 133s - loss: 2.1057 - acc: 0.3726 - val_loss: 2.7536 - val_acc: 0.3174\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2.53123\n",
      "Epoch 26/3000\n",
      " - 133s - loss: 2.1043 - acc: 0.3727 - val_loss: 2.7312 - val_acc: 0.3146\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.53123\n",
      "Epoch 27/3000\n",
      " - 133s - loss: 2.1017 - acc: 0.3733 - val_loss: 2.7419 - val_acc: 0.3186\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.53123\n",
      "Epoch 28/3000\n",
      " - 133s - loss: 2.0987 - acc: 0.3742 - val_loss: 2.7273 - val_acc: 0.3167\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.53123\n",
      "Epoch 29/3000\n",
      " - 133s - loss: 2.0979 - acc: 0.3745 - val_loss: 2.7400 - val_acc: 0.3176\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.53123\n",
      "Epoch 30/3000\n",
      " - 134s - loss: 2.0952 - acc: 0.3741 - val_loss: 2.7363 - val_acc: 0.3155\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.53123\n",
      "Epoch 31/3000\n",
      " - 133s - loss: 2.0932 - acc: 0.3745 - val_loss: 2.7335 - val_acc: 0.3174\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 2.53123\n",
      "Epoch 32/3000\n",
      " - 133s - loss: 2.0924 - acc: 0.3749 - val_loss: 2.7583 - val_acc: 0.3175\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 2.53123\n",
      "Epoch 33/3000\n",
      " - 133s - loss: 2.0907 - acc: 0.3753 - val_loss: 2.7502 - val_acc: 0.3169\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 2.53123\n",
      "Epoch 34/3000\n",
      " - 133s - loss: 2.0886 - acc: 0.3755 - val_loss: 2.7595 - val_acc: 0.3162\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 2.53123\n",
      "Epoch 35/3000\n",
      " - 133s - loss: 2.0865 - acc: 0.3755 - val_loss: 2.7583 - val_acc: 0.3164\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 2.53123\n",
      "Epoch 36/3000\n",
      " - 133s - loss: 2.0858 - acc: 0.3759 - val_loss: 2.7599 - val_acc: 0.3169\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 2.53123\n",
      "Epoch 37/3000\n",
      " - 133s - loss: 2.0826 - acc: 0.3761 - val_loss: 2.7464 - val_acc: 0.3176\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 2.53123\n",
      "Epoch 38/3000\n",
      " - 133s - loss: 2.0829 - acc: 0.3760 - val_loss: 2.7541 - val_acc: 0.3174\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 2.53123\n",
      "Epoch 39/3000\n",
      " - 133s - loss: 2.0805 - acc: 0.3766 - val_loss: 2.7576 - val_acc: 0.3175\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 2.53123\n",
      "Epoch 40/3000\n",
      " - 133s - loss: 2.0793 - acc: 0.3766 - val_loss: 2.7532 - val_acc: 0.3165\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 2.53123\n",
      "Epoch 41/3000\n",
      " - 133s - loss: 2.0793 - acc: 0.3765 - val_loss: 2.7585 - val_acc: 0.3188\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 2.53123\n",
      "Epoch 42/3000\n",
      " - 133s - loss: 2.0763 - acc: 0.3766 - val_loss: 2.7577 - val_acc: 0.3171\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 2.53123\n",
      "Epoch 43/3000\n",
      " - 133s - loss: 2.0761 - acc: 0.3769 - val_loss: 2.7540 - val_acc: 0.3175\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 2.53123\n",
      "Epoch 44/3000\n",
      " - 133s - loss: 2.0752 - acc: 0.3770 - val_loss: 2.7558 - val_acc: 0.3191\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 2.53123\n",
      "Epoch 45/3000\n",
      " - 133s - loss: 2.0745 - acc: 0.3769 - val_loss: 2.7422 - val_acc: 0.3201\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 2.53123\n",
      "Epoch 46/3000\n",
      " - 133s - loss: 2.0724 - acc: 0.3774 - val_loss: 2.7563 - val_acc: 0.3188\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 2.53123\n",
      "Epoch 47/3000\n",
      " - 133s - loss: 2.0726 - acc: 0.3773 - val_loss: 2.7561 - val_acc: 0.3165\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 2.53123\n",
      "Epoch 48/3000\n",
      " - 133s - loss: 2.0722 - acc: 0.3772 - val_loss: 2.7522 - val_acc: 0.3185\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 2.53123\n",
      "Epoch 49/3000\n",
      " - 133s - loss: 2.0710 - acc: 0.3773 - val_loss: 2.7675 - val_acc: 0.3189\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 2.53123\n",
      "Epoch 50/3000\n",
      " - 133s - loss: 2.0699 - acc: 0.3775 - val_loss: 2.7763 - val_acc: 0.3177\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 2.53123\n",
      "Epoch 51/3000\n",
      " - 133s - loss: 2.0679 - acc: 0.3782 - val_loss: 2.7628 - val_acc: 0.3155\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 2.53123\n",
      "Epoch 52/3000\n",
      " - 133s - loss: 2.0689 - acc: 0.3775 - val_loss: 2.7524 - val_acc: 0.3207\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 2.53123\n",
      "Epoch 53/3000\n",
      " - 133s - loss: 2.0679 - acc: 0.3777 - val_loss: 2.7556 - val_acc: 0.3196\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 2.53123\n",
      "Epoch 54/3000\n",
      " - 133s - loss: 2.0671 - acc: 0.3779 - val_loss: 2.7621 - val_acc: 0.3195\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 2.53123\n",
      "Epoch 55/3000\n",
      " - 133s - loss: 2.0659 - acc: 0.3779 - val_loss: 2.7617 - val_acc: 0.3190\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 2.53123\n",
      "Epoch 56/3000\n",
      " - 133s - loss: 2.0664 - acc: 0.3779 - val_loss: 2.7782 - val_acc: 0.3210\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 2.53123\n",
      "Epoch 57/3000\n",
      " - 133s - loss: 2.0644 - acc: 0.3783 - val_loss: 2.7599 - val_acc: 0.3186\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 2.53123\n",
      "Epoch 58/3000\n",
      " - 133s - loss: 2.0643 - acc: 0.3779 - val_loss: 2.7578 - val_acc: 0.3151\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 2.53123\n",
      "Epoch 59/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 133s - loss: 2.0635 - acc: 0.3781 - val_loss: 2.7756 - val_acc: 0.3145\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 2.53123\n",
      "Epoch 60/3000\n",
      " - 133s - loss: 2.0639 - acc: 0.3784 - val_loss: 2.7631 - val_acc: 0.3173\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 2.53123\n",
      "Epoch 61/3000\n",
      " - 133s - loss: 2.0639 - acc: 0.3782 - val_loss: 2.7627 - val_acc: 0.3177\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 2.53123\n",
      "Epoch 62/3000\n",
      " - 133s - loss: 2.0632 - acc: 0.3790 - val_loss: 2.7555 - val_acc: 0.3172\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 2.53123\n",
      "Epoch 63/3000\n",
      " - 133s - loss: 2.0623 - acc: 0.3782 - val_loss: 2.7640 - val_acc: 0.3163\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 2.53123\n",
      "Epoch 64/3000\n",
      " - 133s - loss: 2.0624 - acc: 0.3788 - val_loss: 2.7612 - val_acc: 0.3176\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 2.53123\n",
      "Epoch 65/3000\n",
      " - 133s - loss: 2.0617 - acc: 0.3789 - val_loss: 2.7512 - val_acc: 0.3186\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 2.53123\n",
      "Epoch 66/3000\n",
      " - 133s - loss: 2.0606 - acc: 0.3788 - val_loss: 2.7609 - val_acc: 0.3166\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 2.53123\n",
      "Epoch 67/3000\n",
      " - 133s - loss: 2.0603 - acc: 0.3790 - val_loss: 2.7577 - val_acc: 0.3173\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 2.53123\n",
      "Epoch 68/3000\n",
      " - 133s - loss: 2.0601 - acc: 0.3786 - val_loss: 2.7615 - val_acc: 0.3175\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 2.53123\n",
      "Epoch 69/3000\n",
      " - 133s - loss: 2.0598 - acc: 0.3788 - val_loss: 2.7484 - val_acc: 0.3178\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 2.53123\n",
      "Epoch 70/3000\n",
      " - 133s - loss: 2.0601 - acc: 0.3790 - val_loss: 2.7867 - val_acc: 0.3162\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 2.53123\n",
      "Epoch 71/3000\n",
      " - 133s - loss: 2.0595 - acc: 0.3791 - val_loss: 2.7854 - val_acc: 0.3188\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 2.53123\n",
      "Epoch 72/3000\n",
      " - 133s - loss: 2.0601 - acc: 0.3792 - val_loss: 2.7532 - val_acc: 0.3185\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 2.53123\n",
      "Epoch 73/3000\n",
      " - 133s - loss: 2.0588 - acc: 0.3797 - val_loss: 2.7703 - val_acc: 0.3174\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 2.53123\n",
      "Epoch 74/3000\n",
      " - 133s - loss: 2.0592 - acc: 0.3794 - val_loss: 2.7674 - val_acc: 0.3201\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 2.53123\n",
      "Epoch 75/3000\n",
      " - 133s - loss: 2.0575 - acc: 0.3798 - val_loss: 2.7692 - val_acc: 0.3188\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 2.53123\n",
      "Epoch 76/3000\n",
      " - 133s - loss: 2.0582 - acc: 0.3794 - val_loss: 2.7630 - val_acc: 0.3173\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 2.53123\n",
      "Epoch 77/3000\n",
      " - 133s - loss: 2.0579 - acc: 0.3799 - val_loss: 2.7512 - val_acc: 0.3198\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 2.53123\n",
      "Epoch 78/3000\n",
      " - 133s - loss: 2.0569 - acc: 0.3798 - val_loss: 2.7536 - val_acc: 0.3192\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 2.53123\n",
      "Epoch 79/3000\n",
      " - 133s - loss: 2.0573 - acc: 0.3796 - val_loss: 2.7721 - val_acc: 0.3177\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 2.53123\n",
      "Epoch 80/3000\n",
      " - 133s - loss: 2.0569 - acc: 0.3800 - val_loss: 2.7660 - val_acc: 0.3165\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 2.53123\n",
      "Epoch 81/3000\n",
      " - 133s - loss: 2.0566 - acc: 0.3801 - val_loss: 2.7683 - val_acc: 0.3169\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 2.53123\n",
      "Epoch 82/3000\n",
      " - 133s - loss: 2.0561 - acc: 0.3801 - val_loss: 2.7682 - val_acc: 0.3212\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 2.53123\n",
      "Epoch 83/3000\n",
      " - 133s - loss: 2.0578 - acc: 0.3797 - val_loss: 2.7536 - val_acc: 0.3205\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 2.53123\n",
      "Epoch 84/3000\n",
      " - 133s - loss: 2.0560 - acc: 0.3800 - val_loss: 2.7821 - val_acc: 0.3142\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 2.53123\n",
      "Epoch 85/3000\n",
      " - 133s - loss: 2.0566 - acc: 0.3801 - val_loss: 2.7603 - val_acc: 0.3189\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 2.53123\n",
      "Epoch 86/3000\n",
      " - 133s - loss: 2.0556 - acc: 0.3804 - val_loss: 2.7816 - val_acc: 0.3193\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 2.53123\n",
      "Epoch 87/3000\n",
      " - 133s - loss: 2.0556 - acc: 0.3801 - val_loss: 2.7897 - val_acc: 0.3192\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 2.53123\n",
      "Epoch 88/3000\n",
      " - 133s - loss: 2.0551 - acc: 0.3804 - val_loss: 2.7685 - val_acc: 0.3192\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 2.53123\n",
      "Epoch 89/3000\n",
      " - 133s - loss: 2.0565 - acc: 0.3800 - val_loss: 2.7575 - val_acc: 0.3182\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 2.53123\n",
      "Epoch 90/3000\n",
      " - 133s - loss: 2.0552 - acc: 0.3806 - val_loss: 2.7676 - val_acc: 0.3155\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 2.53123\n",
      "Epoch 91/3000\n",
      " - 133s - loss: 2.0558 - acc: 0.3803 - val_loss: 2.7694 - val_acc: 0.3209\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 2.53123\n",
      "Epoch 92/3000\n",
      " - 133s - loss: 2.0549 - acc: 0.3808 - val_loss: 2.7543 - val_acc: 0.3199\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 2.53123\n",
      "Epoch 93/3000\n",
      " - 133s - loss: 2.0539 - acc: 0.3808 - val_loss: 2.7659 - val_acc: 0.3170\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 2.53123\n",
      "Epoch 94/3000\n",
      " - 133s - loss: 2.0548 - acc: 0.3805 - val_loss: 2.7681 - val_acc: 0.3166\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 2.53123\n",
      "Epoch 95/3000\n",
      " - 133s - loss: 2.0542 - acc: 0.3801 - val_loss: 2.7796 - val_acc: 0.3171\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 2.53123\n",
      "Epoch 96/3000\n",
      " - 133s - loss: 2.0547 - acc: 0.3805 - val_loss: 2.7552 - val_acc: 0.3168\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 2.53123\n",
      "Epoch 97/3000\n",
      " - 133s - loss: 2.0540 - acc: 0.3810 - val_loss: 2.7507 - val_acc: 0.3205\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 2.53123\n",
      "Epoch 98/3000\n",
      " - 133s - loss: 2.0549 - acc: 0.3807 - val_loss: 2.7614 - val_acc: 0.3191\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 2.53123\n",
      "Epoch 99/3000\n",
      " - 133s - loss: 2.0540 - acc: 0.3808 - val_loss: 2.7678 - val_acc: 0.3194\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 2.53123\n",
      "Epoch 100/3000\n",
      " - 133s - loss: 2.0536 - acc: 0.3812 - val_loss: 2.7541 - val_acc: 0.3191\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 2.53123\n",
      "Epoch 101/3000\n",
      " - 133s - loss: 2.0545 - acc: 0.3811 - val_loss: 2.7817 - val_acc: 0.3186\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 2.53123\n",
      "Epoch 102/3000\n",
      " - 133s - loss: 2.0539 - acc: 0.3809 - val_loss: 2.7622 - val_acc: 0.3224\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 2.53123\n",
      "Epoch 103/3000\n",
      " - 133s - loss: 2.0534 - acc: 0.3807 - val_loss: 2.7707 - val_acc: 0.3183\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 2.53123\n",
      "Epoch 104/3000\n",
      " - 133s - loss: 2.0548 - acc: 0.3813 - val_loss: 2.7589 - val_acc: 0.3164\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 2.53123\n",
      "Epoch 105/3000\n",
      " - 133s - loss: 2.0545 - acc: 0.3807 - val_loss: 2.7614 - val_acc: 0.3208\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 2.53123\n",
      "Epoch 106/3000\n",
      " - 133s - loss: 2.0547 - acc: 0.3809 - val_loss: 2.7784 - val_acc: 0.3185\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 2.53123\n",
      "Epoch 107/3000\n",
      " - 133s - loss: 2.0537 - acc: 0.3810 - val_loss: 2.7992 - val_acc: 0.3138\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 2.53123\n",
      "Epoch 108/3000\n",
      " - 133s - loss: 2.0540 - acc: 0.3813 - val_loss: 2.7749 - val_acc: 0.3181\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 2.53123\n",
      "Epoch 109/3000\n",
      " - 133s - loss: 2.0544 - acc: 0.3806 - val_loss: 2.7544 - val_acc: 0.3203\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 2.53123\n",
      "Epoch 110/3000\n",
      " - 133s - loss: 2.0537 - acc: 0.3817 - val_loss: 2.7651 - val_acc: 0.3169\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 2.53123\n",
      "Epoch 111/3000\n",
      " - 133s - loss: 2.0531 - acc: 0.3815 - val_loss: 2.7765 - val_acc: 0.3160\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 2.53123\n",
      "Epoch 112/3000\n",
      " - 133s - loss: 2.0534 - acc: 0.3813 - val_loss: 2.8317 - val_acc: 0.3175\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 2.53123\n",
      "Epoch 113/3000\n",
      " - 133s - loss: 2.0536 - acc: 0.3810 - val_loss: 2.7886 - val_acc: 0.3164\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 2.53123\n",
      "Epoch 114/3000\n",
      " - 133s - loss: 2.0545 - acc: 0.3812 - val_loss: 2.7651 - val_acc: 0.3170\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 2.53123\n",
      "Epoch 115/3000\n",
      " - 133s - loss: 2.0532 - acc: 0.3813 - val_loss: 2.7539 - val_acc: 0.3191\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 2.53123\n",
      "Epoch 116/3000\n",
      " - 133s - loss: 2.0536 - acc: 0.3815 - val_loss: 2.7600 - val_acc: 0.3188\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 2.53123\n",
      "Epoch 117/3000\n",
      " - 133s - loss: 2.0545 - acc: 0.3816 - val_loss: 2.7640 - val_acc: 0.3192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00117: val_loss did not improve from 2.53123\n",
      "Epoch 118/3000\n",
      " - 133s - loss: 2.0526 - acc: 0.3816 - val_loss: 2.7855 - val_acc: 0.3177\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 2.53123\n",
      "Epoch 119/3000\n",
      " - 133s - loss: 2.0532 - acc: 0.3812 - val_loss: 2.7786 - val_acc: 0.3191\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 2.53123\n",
      "Epoch 120/3000\n",
      " - 133s - loss: 2.0527 - acc: 0.3815 - val_loss: 2.7753 - val_acc: 0.3181\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 2.53123\n",
      "Epoch 121/3000\n",
      " - 133s - loss: 2.0522 - acc: 0.3818 - val_loss: 2.7645 - val_acc: 0.3178\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 2.53123\n",
      "Epoch 122/3000\n",
      " - 133s - loss: 2.0530 - acc: 0.3819 - val_loss: 2.7769 - val_acc: 0.3197\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 2.53123\n",
      "Epoch 123/3000\n",
      " - 133s - loss: 2.0532 - acc: 0.3817 - val_loss: 2.7678 - val_acc: 0.3196\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 2.53123\n",
      "Epoch 124/3000\n",
      " - 134s - loss: 2.0535 - acc: 0.3816 - val_loss: 2.7586 - val_acc: 0.3184\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 2.53123\n",
      "Epoch 125/3000\n",
      " - 133s - loss: 2.0527 - acc: 0.3816 - val_loss: 2.7849 - val_acc: 0.3171\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 2.53123\n",
      "Epoch 126/3000\n",
      " - 133s - loss: 2.0531 - acc: 0.3816 - val_loss: 2.7798 - val_acc: 0.3159\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 2.53123\n",
      "Epoch 127/3000\n",
      " - 133s - loss: 2.0537 - acc: 0.3818 - val_loss: 2.7652 - val_acc: 0.3173\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 2.53123\n",
      "Epoch 128/3000\n",
      " - 133s - loss: 2.0522 - acc: 0.3813 - val_loss: 2.7878 - val_acc: 0.3176\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 2.53123\n",
      "Epoch 129/3000\n",
      " - 133s - loss: 2.0526 - acc: 0.3816 - val_loss: 2.7822 - val_acc: 0.3189\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 2.53123\n",
      "Epoch 130/3000\n",
      " - 133s - loss: 2.0541 - acc: 0.3818 - val_loss: 2.7673 - val_acc: 0.3191\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 2.53123\n",
      "Epoch 131/3000\n",
      " - 133s - loss: 2.0530 - acc: 0.3820 - val_loss: 2.7720 - val_acc: 0.3198\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 2.53123\n",
      "Epoch 132/3000\n",
      " - 133s - loss: 2.0532 - acc: 0.3818 - val_loss: 2.7651 - val_acc: 0.3177\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 2.53123\n",
      "Epoch 133/3000\n",
      " - 133s - loss: 2.0518 - acc: 0.3822 - val_loss: 2.7759 - val_acc: 0.3182\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 2.53123\n",
      "Epoch 134/3000\n",
      " - 133s - loss: 2.0533 - acc: 0.3821 - val_loss: 2.7721 - val_acc: 0.3204\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 2.53123\n",
      "Epoch 135/3000\n",
      " - 133s - loss: 2.0542 - acc: 0.3814 - val_loss: 2.7662 - val_acc: 0.3180\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 2.53123\n",
      "Epoch 136/3000\n",
      " - 133s - loss: 2.0546 - acc: 0.3819 - val_loss: 2.7840 - val_acc: 0.3194\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 2.53123\n",
      "Epoch 137/3000\n",
      " - 133s - loss: 2.0534 - acc: 0.3820 - val_loss: 2.7605 - val_acc: 0.3217\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 2.53123\n",
      "Epoch 138/3000\n",
      " - 133s - loss: 2.0531 - acc: 0.3821 - val_loss: 2.7677 - val_acc: 0.3163\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 2.53123\n",
      "Epoch 139/3000\n",
      " - 133s - loss: 2.0528 - acc: 0.3824 - val_loss: 2.7502 - val_acc: 0.3185\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 2.53123\n",
      "Epoch 140/3000\n",
      " - 133s - loss: 2.0520 - acc: 0.3825 - val_loss: 2.7558 - val_acc: 0.3202\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 2.53123\n",
      "Epoch 141/3000\n",
      " - 133s - loss: 2.0529 - acc: 0.3821 - val_loss: 2.7946 - val_acc: 0.3192\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 2.53123\n",
      "Epoch 142/3000\n",
      " - 133s - loss: 2.0527 - acc: 0.3821 - val_loss: 2.7781 - val_acc: 0.3206\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 2.53123\n",
      "Epoch 143/3000\n",
      " - 133s - loss: 2.0530 - acc: 0.3821 - val_loss: 2.7665 - val_acc: 0.3198\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 2.53123\n",
      "Epoch 144/3000\n",
      " - 133s - loss: 2.0528 - acc: 0.3825 - val_loss: 2.7704 - val_acc: 0.3172\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 2.53123\n",
      "Epoch 145/3000\n",
      " - 133s - loss: 2.0527 - acc: 0.3821 - val_loss: 2.7953 - val_acc: 0.3175\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 2.53123\n",
      "Epoch 146/3000\n",
      " - 133s - loss: 2.0540 - acc: 0.3819 - val_loss: 2.7624 - val_acc: 0.3191\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 2.53123\n",
      "Epoch 147/3000\n",
      " - 133s - loss: 2.0529 - acc: 0.3818 - val_loss: 2.7694 - val_acc: 0.3176\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 2.53123\n",
      "Epoch 148/3000\n",
      " - 133s - loss: 2.0526 - acc: 0.3824 - val_loss: 2.7823 - val_acc: 0.3178\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 2.53123\n",
      "Epoch 149/3000\n",
      " - 133s - loss: 2.0530 - acc: 0.3823 - val_loss: 2.7590 - val_acc: 0.3221\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 2.53123\n",
      "Epoch 150/3000\n",
      " - 133s - loss: 2.0530 - acc: 0.3817 - val_loss: 2.7858 - val_acc: 0.3193\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 2.53123\n",
      "Epoch 151/3000\n",
      " - 133s - loss: 2.0547 - acc: 0.3820 - val_loss: 2.7576 - val_acc: 0.3189\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 2.53123\n",
      "Epoch 152/3000\n",
      " - 133s - loss: 2.0534 - acc: 0.3822 - val_loss: 2.7734 - val_acc: 0.3200\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 2.53123\n",
      "Epoch 153/3000\n",
      " - 133s - loss: 2.0538 - acc: 0.3818 - val_loss: 2.7688 - val_acc: 0.3193\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 2.53123\n",
      "Epoch 154/3000\n",
      " - 133s - loss: 2.0530 - acc: 0.3819 - val_loss: 2.7987 - val_acc: 0.3173\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 2.53123\n",
      "Epoch 155/3000\n",
      " - 133s - loss: 2.0525 - acc: 0.3827 - val_loss: 2.7734 - val_acc: 0.3203\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 2.53123\n",
      "Epoch 156/3000\n",
      " - 133s - loss: 2.0543 - acc: 0.3822 - val_loss: 2.7720 - val_acc: 0.3197\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 2.53123\n",
      "Epoch 157/3000\n",
      " - 133s - loss: 2.0534 - acc: 0.3821 - val_loss: 2.7767 - val_acc: 0.3181\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 2.53123\n",
      "Epoch 158/3000\n",
      " - 133s - loss: 2.0535 - acc: 0.3825 - val_loss: 2.8040 - val_acc: 0.3205\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 2.53123\n",
      "Epoch 159/3000\n",
      " - 133s - loss: 2.0538 - acc: 0.3824 - val_loss: 2.7713 - val_acc: 0.3148\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 2.53123\n",
      "Epoch 160/3000\n",
      " - 133s - loss: 2.0539 - acc: 0.3820 - val_loss: 2.7803 - val_acc: 0.3221\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 2.53123\n",
      "Epoch 161/3000\n",
      " - 133s - loss: 2.0537 - acc: 0.3822 - val_loss: 2.7999 - val_acc: 0.3192\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 2.53123\n",
      "Epoch 162/3000\n",
      " - 133s - loss: 2.0549 - acc: 0.3825 - val_loss: 2.7981 - val_acc: 0.3175\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 2.53123\n",
      "Epoch 163/3000\n",
      " - 133s - loss: 2.0529 - acc: 0.3825 - val_loss: 2.7847 - val_acc: 0.3176\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 2.53123\n",
      "Epoch 164/3000\n",
      " - 133s - loss: 2.0528 - acc: 0.3826 - val_loss: 2.7724 - val_acc: 0.3195\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 2.53123\n",
      "Epoch 165/3000\n",
      " - 133s - loss: 2.0528 - acc: 0.3831 - val_loss: 2.7695 - val_acc: 0.3198\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 2.53123\n",
      "Epoch 166/3000\n",
      " - 133s - loss: 2.0534 - acc: 0.3824 - val_loss: 2.7748 - val_acc: 0.3205\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 2.53123\n",
      "Epoch 167/3000\n",
      " - 133s - loss: 2.0548 - acc: 0.3819 - val_loss: 2.7535 - val_acc: 0.3188\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 2.53123\n",
      "Epoch 168/3000\n",
      " - 133s - loss: 2.0556 - acc: 0.3824 - val_loss: 2.7800 - val_acc: 0.3216\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 2.53123\n",
      "Epoch 169/3000\n",
      " - 133s - loss: 2.0537 - acc: 0.3823 - val_loss: 2.7905 - val_acc: 0.3192\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 2.53123\n",
      "Epoch 170/3000\n",
      " - 133s - loss: 2.0527 - acc: 0.3827 - val_loss: 2.7961 - val_acc: 0.3210\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 2.53123\n",
      "Epoch 171/3000\n",
      " - 133s - loss: 2.0530 - acc: 0.3826 - val_loss: 2.7657 - val_acc: 0.3203\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 2.53123\n",
      "Epoch 172/3000\n",
      " - 133s - loss: 2.0536 - acc: 0.3824 - val_loss: 2.7733 - val_acc: 0.3189\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 2.53123\n",
      "Epoch 173/3000\n",
      " - 133s - loss: 2.0532 - acc: 0.3829 - val_loss: 2.7677 - val_acc: 0.3177\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 2.53123\n",
      "Epoch 174/3000\n",
      " - 133s - loss: 2.0542 - acc: 0.3825 - val_loss: 2.7869 - val_acc: 0.3185\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 2.53123\n",
      "Epoch 175/3000\n",
      " - 133s - loss: 2.0530 - acc: 0.3826 - val_loss: 2.7920 - val_acc: 0.3169\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 2.53123\n",
      "Epoch 176/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 133s - loss: 2.0543 - acc: 0.3822 - val_loss: 2.7716 - val_acc: 0.3203\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 2.53123\n",
      "Epoch 177/3000\n",
      " - 133s - loss: 2.0542 - acc: 0.3826 - val_loss: 2.7903 - val_acc: 0.3178\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 2.53123\n",
      "Epoch 178/3000\n",
      " - 133s - loss: 2.0535 - acc: 0.3825 - val_loss: 2.7619 - val_acc: 0.3211\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 2.53123\n",
      "Epoch 179/3000\n",
      " - 133s - loss: 2.0526 - acc: 0.3829 - val_loss: 2.7803 - val_acc: 0.3186\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 2.53123\n",
      "Epoch 180/3000\n",
      " - 133s - loss: 2.0538 - acc: 0.3827 - val_loss: 2.7857 - val_acc: 0.3214\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 2.53123\n",
      "Epoch 181/3000\n",
      " - 133s - loss: 2.0545 - acc: 0.3828 - val_loss: 2.7812 - val_acc: 0.3212\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 2.53123\n",
      "Epoch 182/3000\n",
      " - 133s - loss: 2.0527 - acc: 0.3833 - val_loss: 2.7688 - val_acc: 0.3210\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 2.53123\n",
      "Epoch 183/3000\n",
      " - 133s - loss: 2.0546 - acc: 0.3826 - val_loss: 2.8005 - val_acc: 0.3167\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 2.53123\n",
      "Epoch 184/3000\n",
      " - 133s - loss: 2.0533 - acc: 0.3827 - val_loss: 2.8089 - val_acc: 0.3189\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 2.53123\n",
      "Epoch 185/3000\n",
      " - 133s - loss: 2.0536 - acc: 0.3828 - val_loss: 2.7815 - val_acc: 0.3173\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 2.53123\n",
      "Epoch 186/3000\n",
      " - 133s - loss: 2.0552 - acc: 0.3827 - val_loss: 2.8157 - val_acc: 0.3202\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 2.53123\n",
      "Epoch 187/3000\n",
      " - 133s - loss: 2.0534 - acc: 0.3829 - val_loss: 2.7657 - val_acc: 0.3182\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 2.53123\n",
      "Epoch 188/3000\n",
      " - 133s - loss: 2.0539 - acc: 0.3833 - val_loss: 2.7720 - val_acc: 0.3207\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 2.53123\n",
      "Epoch 189/3000\n",
      " - 133s - loss: 2.0550 - acc: 0.3829 - val_loss: 2.7720 - val_acc: 0.3206\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 2.53123\n",
      "Epoch 190/3000\n",
      " - 133s - loss: 2.0552 - acc: 0.3828 - val_loss: 2.7770 - val_acc: 0.3161\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 2.53123\n",
      "Epoch 191/3000\n",
      " - 133s - loss: 2.0544 - acc: 0.3826 - val_loss: 2.7811 - val_acc: 0.3178\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 2.53123\n",
      "Epoch 192/3000\n",
      " - 133s - loss: 2.0544 - acc: 0.3826 - val_loss: 2.7853 - val_acc: 0.3203\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 2.53123\n",
      "Epoch 193/3000\n",
      " - 133s - loss: 2.0554 - acc: 0.3828 - val_loss: 2.7859 - val_acc: 0.3189\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 2.53123\n",
      "Epoch 194/3000\n",
      " - 133s - loss: 2.0545 - acc: 0.3831 - val_loss: 2.7665 - val_acc: 0.3207\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 2.53123\n",
      "Epoch 195/3000\n",
      " - 133s - loss: 2.0556 - acc: 0.3828 - val_loss: 2.7877 - val_acc: 0.3183\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 2.53123\n",
      "Epoch 196/3000\n",
      " - 133s - loss: 2.0544 - acc: 0.3833 - val_loss: 2.7953 - val_acc: 0.3168\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 2.53123\n",
      "Epoch 197/3000\n",
      " - 133s - loss: 2.0550 - acc: 0.3825 - val_loss: 2.7735 - val_acc: 0.3156\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 2.53123\n",
      "Epoch 198/3000\n",
      " - 133s - loss: 2.0558 - acc: 0.3826 - val_loss: 2.7896 - val_acc: 0.3192\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 2.53123\n",
      "Epoch 199/3000\n",
      " - 133s - loss: 2.0552 - acc: 0.3831 - val_loss: 2.7792 - val_acc: 0.3186\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 2.53123\n",
      "Epoch 200/3000\n",
      " - 133s - loss: 2.0558 - acc: 0.3831 - val_loss: 2.7541 - val_acc: 0.3209\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 2.53123\n",
      "Epoch 201/3000\n",
      " - 133s - loss: 2.0552 - acc: 0.3829 - val_loss: 2.7903 - val_acc: 0.3190\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 2.53123\n",
      "Epoch 202/3000\n",
      " - 133s - loss: 2.0553 - acc: 0.3828 - val_loss: 2.8075 - val_acc: 0.3188\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 2.53123\n",
      "Epoch 203/3000\n",
      " - 133s - loss: 2.0549 - acc: 0.3829 - val_loss: 2.7890 - val_acc: 0.3226\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 2.53123\n",
      "Epoch 204/3000\n",
      " - 133s - loss: 2.0552 - acc: 0.3831 - val_loss: 2.8170 - val_acc: 0.3183\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 2.53123\n",
      "Epoch 205/3000\n",
      " - 133s - loss: 2.0561 - acc: 0.3830 - val_loss: 2.7879 - val_acc: 0.3175\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 2.53123\n",
      "Epoch 206/3000\n",
      " - 133s - loss: 2.0544 - acc: 0.3832 - val_loss: 2.7804 - val_acc: 0.3207\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 2.53123\n",
      "Epoch 207/3000\n",
      " - 133s - loss: 2.0553 - acc: 0.3832 - val_loss: 2.7644 - val_acc: 0.3196\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 2.53123\n",
      "Epoch 208/3000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "n_epochs = 3000\n",
    "# keras requires 0, 1 binary label input\n",
    "from keras.utils import to_categorical\n",
    "train_y_binary = to_categorical(ytrain)\n",
    "val_y_binary = to_categorical(yval)\n",
    "\n",
    "history = model.fit(np.array(xtrain.imps.tolist()), \n",
    "                    train_y_binary, \n",
    "                    epochs=n_epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(np.array(xval.imps.tolist()), val_y_binary),\n",
    "                    verbose = 2, \n",
    "                    shuffle = True,\n",
    "                   callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
