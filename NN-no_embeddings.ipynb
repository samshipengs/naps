{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime, time, os, gc, re, sys\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import ignore_warnings, load_data\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[05-05 11:22:11 - utils - load_data - INFO] Loading train using 1,000,000 rows which is 0.06% out of total train data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw train shape: (1,000,000, 12)\n",
      "train after filtering shape: (99,872, 12)\n"
     ]
    }
   ],
   "source": [
    "train = load_data('train', nrows=1000000)\n",
    "def fprint(df, name):\n",
    "    print(f'{name} shape: ({df.shape[0]:,}, {df.shape[1]})')\n",
    "    \n",
    "fprint(train, 'raw train')\n",
    "train = (train[(train['action_type'] == 'clickout item') & \n",
    "               (train['impressions'].notna()) & \n",
    "               (train['reference'].notna())]\n",
    "         .reset_index(drop=True))\n",
    "\n",
    "fprint(train, 'train after filtering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # also get test\n",
    "# test = load_data('test')#, nrows=1000000)\n",
    "# fprint(test, 'raw test')\n",
    "# test = test[(test.action_type == 'clickout item') & (test.impressions.notna()) & (test.reference.notna())].reset_index(drop=True)\n",
    "# fprint(test, 'test after')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.concat([train, test], ignore_index=True)\n",
    "# del test\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping duplicates shape: (99,382, 12)\n"
     ]
    }
   ],
   "source": [
    "# drop duplciates of rows with all same info except step\n",
    "cols = [c for c in train.columns if c != 'step']\n",
    "train = train.drop_duplicates(subset=cols, keep='last').reset_index(drop=True)\n",
    "fprint(train, 'after dropping duplicates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.7 ms, sys: 0 ns, total: 43.7 ms\n",
      "Wall time: 43.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def session_duration(x):\n",
    "    return x.max() - x.min()\n",
    "# session_fts = train.groupby('session_id').agg({'session_id': 'size', 'timestamp': session_duration})\n",
    "session_fts = train.groupby('session_id').size().reset_index(name='session_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from clean_session import preprocess_sessions\n",
    "# # train = preprocess_sessions(train,data_source='data')\n",
    "# train = preprocess_sessions(None,data_source='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# train_last = train.groupby('session_id').last().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_last = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (99,382, 13)\n"
     ]
    }
   ],
   "source": [
    "train = pd.merge(train, session_fts, on='session_id')\n",
    "fprint(train, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fprint(train_last, 'train_last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "London, United Kingdom    0.019400\n",
       "Paris, France             0.033316\n",
       "Tokyo, Japan              0.046065\n",
       "New York, USA             0.057365\n",
       "Istanbul, Turkey          0.067467\n",
       "Name: city, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get only common cities\n",
    "city_counts = train['city'].value_counts()\n",
    "city_counts_cs = city_counts.cumsum()/(city_counts.sum())\n",
    "city_counts_cs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (89,442, 13)\n"
     ]
    }
   ],
   "source": [
    "# set the threshold we select cities\n",
    "th = 0.9\n",
    "above_th = city_counts_cs[city_counts_cs<th]\n",
    "common_cities = above_th.index.values\n",
    "train = train[train['city'].isin(common_cities)].reset_index(drop=True)\n",
    "fprint(train, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# create a label see if it contains any sortings\n",
    "train['no_reorder'] = train['current_filters'].str.contains(r'\\b(sort|focus)', case=False)\n",
    "train['no_reorder'].fillna(False, inplace=True)\n",
    "# train['current_filters'].dropna()[train['current_filters'].dropna().str.contains(r'\\bsort', case=False)].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current filters\n",
    "train['cfs'] = train['current_filters'].str.lower().str.split('|')\n",
    "train['ncfs'] = train['cfs'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add country infomation\n",
    "train['country'] = train.city.str.split(',').str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add prices split now after shrink (otherwise prices being list cannot get shrinked)\n",
    "# prices\n",
    "train['prices'] = train.prices.str.split('|')\n",
    "train['prices'] = train.prices.apply(lambda x: [int(p) for p in x])\n",
    "# pad it\n",
    "train['prices'] = train.prices.apply(lambda x: np.pad(x, (0, 25-len(x)), mode='constant'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of impressions\n",
    "train['nimps'] = train.impressions.str.split('|').str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.59 s, sys: 72 ms, total: 1.66 s\n",
      "Wall time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# impressions\n",
    "train['impressions'] = train['impressions'].str.split('|')\n",
    "# convert impression id to int\n",
    "train['impressions'] = train['impressions'].apply(lambda x: [int(i) for i in x])\n",
    "train['reference'] = train['reference'].astype(int)\n",
    "# pad to 25 len\n",
    "train['impressions'] = train['impressions'].apply(lambda x: np.pad(x, (0, 25-len(x)), mode='constant'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257084"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_item_ids = len(set(np.concatenate(train['impressions'].values)))\n",
    "n_item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (89,442, 19)\n"
     ]
    }
   ],
   "source": [
    "# filter out nan rows with reference_id not in impressions list, since if the true target in test\n",
    "# is not in the impression list then it would not get evaluated\n",
    "def assign_target(row):\n",
    "    ref = row['reference']\n",
    "    imp = list(row['impressions'])\n",
    "    if ref in imp:\n",
    "        return imp.index(ref)\n",
    "    else:\n",
    "        return np.nan\n",
    "train['target'] = train.apply(assign_target, axis=1)\n",
    "fprint(train, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop number of rows reference id not in impressions list: 50\n",
      "train shape: (89,392, 19)\n"
     ]
    }
   ],
   "source": [
    "print(f'drop number of rows reference id not in impressions list: {train.target.isna().sum()}')\n",
    "# drop the ones whose reference is not in the impression list\n",
    "train = train[train['target'].notna()].reset_index(drop=True)\n",
    "train['target'] = train['target'].astype(int)\n",
    "fprint(train, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    27499\n",
       "1     9486\n",
       "2     6713\n",
       "3     5319\n",
       "4     4685\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the target distribution\n",
    "pd.value_counts(train['target']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = load_data('item_metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>properties</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5101</td>\n",
       "      <td>Satellite TV|Golf Course|Airport Shuttle|Cosme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5416</td>\n",
       "      <td>Satellite TV|Cosmetic Mirror|Safe (Hotel)|Tele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5834</td>\n",
       "      <td>Satellite TV|Cosmetic Mirror|Safe (Hotel)|Tele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5910</td>\n",
       "      <td>Satellite TV|Sailing|Cosmetic Mirror|Telephone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6066</td>\n",
       "      <td>Satellite TV|Sailing|Diving|Cosmetic Mirror|Sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id                                         properties\n",
       "0     5101  Satellite TV|Golf Course|Airport Shuttle|Cosme...\n",
       "1     5416  Satellite TV|Cosmetic Mirror|Safe (Hotel)|Tele...\n",
       "2     5834  Satellite TV|Cosmetic Mirror|Safe (Hotel)|Tele...\n",
       "3     5910  Satellite TV|Sailing|Cosmetic Mirror|Telephone...\n",
       "4     6066  Satellite TV|Sailing|Diving|Cosmetic Mirror|Sa..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['properties'] = meta_df['properties'].str.lower().str.split('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_properties = np.concatenate(meta_df['properties'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = pd.value_counts(all_properties).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hostal (es)     3282\n",
       "camping site    2526\n",
       "szep kartya     1533\n",
       "kosher food     1248\n",
       "water slide      349\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(all_properties).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_properties = list(set(all_properties))\n",
    "property2natural = {v: k for k, v in enumerate(unique_properties)}\n",
    "del all_properties\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_properties = len(unique_properties)\n",
    "n_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['properties'] = meta_df['properties'].apply(lambda ps: [property2natural[p] for p in ps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['properties'] = meta_df['properties'].apply(lambda ps: np.sum(np.eye(n_properties, dtype=int)[ps], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_mapping = dict(meta_df[['item_id', 'properties']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a mapping for the padded values\n",
    "# meta_mapping[0] = np.zeros(n_properties, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del meta_df, unique_properties, property2natural\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.51 s, sys: 360 ms, total: 4.87 s\n",
      "Wall time: 4.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train['impressions'] = (train['impressions']\n",
    "                        .apply(lambda imps: np.vstack([meta_mapping[i] \n",
    "                                                      if i in meta_mapping.keys()\n",
    "                                                      else np.zeros(n_properties, dtype=int) \n",
    "                                                      for i in imps])))\n",
    "del meta_mapping\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sort by price        6613\n",
       "focus on distance    4473\n",
       "hotel                3779\n",
       "5 star               2995\n",
       "4 star               2859\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# current_filters\n",
    "all_cfs = np.concatenate(train['cfs'].dropna().values)\n",
    "pd.value_counts(all_cfs).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sort by price        0.141822\n",
       "focus on distance    0.237749\n",
       "hotel                0.318793\n",
       "5 star               0.383023\n",
       "4 star               0.444337\n",
       "dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(all_cfs, normalize=True).cumsum().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_cfs = list(set(all_cfs))\n",
    "cfs_mapping = {v: k for k, v in enumerate(unique_cfs)}\n",
    "n_cfs = len(unique_cfs)\n",
    "n_cfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['cfs'].notna(), 'cfs'] = (train.loc[train['cfs'].notna(), 'cfs']\n",
    "                                          .apply(lambda cfs: [cfs_mapping[cf] for cf in cfs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 307 ms, sys: 0 ns, total: 307 ms\n",
      "Wall time: 307 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train['cfs'] = (train['cfs'].apply(lambda cfs: np.sum(np.eye(n_cfs, dtype=int)[cfs], axis=0) \n",
    "                                   if type(cfs) ==list else np.zeros(n_cfs, dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del cfs_mapping\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting city\n",
      "converting platform\n",
      "converting device\n",
      "converting country\n"
     ]
    }
   ],
   "source": [
    "# encode city, platform and device\n",
    "def categorize(df, cols):\n",
    "    for col in cols:\n",
    "        print('converting', col)\n",
    "        unique_values = df[col].unique()\n",
    "        mapping = {v: k for k, v in enumerate(unique_values)}\n",
    "        df[col] = df[col].map(mapping)\n",
    "        \n",
    "categorize(train, ['city', 'platform', 'device', 'country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['device'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train, columns=['device'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # look at price distribution\n",
    "# all_prices = np.concatenate(train.prices.values)\n",
    "# _= plt.hist(all_prices, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.max(all_prices), np.min(all_prices[all_prices!=0]), np.mean(all_prices), np.median(all_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _= plt.hist(np.log1p(all_prices), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = np.mean(np.log1p(all_prices))\n",
    "# sd = np.std(np.log1p(all_prices))\n",
    "# _ = plt.hist((np.log1p(all_prices)-m)/sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe normalize to percentage within each records, check does each item_id have the same price over all records\n",
    "def normalize(ps):\n",
    "    p_arr = np.array(ps)\n",
    "    return p_arr/(p_arr.max())\n",
    "\n",
    "train['prices'] = train['prices'].apply(normalize)\n",
    "prices = np.array(list(train['prices'].values))\n",
    "del train['prices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # IMPRESSIONS\n",
    "# impressions = np.array(list(train['impressions'].values))\n",
    "# # map each item_id to integer value\n",
    "# # impressions = np.array([[imps_mapping[j] for j in i] for i in impressions])\n",
    "# del train['impressions']\n",
    "# CURRENT_FILTERS\n",
    "cfilters = np.array(list(train['cfs'].values))\n",
    "del train['cfs']\n",
    "# record no_reorder info\n",
    "no_reorders = train['no_reorder'].values\n",
    "del train['no_reorder']\n",
    "# CITY\n",
    "cities = train['city'].values\n",
    "n_city = train['city'].nunique()\n",
    "del train['city']\n",
    "# COUNTRY\n",
    "countries = train['country'].values\n",
    "n_country = train['country'].nunique()\n",
    "del train['country']\n",
    "# PLATFORM\n",
    "platforms = train['platform'].values\n",
    "n_plat = train['platform'].nunique()\n",
    "del train['platform']\n",
    "# DEVICES\n",
    "devices = train[['device_1', 'device_2']].values\n",
    "del train['device_1'], train['device_2']\n",
    "\n",
    "# SIZES\n",
    "sizes = train['session_size'].values\n",
    "del train['session_size']\n",
    "\n",
    "# SESSION_ID\n",
    "sids = train['session_id'].values\n",
    "del train['session_id']\n",
    "# TARGETS\n",
    "targets = train['target'].values\n",
    "del train['target']\n",
    "\n",
    "\n",
    "# IMPRESSIONS\n",
    "impressions = np.array(list(train['impressions'].values))\n",
    "del train['impressions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(no_reorders, sizes, impressions, prices, cfilters, cities, countries, platforms, devices, \n",
    "                        batchsize, targets, shuffle=True, reorder=True):\n",
    "    # default we will shuffle\n",
    "    indices = np.arange(len(targets))\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        for start_idx in range(0, len(targets), batchsize):\n",
    "            excerpt = indices[start_idx:start_idx+batchsize]\n",
    "            nos = no_reorders[excerpt]\n",
    "#             print(f'allowed reorder:{nos.sum()}')\n",
    "            ss = sizes[excerpt]\n",
    "            imps = impressions[excerpt]\n",
    "            ps = prices[excerpt]\n",
    "            cfs = cfilters[excerpt]\n",
    "            cis = cities[excerpt]\n",
    "            cos = countries[excerpt]\n",
    "            plats = platforms[excerpt]\n",
    "            ds = devices[excerpt]\n",
    "            ys = targets[excerpt]\n",
    "#             print(pd.value_counts(ys))\n",
    "            if reorder and np.sum(nos) != 0:\n",
    "                re_ss = ss[nos]\n",
    "                re_imps = imps[nos]\n",
    "                re_ps = ps[nos]\n",
    "                re_cfs = cfs[nos]\n",
    "                re_cis = cis[nos]\n",
    "                re_cos = cos[nos]\n",
    "                re_plats = plats[nos]\n",
    "                re_ds = ds[nos]\n",
    "                re_ys = ys[nos]\n",
    "\n",
    "                # randomly shuffle the order of the impressions and prices\n",
    "                reorder_ind = ([np.random.choice(np.arange(25, dtype='int'), 25, replace=False) \n",
    "                                for _ in range(len(re_imps))])\n",
    "#                 print([i[0] for i in reorder_ind])\n",
    "                reorder_imp = re_imps[np.arange(re_imps.shape[0])[:, None], reorder_ind]\n",
    "                reorder_price = re_ps[np.arange(re_ps.shape[0])[:, None], reorder_ind]\n",
    "#                 reorder_ys = re_ys[np.arange(re_ys.shape[0]), reorder_ind]\n",
    "                reorder_ys = [reorder_ind[i][re_ys[i]] for i in range(len(reorder_ind))]\n",
    "#                 print(pd.value_counts(reorder_ys))\n",
    "        \n",
    "                # concatenate them back\n",
    "                yield ([np.hstack((ss, re_ss)),\n",
    "                        np.vstack((imps, reorder_imp)),\n",
    "                        np.vstack((ps, reorder_price))[:, :, None],\n",
    "                        np.vstack((cfs, re_cfs)),\n",
    "                        np.hstack((cis, re_cis)), \n",
    "                        np.hstack((cos, re_cos)), \n",
    "                        np.hstack((plats, re_plats)),\n",
    "                        np.vstack((ds, re_ds))],\n",
    "#                        np.vstack((ys, reorder_ys)))\n",
    "                       np.hstack((ys, reorder_ys)))\n",
    "            else:\n",
    "                yield ([ss, imps, ps[:, :, None], cfs, cis, cos, plats, ds], ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_auc_score\n",
    "# from keras.callbacks import Callback\n",
    "\n",
    "# class Mrr(Callback):\n",
    "#     def __init__(self, validation_data=validation_data, interval=2):\n",
    "#         super(Callback, self).__init__()\n",
    "#         self.interval = interval\n",
    "#         self.X_val, self.y_val = validation_data\n",
    "#         self.y_val = y_val\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         if epoch % self.interval == 0:\n",
    "#             y_pred = self.model.predict_generator(self.val_gen, verbose=0)\n",
    "#             val_mrr = np.mean(1/(np.where(np.argsort(y_pred)[:, ::-1] == y_val[val_pred.shape[0]].reshape(-1, 1))[1]+1))\n",
    "#             print(\"interval evaluation - epoch: {:d} - score: {:.6f}\".format(epoch, val_mrr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.random.randint(0, 10, 10)\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = [np.random.choice(np.arange(25, dtype='int'), 25, replace=False) \n",
    "#                                 for _ in range(len(a))]\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [b[i][a[i]] for i in range(len(a))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nemd(n):\n",
    "    return int(n**(1/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 3, 2)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nemd(n_city), nemd(n_country), nemd(n_plat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data generator [imps, ps, cis, cos, plats, ds], ys\n",
    "# train_gen = iterate_minibatches(trn_no, trn_imp, trn_price, trn_cfilter, trn_city, trn_country, \n",
    "#                                 trn_plat, trn_dev, batch_size, train_y_binary, shuffle=True, reorder=True)\n",
    "\n",
    "# create data generator [imps, ps, cis, cos, plats, ds], ys\n",
    "# train_gen = iterate_minibatches(trn_no, trn_imp, trn_price, trn_cfilter, trn_city, trn_country, \n",
    "#                                 trn_plat, trn_dev, batch_size, y_trn, shuffle=True, reorder=True)\n",
    "\n",
    "# val_gen = iterate_minibatches(val_no, val_imp, val_price, val_cfilter, val_city, val_country, \n",
    "#                               val_plat, val_dev, batch_size, val_y_binary, shuffle=False, reorder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in train_gen:\n",
    "# #     for j in i[0]:\n",
    "# #         print(j.shape)\n",
    "#     print(pd.value_counts(i[1]))\n",
    "# #     _ =i[1]\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in val_gen:\n",
    "#     for j in i[0]:\n",
    "#         print(j.shape)\n",
    "#     print('===', i[1].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in [trn_imp, trn_price, trn_cfilter, trn_city, trn_country, trn_plat, trn_dev]:\n",
    "#     print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'city': 3413, 'country': 134, 'cfilter': 99, 'platform': 55}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_uniques = {'city': n_city, 'country': n_country, 'cfilter': n_cfs, 'platform': n_plat}\n",
    "n_uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city, 7.64\n",
      "country, 3.40\n",
      "cfilter, 3.15\n",
      "platform, 2.72\n"
     ]
    }
   ],
   "source": [
    "for k,v in n_uniques.items():\n",
    "    print(f'{k}, {v**0.25:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len: 74,484 | val len: 14,908 | numer of parameters: 55,322 | train_len/nparams=1.34637\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "impression_input (InputLayer)   (None, 25, 157)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "imp_conv1 (Conv1D)              (None, 21, 16)       12576       impression_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "price_input (InputLayer)        (None, 25, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "imp_maxpool1 (MaxPooling1D)     (None, 10, 16)       0           imp_conv1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "price_conv1 (Conv1D)            (None, 21, 8)        48          price_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "imp_conv2 (Conv1D)              (None, 6, 32)        2592        imp_maxpool1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "price_maxpool1 (MaxPooling1D)   (None, 10, 8)        0           price_conv1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "imp_maxpool2 (MaxPooling1D)     (None, 3, 32)        0           imp_conv2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "price_con2 (Conv1D)             (None, 6, 16)        656         price_maxpool1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cfilter_input (InputLayer)      (None, 99)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 96)           0           imp_maxpool2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "price_maxpool2 (MaxPooling1D)   (None, 3, 16)        0           price_con2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 30)           3000        cfilter_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 96)           0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 48)           0           price_maxpool2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 30)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "city_input (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "country_input (InputLayer)      (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "platform (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 174)          0           dropout_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 8)         27304       city_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 4)         536         country_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 3)         165         platform[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 30)           5250        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 8)            0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 4)            0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 3)            0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "device_input (InputLayer)       (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "size_input (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 30)           0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            36          flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            20          flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 3)            12          flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 4)            12          device_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 46)           0           size_input[0][0]                 \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 46)           0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 30)           1410        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 30)           930         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 25)           775         dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 55,322\n",
      "Trainable params: 55,322\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "581/581 [==============================] - 8s 13ms/step - loss: 3.0374 - acc: 0.1712 - val_loss: 2.6966 - val_acc: 0.3078\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.69655, saving model to test.model\n",
      "Epoch 2/500\n",
      "581/581 [==============================] - 7s 11ms/step - loss: 2.9963 - acc: 0.1739 - val_loss: 2.6794 - val_acc: 0.3087\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.69655 to 2.67938, saving model to test.model\n",
      "Epoch 3/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9846 - acc: 0.1730 - val_loss: 2.6542 - val_acc: 0.3052\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.67938 to 2.65417, saving model to test.model\n",
      "Epoch 4/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9744 - acc: 0.1738 - val_loss: 2.6764 - val_acc: 0.3047\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.65417\n",
      "Epoch 5/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9653 - acc: 0.1742 - val_loss: 2.6926 - val_acc: 0.3041\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.65417\n",
      "Epoch 6/500\n",
      "581/581 [==============================] - 7s 11ms/step - loss: 2.9632 - acc: 0.1744 - val_loss: 2.6841 - val_acc: 0.2982\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.65417\n",
      "Epoch 7/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9554 - acc: 0.1754 - val_loss: 2.7206 - val_acc: 0.2970\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.65417\n",
      "Epoch 8/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9524 - acc: 0.1765 - val_loss: 2.6798 - val_acc: 0.2969\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.65417\n",
      "Epoch 9/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9499 - acc: 0.1760 - val_loss: 2.6744 - val_acc: 0.2962\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.65417\n",
      "Epoch 10/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9461 - acc: 0.1767 - val_loss: 2.6726 - val_acc: 0.2963\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.65417\n",
      "Epoch 11/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9428 - acc: 0.1774 - val_loss: 2.6843 - val_acc: 0.2939\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.65417\n",
      "Epoch 12/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9397 - acc: 0.1781 - val_loss: 2.6679 - val_acc: 0.2963\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.65417\n",
      "Epoch 13/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9366 - acc: 0.1784 - val_loss: 2.6876 - val_acc: 0.2903\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.65417\n",
      "Epoch 14/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9334 - acc: 0.1777 - val_loss: 2.6937 - val_acc: 0.2936\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.65417\n",
      "Epoch 15/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9340 - acc: 0.1781 - val_loss: 2.6802 - val_acc: 0.2946\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.65417\n",
      "Epoch 16/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9301 - acc: 0.1781 - val_loss: 2.6944 - val_acc: 0.2922\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2.65417\n",
      "Epoch 17/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9270 - acc: 0.1786 - val_loss: 2.6984 - val_acc: 0.2897\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.65417\n",
      "Epoch 18/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9253 - acc: 0.1788 - val_loss: 2.7076 - val_acc: 0.2882\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.65417\n",
      "Epoch 19/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9251 - acc: 0.1786 - val_loss: 2.6967 - val_acc: 0.2900\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.65417\n",
      "Epoch 20/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9199 - acc: 0.1805 - val_loss: 2.7050 - val_acc: 0.2883\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.65417\n",
      "Epoch 21/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9217 - acc: 0.1792 - val_loss: 2.7008 - val_acc: 0.2890\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.65417\n",
      "Epoch 22/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9209 - acc: 0.1793 - val_loss: 2.7072 - val_acc: 0.2875\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.65417\n",
      "Epoch 23/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9187 - acc: 0.1789 - val_loss: 2.7038 - val_acc: 0.2873\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.65417\n",
      "Epoch 24/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9146 - acc: 0.1798 - val_loss: 2.7135 - val_acc: 0.2892\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.65417\n",
      "Epoch 25/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9167 - acc: 0.1797 - val_loss: 2.7205 - val_acc: 0.2853\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2.65417\n",
      "Epoch 26/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9128 - acc: 0.1803 - val_loss: 2.7172 - val_acc: 0.2876\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.65417\n",
      "Epoch 27/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9126 - acc: 0.1801 - val_loss: 2.7228 - val_acc: 0.2857\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.65417\n",
      "Epoch 28/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9103 - acc: 0.1809 - val_loss: 2.7478 - val_acc: 0.2807\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.65417\n",
      "Epoch 29/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9102 - acc: 0.1804 - val_loss: 2.7340 - val_acc: 0.2858\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.65417\n",
      "Epoch 30/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9112 - acc: 0.1800 - val_loss: 2.7201 - val_acc: 0.2898\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.65417\n",
      "Epoch 31/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9079 - acc: 0.1811 - val_loss: 2.7445 - val_acc: 0.2867\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 2.65417\n",
      "Epoch 32/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9076 - acc: 0.1819 - val_loss: 2.7357 - val_acc: 0.2817\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 2.65417\n",
      "Epoch 33/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9080 - acc: 0.1805 - val_loss: 2.7430 - val_acc: 0.2813\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 2.65417\n",
      "Epoch 34/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9054 - acc: 0.1817 - val_loss: 2.7130 - val_acc: 0.2859\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 2.65417\n",
      "Epoch 35/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9067 - acc: 0.1810 - val_loss: 2.7595 - val_acc: 0.2785\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 2.65417\n",
      "Epoch 36/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9048 - acc: 0.1814 - val_loss: 2.7243 - val_acc: 0.2886\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 2.65417\n",
      "Epoch 37/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9015 - acc: 0.1815 - val_loss: 2.7422 - val_acc: 0.2882\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 2.65417\n",
      "Epoch 38/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9015 - acc: 0.1821 - val_loss: 2.7296 - val_acc: 0.2853\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 2.65417\n",
      "Epoch 39/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9000 - acc: 0.1819 - val_loss: 2.7422 - val_acc: 0.2838\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 2.65417\n",
      "Epoch 40/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.9009 - acc: 0.1825 - val_loss: 2.7516 - val_acc: 0.2821\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 2.65417\n",
      "Epoch 41/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8993 - acc: 0.1825 - val_loss: 2.7459 - val_acc: 0.2847\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 2.65417\n",
      "Epoch 42/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8998 - acc: 0.1825 - val_loss: 2.7524 - val_acc: 0.2837\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 2.65417\n",
      "Epoch 43/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8997 - acc: 0.1826 - val_loss: 2.7513 - val_acc: 0.2804\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 2.65417\n",
      "Epoch 44/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8966 - acc: 0.1829 - val_loss: 2.7372 - val_acc: 0.2838\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 2.65417\n",
      "Epoch 45/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8978 - acc: 0.1821 - val_loss: 2.7469 - val_acc: 0.2858\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 2.65417\n",
      "Epoch 46/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8946 - acc: 0.1831 - val_loss: 2.7836 - val_acc: 0.2763\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 2.65417\n",
      "Epoch 47/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8934 - acc: 0.1833 - val_loss: 2.7877 - val_acc: 0.2749\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 2.65417\n",
      "Epoch 48/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8940 - acc: 0.1825 - val_loss: 2.7548 - val_acc: 0.2838\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 2.65417\n",
      "Epoch 49/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8955 - acc: 0.1833 - val_loss: 2.7610 - val_acc: 0.2800\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 2.65417\n",
      "Epoch 50/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8902 - acc: 0.1836 - val_loss: 2.7313 - val_acc: 0.2841\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 2.65417\n",
      "Epoch 51/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8934 - acc: 0.1834 - val_loss: 2.7504 - val_acc: 0.2798\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 2.65417\n",
      "Epoch 52/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8920 - acc: 0.1836 - val_loss: 2.7680 - val_acc: 0.2764\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 2.65417\n",
      "Epoch 53/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8920 - acc: 0.1829 - val_loss: 2.7518 - val_acc: 0.2790\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 2.65417\n",
      "Epoch 54/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8883 - acc: 0.1846 - val_loss: 2.7589 - val_acc: 0.2760\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 2.65417\n",
      "Epoch 55/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8882 - acc: 0.1841 - val_loss: 2.7351 - val_acc: 0.2840\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 2.65417\n",
      "Epoch 56/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8889 - acc: 0.1837 - val_loss: 2.7604 - val_acc: 0.2823\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 2.65417\n",
      "Epoch 57/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8908 - acc: 0.1833 - val_loss: 2.7770 - val_acc: 0.2767\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 2.65417\n",
      "Epoch 58/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8870 - acc: 0.1848 - val_loss: 2.7500 - val_acc: 0.2842\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 2.65417\n",
      "Epoch 59/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8882 - acc: 0.1840 - val_loss: 2.7364 - val_acc: 0.2830\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 2.65417\n",
      "Epoch 60/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8869 - acc: 0.1852 - val_loss: 2.7924 - val_acc: 0.2733\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 2.65417\n",
      "Epoch 61/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8884 - acc: 0.1844 - val_loss: 2.7838 - val_acc: 0.2766\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 2.65417\n",
      "Epoch 62/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8816 - acc: 0.1853 - val_loss: 2.7755 - val_acc: 0.2811\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 2.65417\n",
      "Epoch 63/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8840 - acc: 0.1849 - val_loss: 2.7586 - val_acc: 0.2803\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 2.65417\n",
      "Epoch 64/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8839 - acc: 0.1848 - val_loss: 2.7951 - val_acc: 0.2707\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 2.65417\n",
      "Epoch 65/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8855 - acc: 0.1848 - val_loss: 2.7720 - val_acc: 0.2771\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 2.65417\n",
      "Epoch 66/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8820 - acc: 0.1846 - val_loss: 2.7832 - val_acc: 0.2775\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 2.65417\n",
      "Epoch 67/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8825 - acc: 0.1850 - val_loss: 2.7755 - val_acc: 0.2792\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 2.65417\n",
      "Epoch 68/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8815 - acc: 0.1856 - val_loss: 2.7645 - val_acc: 0.2778\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 2.65417\n",
      "Epoch 69/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8794 - acc: 0.1862 - val_loss: 2.7861 - val_acc: 0.2771\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 2.65417\n",
      "Epoch 70/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8824 - acc: 0.1851 - val_loss: 2.7783 - val_acc: 0.2810\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 2.65417\n",
      "Epoch 71/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8817 - acc: 0.1861 - val_loss: 2.7863 - val_acc: 0.2760\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 2.65417\n",
      "Epoch 72/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8807 - acc: 0.1853 - val_loss: 2.7570 - val_acc: 0.2806\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 2.65417\n",
      "Epoch 73/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8795 - acc: 0.1855 - val_loss: 2.7695 - val_acc: 0.2853\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 2.65417\n",
      "Epoch 74/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8805 - acc: 0.1860 - val_loss: 2.7717 - val_acc: 0.2774\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 2.65417\n",
      "Epoch 75/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8803 - acc: 0.1860 - val_loss: 2.7870 - val_acc: 0.2790\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 2.65417\n",
      "Epoch 76/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8792 - acc: 0.1860 - val_loss: 2.7737 - val_acc: 0.2800\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 2.65417\n",
      "Epoch 77/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8808 - acc: 0.1859 - val_loss: 2.7775 - val_acc: 0.2755\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 2.65417\n",
      "Epoch 78/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8795 - acc: 0.1868 - val_loss: 2.7654 - val_acc: 0.2779\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 2.65417\n",
      "Epoch 79/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8810 - acc: 0.1866 - val_loss: 2.7769 - val_acc: 0.2774\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 2.65417\n",
      "Epoch 80/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8784 - acc: 0.1856 - val_loss: 2.7855 - val_acc: 0.2736\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 2.65417\n",
      "Epoch 81/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8765 - acc: 0.1862 - val_loss: 2.7826 - val_acc: 0.2774\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 2.65417\n",
      "Epoch 82/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8794 - acc: 0.1854 - val_loss: 2.8020 - val_acc: 0.2761\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 2.65417\n",
      "Epoch 83/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8792 - acc: 0.1868 - val_loss: 2.8008 - val_acc: 0.2689\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 2.65417\n",
      "Epoch 84/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8773 - acc: 0.1872 - val_loss: 2.7916 - val_acc: 0.2775\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 2.65417\n",
      "Epoch 85/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8785 - acc: 0.1860 - val_loss: 2.8384 - val_acc: 0.2677\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 2.65417\n",
      "Epoch 86/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8781 - acc: 0.1870 - val_loss: 2.7952 - val_acc: 0.2771\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 2.65417\n",
      "Epoch 87/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8773 - acc: 0.1859 - val_loss: 2.7795 - val_acc: 0.2820\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 2.65417\n",
      "Epoch 88/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8756 - acc: 0.1868 - val_loss: 2.8080 - val_acc: 0.2753\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 2.65417\n",
      "Epoch 89/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8770 - acc: 0.1871 - val_loss: 2.7915 - val_acc: 0.2748\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 2.65417\n",
      "Epoch 90/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8756 - acc: 0.1873 - val_loss: 2.7981 - val_acc: 0.2750\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 2.65417\n",
      "Epoch 91/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8755 - acc: 0.1865 - val_loss: 2.8119 - val_acc: 0.2741\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 2.65417\n",
      "Epoch 92/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8735 - acc: 0.1878 - val_loss: 2.7692 - val_acc: 0.2783\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 2.65417\n",
      "Epoch 93/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8748 - acc: 0.1865 - val_loss: 2.7714 - val_acc: 0.2811\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 2.65417\n",
      "Epoch 94/500\n",
      "581/581 [==============================] - 7s 11ms/step - loss: 2.8748 - acc: 0.1872 - val_loss: 2.8061 - val_acc: 0.2790\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 2.65417\n",
      "Epoch 95/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8773 - acc: 0.1863 - val_loss: 2.7925 - val_acc: 0.2736\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 2.65417\n",
      "Epoch 96/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8716 - acc: 0.1878 - val_loss: 2.7840 - val_acc: 0.2796\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 2.65417\n",
      "Epoch 97/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8733 - acc: 0.1879 - val_loss: 2.7835 - val_acc: 0.2777\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 2.65417\n",
      "Epoch 98/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8745 - acc: 0.1861 - val_loss: 2.7999 - val_acc: 0.2701\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 2.65417\n",
      "Epoch 99/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8765 - acc: 0.1873 - val_loss: 2.7787 - val_acc: 0.2786\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 2.65417\n",
      "Epoch 100/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8733 - acc: 0.1872 - val_loss: 2.7930 - val_acc: 0.2750\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 2.65417\n",
      "Epoch 101/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8716 - acc: 0.1887 - val_loss: 2.7962 - val_acc: 0.2754\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 2.65417\n",
      "Epoch 102/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8723 - acc: 0.1872 - val_loss: 2.8072 - val_acc: 0.2754\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 2.65417\n",
      "Epoch 103/500\n",
      "581/581 [==============================] - 6s 11ms/step - loss: 2.8712 - acc: 0.1872 - val_loss: 2.8230 - val_acc: 0.2669\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 2.65417\n",
      "\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 00103: early stopping\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 8 array(s), but instead got the following list of 7 arrays: [array([[[0, 0, 0, ..., 0, 1, 0],\n        [1, 1, 1, ..., 0, 1, 0],\n        [1, 1, 0, ..., 0, 1, 0],\n        ...,\n        [1, 1, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 1, 0],\n        [0, 0, 0, ......",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-5bcb6a2746a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m     trn_pred = model.predict(x=[trn_imp, trn_price[:, :, None], trn_cfilter, \n\u001b[1;32m     72\u001b[0m                                 trn_city, trn_country, trn_plat, trn_dev], \n\u001b[0;32m---> 73\u001b[0;31m                              batch_size=1024)\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mtrn_pred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_trn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mtrn_mrr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_pred_label\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 8 array(s), but instead got the following list of 7 arrays: [array([[[0, 0, 0, ..., 0, 1, 0],\n        [1, 1, 1, ..., 0, 1, 0],\n        [1, 1, 0, ..., 0, 1, 0],\n        ...,\n        [1, 1, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 1, 0],\n        [0, 0, 0, ......"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from datetime import datetime as dt\n",
    "from nn_model import build_model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "batch_size = 128\n",
    "# n_repeat = 2\n",
    "n_epochs = 500\n",
    "\n",
    "skf = StratifiedKFold(n_splits=6)\n",
    "\n",
    "for trn_ind, val_ind in skf.split(targets, targets):\n",
    "    trn_ss, val_ss = sizes[trn_ind], sizes[val_ind]\n",
    "    trn_no, val_no = no_reorders[trn_ind], no_reorders[val_ind]\n",
    "    trn_imp, val_imp = impressions[trn_ind], impressions[val_ind]\n",
    "    trn_price, val_price = prices[trn_ind], prices[val_ind]\n",
    "    trn_cfilter, val_cfilter = cfilters[trn_ind], cfilters[val_ind]\n",
    "    trn_city, val_city = cities[trn_ind], cities[val_ind]\n",
    "    trn_country, val_country = countries[trn_ind], countries[val_ind]\n",
    "    trn_plat, val_plat = platforms[trn_ind], platforms[val_ind]\n",
    "    trn_dev, val_dev = devices[trn_ind], devices[val_ind]\n",
    "    \n",
    "    y_trn, y_val = targets[trn_ind], targets[val_ind]\n",
    "    \n",
    "    # create data generator [imps, ps, cis, cos, plats, ds], ys\n",
    "    train_gen = iterate_minibatches(trn_ss, trn_no, trn_imp, trn_price, trn_cfilter, trn_city, trn_country, \n",
    "                                    trn_plat, trn_dev, batch_size, y_trn, shuffle=True, reorder=True)\n",
    "    \n",
    "    val_gen = iterate_minibatches(val_ss, val_no, val_imp, val_price, val_cfilter, val_city, val_country, \n",
    "                                  val_plat, val_dev, batch_size, y_val, shuffle=False, reorder=False)\n",
    "#     TEMP\n",
    "#     del impressions, prices, cities, platforms, devices\n",
    "#     gc.collect()\n",
    "    \n",
    "    # =====================================================================================\n",
    "    # create model\n",
    "    model = build_model(n_uniques, conv1d_filter_size=5, dense_act='linear', conv_act='relu')\n",
    "    \n",
    "    # print out model info\n",
    "    nparams = model.count_params()\n",
    "    print((f'train len: {len(y_trn):,} | val len: {len(y_val):,} '\n",
    "           f'| numer of parameters: {nparams:,} | train_len/nparams={len(y_trn)/nparams:.5f}'))\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='model.png')\n",
    "    # add some callbacks\n",
    "    callbacks = []\n",
    "    model_file = 'test.model'\n",
    "    callbacks = [ModelCheckpoint(model_file, save_best_only=True, verbose=1)]\n",
    "    log_dir = \"logs/{}\".format(dt.now().strftime('%m-%d-%H-%M'))\n",
    "    tb = TensorBoard(log_dir=log_dir, write_graph=True, write_grads=True)\n",
    "    callbacks.append(tb)\n",
    "    # simple early stopping\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', patience=100, verbose=1)\n",
    "    callbacks.append(es)\n",
    "    # rp\n",
    "    rp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=100, verbose=1)\n",
    "    callbacks.append(rp)\n",
    "    # add mrr callback\n",
    "#     callbacks.append(IntervalEvaluation())\n",
    "    \n",
    "    history = model.fit_generator(train_gen, \n",
    "                                  steps_per_epoch=len(y_trn)//batch_size, \n",
    "                                  epochs=n_epochs, \n",
    "                                  verbose=1,\n",
    "                                  callbacks=callbacks, \n",
    "                                  validation_data=val_gen, \n",
    "                                  validation_steps=len(y_val)//batch_size)\n",
    "\n",
    "    # make prediction\n",
    "    trn_pred = model.predict(x=[trn_imp, trn_price[:, :, None], trn_cfilter, \n",
    "                                trn_city, trn_country, trn_plat, trn_dev], \n",
    "                             batch_size=1024)\n",
    "    trn_pred_label = np.where(np.argsort(trn_pred)[:, ::-1] == y_trn.reshape(-1, 1))[1]\n",
    "    trn_mrr = np.mean(1/(trn_pred_label+1))\n",
    "\n",
    "    val_pred = model.predict(x=[val_imp, val_price[:, :, None], val_cfilter, \n",
    "                                val_city, val_country, val_plat, val_dev], \n",
    "                             batch_size=1024)\n",
    "    val_pred_label = np.where(np.argsort(val_pred)[:, ::-1] == y_val.reshape(-1, 1))[1]\n",
    "    val_mrr = np.mean(1/(val_pred_label+1))\n",
    "    print(f'train mrr: {trn_mrr:.2f} | val mrr: {val_mrr:.2f}')\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(trn_pred_label, bins=50, label='train_pred', alpha=0.7)\n",
    "_ = plt.hist(y_trn, bins=50, label = 'train label', alpha=0.7)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(val_pred_label, bins=50, label='val_pred', alpha=0.7)\n",
    "_ = plt.hist(y_val, bins=50, label = 'val label', alpha=0.7)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train mrr: 0.47 | val mrr: 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add mrr at per epochs probably\n",
    "# look at no embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_pred, y_true, normalize='row', level=0, log_scale=False):\n",
    "    compare = pd.DataFrame({'prediction': y_pred, 'y_true': y_true})\n",
    "    counts = compare.groupby('y_true')['prediction'].value_counts()\n",
    "    mat = counts.unstack(level=0)\n",
    "    mat.fillna(0, inplace=True)\n",
    "    \n",
    "    if normalize == 'row':\n",
    "        row_sum = mat.sum(axis=1)\n",
    "        mat = mat.div(row_sum, axis=0)\n",
    "        log_scale = False\n",
    "    elif normalize == 'column':\n",
    "        col_sum = mat.sum(axis=0)\n",
    "        mat = mat.div(col_sum, axis=1)\n",
    "        log_scale = False\n",
    "    # plot\n",
    "    fig = plt.figure(figsize=(35,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    if log_scale:\n",
    "        cax = ax.matshow(np.log1p(mat), interpolation='nearest')#, cmap='coolwarm')#, aspect='auto')\n",
    "    else:\n",
    "        cax = ax.matshow(mat, interpolation='nearest')#, cmap='coolwarm')#, aspect='auto')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xlabel(f'{mat.columns.name}')\n",
    "    ax.xaxis.set_label_position('top') \n",
    "    ax.set_ylabel(f'{mat.index.name}')\n",
    "    \n",
    "    ax.set_xticks(np.arange(mat.shape[1]))\n",
    "    ax.set_xticklabels(list(mat.columns.astype(str)), rotation=90)\n",
    "    ax.set_yticks(np.arange(mat.shape[0]))\n",
    "    _ = ax.set_yticklabels(list(mat.index.astype(str)))\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(trn_pred_label, y_trn, normalize=False, level=0, log_scale=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(trn_pred_label, y_trn, normalize='row')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(trn_pred_label, y_trn, normalize='column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(val_pred_label, y_val, level=0, normalize=None, log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
