{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time \n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "from functools import partial\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from utils import ignore_warnings, load_data\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.7 s, sys: 1.4 s, total: 20.1 s\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# nrows = 10000\n",
    "nrows = None\n",
    "train = load_data('train', nrows=nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[04-28 11:21:44 - utils - preprocess_sessions - INFO] Cliping session dataframe up to last click out (if there is clickout)\n",
      "[04-28 11:28:24 - utils - preprocess_sessions - INFO] filtering out sessions without clickouts, reference, or clickout is nan\n",
      "[04-28 11:28:24 - utils - preprocess_sessions - INFO] data length before filtering: 13,034,626\n",
      "[04-28 11:44:42 - utils - preprocess_sessions - INFO] data length after filtering: 13,034,626\n",
      "[04-28 11:44:42 - utils - preprocess_sessions - INFO] Saving ./cache/preprocessed_data.snappy\n"
     ]
    }
   ],
   "source": [
    "from clean_session import preprocess_sessions\n",
    "train = preprocess_sessions(train,data_source='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13034626, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the rows that is clickout\n",
    "is_clickout = train.action_type == 'clickout item'\n",
    "# # and it is not nan\n",
    "# not_na = train.re.notna()\n",
    "# and the impressions are not nans\n",
    "imp_not_na = train.impressions.notna()\n",
    "# only select the ones with 25 lens \n",
    "train['nimp'] = train.impressions.str.split('|').str.len()\n",
    "twenty_five = train['nimp'] == 25\n",
    "\n",
    "select_mask = is_clickout & imp_not_na & twenty_five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1232016, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train[select_mask].reset_index(drop=True)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['current_filters'].isna(), 'current_filters'] = 'no_filter'\n",
    "train.loc[train['reference'].isna(), 'reference'] = 'no_reference'\n",
    "\n",
    "train['cfs'] = train['current_filters'].str.split('|')\n",
    "train['imps'] = train['impressions'].str.split('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: (1232016, 15)\n",
      "after: (1232016, 15)\n",
      "before: (1232016, 16)\n",
      "after: (1231380, 16)\n"
     ]
    }
   ],
   "source": [
    "print('before:', train.shape)\n",
    "train = train[train.reference.notna()].reset_index(drop=True)\n",
    "print('after:', train.shape)\n",
    "train = train[train.imps.str.len()==25].reset_index(drop=True)\n",
    "def assign_target(row):\n",
    "    ref = row.reference\n",
    "    imp = row.imps\n",
    "    if ref in imp:\n",
    "        return imp.index(ref)\n",
    "    else:\n",
    "        return 25\n",
    "#         return -1\n",
    "train['target'] = train.apply(assign_target, axis=1)\n",
    "# remove the target 25 (i.e. not appearing in the list)\n",
    "print('before:', train.shape)\n",
    "train = train[train.target != 25].reset_index(drop=True)\n",
    "print('after:', train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>step</th>\n",
       "      <th>action_type</th>\n",
       "      <th>reference</th>\n",
       "      <th>platform</th>\n",
       "      <th>city</th>\n",
       "      <th>device</th>\n",
       "      <th>current_filters</th>\n",
       "      <th>impressions</th>\n",
       "      <th>prices</th>\n",
       "      <th>nimp</th>\n",
       "      <th>cfs</th>\n",
       "      <th>imps</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WT30CXPIG450</td>\n",
       "      <td>00000510f1adc</td>\n",
       "      <td>1541064087</td>\n",
       "      <td>1</td>\n",
       "      <td>clickout item</td>\n",
       "      <td>7281198</td>\n",
       "      <td>IN</td>\n",
       "      <td>Ganpatipule, India</td>\n",
       "      <td>desktop</td>\n",
       "      <td>no_filter</td>\n",
       "      <td>2661832|9222426|7051844|4079190|5752778|468398...</td>\n",
       "      <td>46|26|16|38|12|20|21|27|13|21|36|9|144|19|8|19...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>[no_filter]</td>\n",
       "      <td>[2661832, 9222426, 7051844, 4079190, 5752778, ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CITFOTN2IT5P</td>\n",
       "      <td>00003f3b20954</td>\n",
       "      <td>1541097696</td>\n",
       "      <td>1</td>\n",
       "      <td>clickout item</td>\n",
       "      <td>979325</td>\n",
       "      <td>ES</td>\n",
       "      <td>La Manga, Spain</td>\n",
       "      <td>mobile</td>\n",
       "      <td>no_filter</td>\n",
       "      <td>87132|886881|486611|979325|87173|87175|149508|...</td>\n",
       "      <td>330|187|437|159|499|324|476|381|424|159|144|19...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>[no_filter]</td>\n",
       "      <td>[87132, 886881, 486611, 979325, 87173, 87175, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id     session_id   timestamp  step    action_type reference  \\\n",
       "0  WT30CXPIG450  00000510f1adc  1541064087     1  clickout item   7281198   \n",
       "1  CITFOTN2IT5P  00003f3b20954  1541097696     1  clickout item    979325   \n",
       "\n",
       "  platform                city   device current_filters  \\\n",
       "0       IN  Ganpatipule, India  desktop       no_filter   \n",
       "1       ES     La Manga, Spain   mobile       no_filter   \n",
       "\n",
       "                                         impressions  \\\n",
       "0  2661832|9222426|7051844|4079190|5752778|468398...   \n",
       "1  87132|886881|486611|979325|87173|87175|149508|...   \n",
       "\n",
       "                                              prices  nimp          cfs  \\\n",
       "0  46|26|16|38|12|20|21|27|13|21|36|9|144|19|8|19...  25.0  [no_filter]   \n",
       "1  330|187|437|159|499|324|476|381|424|159|144|19...  25.0  [no_filter]   \n",
       "\n",
       "                                                imps  target  \n",
       "0  [2661832, 9222426, 7051844, 4079190, 5752778, ...       6  \n",
       "1  [87132, 886881, 486611, 979325, 87173, 87175, ...       3  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('./cache/hotel_2vec/model.bin')\n",
    "\n",
    "def encoding_depth(imps):\n",
    "    return np.array([model.wv[i] for i in imps])[None, :, :]\n",
    "\n",
    "def encoding(imps):\n",
    "    return np.array([model.wv[i] for i in imps])\n",
    "\n",
    "def encoding_column(imps):\n",
    "    return np.array([model.wv[i] for i in imps])[:, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train['imps'] = train.imps.apply(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting city\n",
      "converting platform\n",
      "converting device\n"
     ]
    }
   ],
   "source": [
    "# encode city, platform and device\n",
    "def categorize(df, cols):\n",
    "    for col in cols:\n",
    "        print('converting', col)\n",
    "        unique_values = df[col].unique()\n",
    "        mapping = {v: k for k, v in enumerate(unique_values)}\n",
    "        df[col] = df[col].map(mapping)\n",
    "        \n",
    "categorize(train, ['city', 'platform', 'device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.device.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train, columns=['device'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train[['session_id', 'timestamp', 'reference', 'imps', 'city', 'device', 'platform', 'prices']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1231380, 17)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['prices'] = train.prices.str.split('|')\n",
    "train['prices'] = train['prices'].apply(lambda prices: [int(p) for p in prices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prices = train.prices.values\n",
    "all_prices = [j for i in all_prices for j in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# _ = plt.hist(all_prices, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = plt.hist(np.log1p(all_prices), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = plt.hist(train_prices, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_mu = np.mean(all_prices)\n",
    "price_sd = np.std(all_prices)\n",
    "prices = np.array(list(train.prices.values))\n",
    "prices = (prices - price_mu)/price_sd\n",
    "del train['prices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119.15391521707353"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "impressions = np.array(list(train.imps.values))\n",
    "del train['imps']\n",
    "\n",
    "cities = train.city.values\n",
    "ncity = train.city.nunique()\n",
    "del train['city']\n",
    "\n",
    "platforms = train.platform.values\n",
    "nplat = train.platform.nunique()\n",
    "\n",
    "del train['platform']\n",
    "sids = train.session_id.values\n",
    "del train['session_id']\n",
    "targets = train.target.values\n",
    "del train['target']\n",
    "devices = train[['device_1', 'device_2']].values\n",
    "del train['device_1'], train['device_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import Callback\n",
    "import tensorflow as tf\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)        \n",
    "        print('Testing loss: {0:.4f}, acc: {1:.4f}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras.backend as K\n",
    "\n",
    "# def mrr(y_true, y_pred):\n",
    "# #     return K.mean(y_pred)\n",
    "#     y_true_item = K.argmax(y_true, axis=-1)\n",
    "#     print(y_true_item)\n",
    "    \n",
    "#     y_pred_sorted = tf.nn.top_k(input, k=25, sorted=True).indices\n",
    "#     return K.mean(1/tf.where(y_pred_sorted==y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 25, 100)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 21, 16)       8016        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 21, 16)       64          conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 336)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 361)          0           flatten_1[0][0]                  \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 361)          1444        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 30)           10860       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 20)        415100      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 10)        550         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 30)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 20)           0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 10)           0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 62)           0           dropout_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 62)           248         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 62)           0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 30)           1890        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 25)           775         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 438,947\n",
      "Trainable params: 438,069\n",
      "Non-trainable params: 878\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      " - 26s - loss: 2.6106 - acc: 0.3228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:434: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 2.5747, acc: 0.3263\n",
      "Epoch 2/300\n",
      " - 21s - loss: 2.5630 - acc: 0.3264\n",
      "Testing loss: 2.5690, acc: 0.3262\n",
      "Epoch 3/300\n",
      " - 21s - loss: 2.5478 - acc: 0.3268\n",
      "Testing loss: 2.5680, acc: 0.3257\n",
      "Epoch 4/300\n",
      " - 21s - loss: 2.5363 - acc: 0.3278\n",
      "Testing loss: 2.5707, acc: 0.3249\n",
      "Epoch 5/300\n",
      " - 21s - loss: 2.5266 - acc: 0.3288\n",
      "Testing loss: 2.5730, acc: 0.3240\n",
      "Epoch 6/300\n",
      " - 21s - loss: 2.5188 - acc: 0.3297\n",
      "Testing loss: 2.5735, acc: 0.3232\n",
      "Epoch 7/300\n",
      " - 21s - loss: 2.5130 - acc: 0.3304\n",
      "Testing loss: 2.5770, acc: 0.3224\n",
      "Epoch 8/300\n",
      " - 21s - loss: 2.5084 - acc: 0.3309\n",
      "Testing loss: 2.5805, acc: 0.3223\n",
      "Epoch 9/300\n",
      " - 21s - loss: 2.5039 - acc: 0.3312\n",
      "Testing loss: 2.5813, acc: 0.3220\n",
      "Epoch 10/300\n",
      " - 21s - loss: 2.5001 - acc: 0.3314\n",
      "Testing loss: 2.5820, acc: 0.3218\n",
      "Epoch 11/300\n",
      " - 21s - loss: 2.4969 - acc: 0.3318\n",
      "Testing loss: 2.5842, acc: 0.3221\n",
      "Epoch 12/300\n",
      " - 21s - loss: 2.4943 - acc: 0.3320\n",
      "Testing loss: 2.5844, acc: 0.3221\n",
      "Epoch 13/300\n",
      " - 21s - loss: 2.4919 - acc: 0.3319\n",
      "Testing loss: 2.5884, acc: 0.3217\n",
      "Epoch 14/300\n",
      " - 21s - loss: 2.4901 - acc: 0.3323\n",
      "Testing loss: 2.5881, acc: 0.3216\n",
      "Epoch 15/300\n",
      " - 21s - loss: 2.4880 - acc: 0.3324\n",
      "Testing loss: 2.5892, acc: 0.3212\n",
      "Epoch 16/300\n",
      " - 21s - loss: 2.4861 - acc: 0.3324\n",
      "Testing loss: 2.5887, acc: 0.3207\n",
      "Epoch 17/300\n",
      " - 21s - loss: 2.4846 - acc: 0.3327\n",
      "Testing loss: 2.5927, acc: 0.3210\n",
      "Epoch 18/300\n",
      " - 21s - loss: 2.4833 - acc: 0.3327\n",
      "Testing loss: 2.5935, acc: 0.3208\n",
      "Epoch 19/300\n",
      " - 21s - loss: 2.4821 - acc: 0.3326\n",
      "Testing loss: 2.5903, acc: 0.3213\n",
      "Epoch 20/300\n",
      " - 21s - loss: 2.4810 - acc: 0.3328\n",
      "Testing loss: 2.5936, acc: 0.3205\n",
      "Epoch 21/300\n",
      " - 21s - loss: 2.4796 - acc: 0.3327\n",
      "Testing loss: 2.5929, acc: 0.3217\n",
      "Epoch 22/300\n",
      " - 21s - loss: 2.4786 - acc: 0.3329\n",
      "Testing loss: 2.5941, acc: 0.3210\n",
      "Epoch 23/300\n",
      " - 21s - loss: 2.4775 - acc: 0.3329\n",
      "Testing loss: 2.5964, acc: 0.3209\n",
      "Epoch 24/300\n",
      " - 21s - loss: 2.4767 - acc: 0.3332\n",
      "Testing loss: 2.6006, acc: 0.3209\n",
      "Epoch 25/300\n",
      " - 21s - loss: 2.4758 - acc: 0.3331\n",
      "Testing loss: 2.5952, acc: 0.3210\n",
      "Epoch 26/300\n",
      " - 21s - loss: 2.4747 - acc: 0.3332\n",
      "Testing loss: 2.5984, acc: 0.3211\n",
      "Epoch 27/300\n",
      " - 21s - loss: 2.4741 - acc: 0.3332\n",
      "Testing loss: 2.5980, acc: 0.3211\n",
      "Epoch 28/300\n",
      " - 21s - loss: 2.4735 - acc: 0.3333\n",
      "Testing loss: 2.5971, acc: 0.3212\n",
      "Epoch 29/300\n",
      " - 21s - loss: 2.4730 - acc: 0.3333\n",
      "Testing loss: 2.5957, acc: 0.3209\n",
      "Epoch 30/300\n",
      " - 21s - loss: 2.4721 - acc: 0.3335\n",
      "Testing loss: 2.6017, acc: 0.3216\n",
      "Epoch 31/300\n",
      " - 21s - loss: 2.4717 - acc: 0.3334\n",
      "Testing loss: 2.5973, acc: 0.3217\n",
      "Epoch 32/300\n",
      " - 21s - loss: 2.4711 - acc: 0.3334\n",
      "Testing loss: 2.5996, acc: 0.3200\n",
      "Epoch 33/300\n",
      " - 21s - loss: 2.4703 - acc: 0.3335\n",
      "Testing loss: 2.6014, acc: 0.3212\n",
      "Epoch 34/300\n",
      " - 21s - loss: 2.4697 - acc: 0.3336\n",
      "Testing loss: 2.5997, acc: 0.3207\n",
      "Epoch 35/300\n",
      " - 21s - loss: 2.4692 - acc: 0.3335\n",
      "Testing loss: 2.6033, acc: 0.3208\n",
      "Epoch 36/300\n",
      " - 21s - loss: 2.4686 - acc: 0.3337\n",
      "Testing loss: 2.5992, acc: 0.3207\n",
      "Epoch 37/300\n",
      " - 21s - loss: 2.4680 - acc: 0.3337\n",
      "Testing loss: 2.6030, acc: 0.3211\n",
      "Epoch 38/300\n",
      " - 21s - loss: 2.4678 - acc: 0.3337\n",
      "Testing loss: 2.6010, acc: 0.3206\n",
      "Epoch 39/300\n",
      " - 21s - loss: 2.4674 - acc: 0.3337\n",
      "Testing loss: 2.6039, acc: 0.3205\n",
      "Epoch 40/300\n",
      " - 21s - loss: 2.4670 - acc: 0.3339\n",
      "Testing loss: 2.6038, acc: 0.3202\n",
      "Epoch 41/300\n",
      " - 21s - loss: 2.4665 - acc: 0.3338\n",
      "Testing loss: 2.6058, acc: 0.3216\n",
      "Epoch 42/300\n",
      " - 21s - loss: 2.4660 - acc: 0.3339\n",
      "Testing loss: 2.5997, acc: 0.3207\n",
      "Epoch 43/300\n",
      " - 21s - loss: 2.4658 - acc: 0.3338\n",
      "Testing loss: 2.6032, acc: 0.3207\n",
      "Epoch 44/300\n",
      " - 21s - loss: 2.4650 - acc: 0.3341\n",
      "Testing loss: 2.5990, acc: 0.3213\n",
      "Epoch 45/300\n",
      " - 21s - loss: 2.4651 - acc: 0.3339\n",
      "Testing loss: 2.6021, acc: 0.3208\n",
      "Epoch 46/300\n",
      " - 21s - loss: 2.4648 - acc: 0.3340\n",
      "Testing loss: 2.6051, acc: 0.3208\n",
      "Epoch 47/300\n",
      " - 21s - loss: 2.4638 - acc: 0.3340\n",
      "Testing loss: 2.6026, acc: 0.3207\n",
      "Epoch 48/300\n",
      " - 21s - loss: 2.4636 - acc: 0.3339\n",
      "Testing loss: 2.6036, acc: 0.3208\n",
      "Epoch 49/300\n",
      " - 21s - loss: 2.4632 - acc: 0.3341\n",
      "Testing loss: 2.6070, acc: 0.3203\n",
      "Epoch 50/300\n",
      " - 21s - loss: 2.4630 - acc: 0.3343\n",
      "Testing loss: 2.6051, acc: 0.3195\n",
      "Epoch 51/300\n",
      " - 21s - loss: 2.4630 - acc: 0.3340\n",
      "Testing loss: 2.6043, acc: 0.3204\n",
      "Epoch 52/300\n",
      " - 21s - loss: 2.4625 - acc: 0.3342\n",
      "Testing loss: 2.6074, acc: 0.3204\n",
      "Epoch 53/300\n",
      " - 21s - loss: 2.4622 - acc: 0.3342\n",
      "Testing loss: 2.6063, acc: 0.3209\n",
      "Epoch 54/300\n",
      " - 21s - loss: 2.4623 - acc: 0.3341\n",
      "Testing loss: 2.6044, acc: 0.3201\n",
      "Epoch 55/300\n",
      " - 21s - loss: 2.4614 - acc: 0.3342\n",
      "Testing loss: 2.6060, acc: 0.3203\n",
      "Epoch 56/300\n",
      " - 21s - loss: 2.4613 - acc: 0.3341\n",
      "Testing loss: 2.6037, acc: 0.3202\n",
      "Epoch 57/300\n",
      " - 21s - loss: 2.4611 - acc: 0.3341\n",
      "Testing loss: 2.6017, acc: 0.3208\n",
      "Epoch 58/300\n",
      " - 21s - loss: 2.4610 - acc: 0.3344\n",
      "Testing loss: 2.6106, acc: 0.3204\n",
      "Epoch 59/300\n",
      " - 21s - loss: 2.4607 - acc: 0.3341\n",
      "Testing loss: 2.6048, acc: 0.3203\n",
      "Epoch 60/300\n",
      " - 21s - loss: 2.4604 - acc: 0.3344\n",
      "Testing loss: 2.6090, acc: 0.3188\n",
      "Epoch 61/300\n",
      " - 21s - loss: 2.4602 - acc: 0.3344\n",
      "Testing loss: 2.6062, acc: 0.3203\n",
      "Epoch 62/300\n",
      " - 21s - loss: 2.4605 - acc: 0.3344\n",
      "Testing loss: 2.6066, acc: 0.3197\n",
      "Epoch 63/300\n",
      " - 21s - loss: 2.4596 - acc: 0.3345\n",
      "Testing loss: 2.6050, acc: 0.3204\n",
      "Epoch 64/300\n",
      " - 21s - loss: 2.4592 - acc: 0.3347\n",
      "Testing loss: 2.6059, acc: 0.3202\n",
      "Epoch 65/300\n",
      " - 21s - loss: 2.4592 - acc: 0.3344\n",
      "Testing loss: 2.6085, acc: 0.3207\n",
      "Epoch 66/300\n",
      " - 21s - loss: 2.4593 - acc: 0.3345\n",
      "Testing loss: 2.6069, acc: 0.3204\n",
      "Epoch 67/300\n",
      " - 21s - loss: 2.4583 - acc: 0.3345\n",
      "Testing loss: 2.6061, acc: 0.3196\n",
      "Epoch 68/300\n",
      " - 21s - loss: 2.4590 - acc: 0.3346\n",
      "Testing loss: 2.6120, acc: 0.3189\n",
      "Epoch 69/300\n",
      " - 21s - loss: 2.4583 - acc: 0.3345\n",
      "Testing loss: 2.6055, acc: 0.3203\n",
      "Epoch 70/300\n",
      " - 21s - loss: 2.4588 - acc: 0.3344\n",
      "Testing loss: 2.6070, acc: 0.3203\n",
      "Epoch 71/300\n",
      " - 21s - loss: 2.4581 - acc: 0.3344\n",
      "Testing loss: 2.6124, acc: 0.3205\n",
      "Epoch 72/300\n",
      " - 21s - loss: 2.4579 - acc: 0.3346\n",
      "Testing loss: 2.6057, acc: 0.3204\n",
      "Epoch 73/300\n",
      " - 21s - loss: 2.4580 - acc: 0.3343\n",
      "Testing loss: 2.6071, acc: 0.3204\n",
      "Epoch 74/300\n",
      " - 21s - loss: 2.4577 - acc: 0.3347\n",
      "Testing loss: 2.6060, acc: 0.3203\n",
      "Epoch 75/300\n",
      " - 21s - loss: 2.4578 - acc: 0.3344\n",
      "Testing loss: 2.6082, acc: 0.3197\n",
      "Epoch 76/300\n",
      " - 21s - loss: 2.4575 - acc: 0.3345\n",
      "Testing loss: 2.6135, acc: 0.3197\n",
      "Epoch 77/300\n",
      " - 21s - loss: 2.4571 - acc: 0.3348\n",
      "Testing loss: 2.6083, acc: 0.3199\n",
      "Epoch 78/300\n",
      " - 21s - loss: 2.4571 - acc: 0.3345\n",
      "Testing loss: 2.6054, acc: 0.3205\n",
      "Epoch 79/300\n",
      " - 21s - loss: 2.4569 - acc: 0.3346\n",
      "Testing loss: 2.6138, acc: 0.3196\n",
      "Epoch 80/300\n",
      " - 21s - loss: 2.4570 - acc: 0.3345\n",
      "Testing loss: 2.6041, acc: 0.3196\n",
      "Epoch 81/300\n",
      " - 21s - loss: 2.4562 - acc: 0.3348\n",
      "Testing loss: 2.6094, acc: 0.3193\n",
      "Epoch 82/300\n",
      " - 21s - loss: 2.4569 - acc: 0.3345\n",
      "Testing loss: 2.6113, acc: 0.3209\n",
      "Epoch 83/300\n",
      " - 21s - loss: 2.4561 - acc: 0.3349\n",
      "Testing loss: 2.6128, acc: 0.3193\n",
      "Epoch 84/300\n",
      " - 21s - loss: 2.4562 - acc: 0.3347\n",
      "Testing loss: 2.6098, acc: 0.3190\n",
      "Epoch 85/300\n",
      " - 21s - loss: 2.4557 - acc: 0.3348\n",
      "Testing loss: 2.6086, acc: 0.3198\n",
      "Epoch 86/300\n",
      " - 21s - loss: 2.4557 - acc: 0.3349\n",
      "Testing loss: 2.6116, acc: 0.3194\n",
      "Epoch 87/300\n",
      " - 21s - loss: 2.4558 - acc: 0.3349\n",
      "Testing loss: 2.6104, acc: 0.3199\n",
      "Epoch 88/300\n",
      " - 21s - loss: 2.4559 - acc: 0.3346\n",
      "Testing loss: 2.6105, acc: 0.3204\n",
      "Epoch 89/300\n",
      " - 21s - loss: 2.4553 - acc: 0.3347\n",
      "Testing loss: 2.6098, acc: 0.3197\n",
      "Epoch 90/300\n",
      " - 21s - loss: 2.4549 - acc: 0.3349\n",
      "Testing loss: 2.6114, acc: 0.3196\n",
      "Epoch 91/300\n",
      " - 21s - loss: 2.4552 - acc: 0.3346\n",
      "Testing loss: 2.6112, acc: 0.3193\n",
      "Epoch 92/300\n",
      " - 21s - loss: 2.4547 - acc: 0.3348\n",
      "Testing loss: 2.6092, acc: 0.3201\n",
      "Epoch 93/300\n",
      " - 21s - loss: 2.4548 - acc: 0.3347\n",
      "Testing loss: 2.6110, acc: 0.3195\n",
      "Epoch 94/300\n",
      " - 21s - loss: 2.4547 - acc: 0.3347\n",
      "Testing loss: 2.6111, acc: 0.3194\n",
      "Epoch 95/300\n",
      " - 21s - loss: 2.4548 - acc: 0.3350\n",
      "Testing loss: 2.6140, acc: 0.3183\n",
      "Epoch 96/300\n",
      " - 21s - loss: 2.4544 - acc: 0.3349\n",
      "Testing loss: 2.6102, acc: 0.3187\n",
      "Epoch 97/300\n",
      " - 21s - loss: 2.4545 - acc: 0.3349\n",
      "Testing loss: 2.6135, acc: 0.3188\n",
      "Epoch 98/300\n",
      " - 21s - loss: 2.4544 - acc: 0.3351\n",
      "Testing loss: 2.6103, acc: 0.3192\n",
      "Epoch 99/300\n",
      " - 21s - loss: 2.4543 - acc: 0.3349\n",
      "Testing loss: 2.6144, acc: 0.3189\n",
      "Epoch 100/300\n",
      " - 21s - loss: 2.4540 - acc: 0.3350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 2.6119, acc: 0.3183\n",
      "Epoch 101/300\n",
      " - 21s - loss: 2.4540 - acc: 0.3350\n",
      "Testing loss: 2.6081, acc: 0.3201\n",
      "Epoch 102/300\n",
      " - 21s - loss: 2.4536 - acc: 0.3349\n",
      "Testing loss: 2.6123, acc: 0.3198\n",
      "Epoch 103/300\n",
      " - 21s - loss: 2.4535 - acc: 0.3353\n",
      "Testing loss: 2.6153, acc: 0.3176\n",
      "Epoch 104/300\n",
      " - 21s - loss: 2.4533 - acc: 0.3351\n",
      "Testing loss: 2.6145, acc: 0.3183\n",
      "Epoch 105/300\n",
      " - 21s - loss: 2.4532 - acc: 0.3351\n",
      "Testing loss: 2.6138, acc: 0.3190\n",
      "Epoch 106/300\n",
      " - 21s - loss: 2.4534 - acc: 0.3352\n",
      "Testing loss: 2.6171, acc: 0.3187\n",
      "Epoch 107/300\n",
      " - 21s - loss: 2.4532 - acc: 0.3350\n",
      "Testing loss: 2.6103, acc: 0.3189\n",
      "Epoch 108/300\n",
      " - 21s - loss: 2.4534 - acc: 0.3348\n",
      "Testing loss: 2.6140, acc: 0.3187\n",
      "Epoch 109/300\n",
      " - 21s - loss: 2.4534 - acc: 0.3350\n",
      "Testing loss: 2.6112, acc: 0.3201\n",
      "Epoch 110/300\n",
      " - 21s - loss: 2.4531 - acc: 0.3350\n",
      "Testing loss: 2.6169, acc: 0.3183\n",
      "Epoch 111/300\n",
      " - 21s - loss: 2.4530 - acc: 0.3350\n",
      "Testing loss: 2.6090, acc: 0.3189\n",
      "Epoch 112/300\n",
      " - 21s - loss: 2.4529 - acc: 0.3350\n",
      "Testing loss: 2.6091, acc: 0.3198\n",
      "Epoch 113/300\n",
      " - 21s - loss: 2.4525 - acc: 0.3352\n",
      "Testing loss: 2.6117, acc: 0.3197\n",
      "Epoch 114/300\n",
      " - 21s - loss: 2.4524 - acc: 0.3352\n",
      "Testing loss: 2.6113, acc: 0.3194\n",
      "Epoch 115/300\n",
      " - 21s - loss: 2.4528 - acc: 0.3349\n",
      "Testing loss: 2.6107, acc: 0.3190\n",
      "Epoch 116/300\n",
      " - 21s - loss: 2.4525 - acc: 0.3351\n",
      "Testing loss: 2.6137, acc: 0.3179\n",
      "Epoch 117/300\n",
      " - 21s - loss: 2.4521 - acc: 0.3354\n",
      "Testing loss: 2.6156, acc: 0.3197\n",
      "Epoch 118/300\n",
      " - 21s - loss: 2.4526 - acc: 0.3351\n",
      "Testing loss: 2.6126, acc: 0.3196\n",
      "Epoch 119/300\n",
      " - 21s - loss: 2.4519 - acc: 0.3350\n",
      "Testing loss: 2.6157, acc: 0.3186\n",
      "Epoch 120/300\n",
      " - 21s - loss: 2.4521 - acc: 0.3352\n",
      "Testing loss: 2.6149, acc: 0.3182\n",
      "Epoch 121/300\n",
      " - 21s - loss: 2.4535 - acc: 0.3350\n",
      "Testing loss: 2.6178, acc: 0.3174\n",
      "Epoch 122/300\n",
      " - 21s - loss: 2.4516 - acc: 0.3352\n",
      "Testing loss: 2.6087, acc: 0.3200\n",
      "Epoch 123/300\n",
      " - 21s - loss: 2.4525 - acc: 0.3350\n",
      "Testing loss: 2.6117, acc: 0.3178\n",
      "Epoch 124/300\n",
      " - 21s - loss: 2.4519 - acc: 0.3354\n",
      "Testing loss: 2.6133, acc: 0.3187\n",
      "Epoch 125/300\n",
      " - 21s - loss: 2.4515 - acc: 0.3352\n",
      "Testing loss: 2.6135, acc: 0.3189\n",
      "Epoch 126/300\n",
      " - 21s - loss: 2.4519 - acc: 0.3351\n",
      "Testing loss: 2.6183, acc: 0.3193\n",
      "Epoch 127/300\n",
      " - 21s - loss: 2.4515 - acc: 0.3353\n",
      "Testing loss: 2.6165, acc: 0.3183\n",
      "Epoch 128/300\n",
      " - 21s - loss: 2.4512 - acc: 0.3353\n",
      "Testing loss: 2.6079, acc: 0.3193\n",
      "Epoch 129/300\n",
      " - 21s - loss: 2.4520 - acc: 0.3353\n",
      "Testing loss: 2.6091, acc: 0.3182\n",
      "Epoch 130/300\n",
      " - 21s - loss: 2.4514 - acc: 0.3355\n",
      "Testing loss: 2.6135, acc: 0.3178\n",
      "Epoch 131/300\n",
      " - 21s - loss: 2.4515 - acc: 0.3352\n",
      "Testing loss: 2.6100, acc: 0.3187\n",
      "Epoch 132/300\n",
      " - 21s - loss: 2.4509 - acc: 0.3355\n",
      "Testing loss: 2.6094, acc: 0.3195\n",
      "Epoch 133/300\n",
      " - 21s - loss: 2.4511 - acc: 0.3354\n",
      "Testing loss: 2.6154, acc: 0.3175\n",
      "Epoch 134/300\n",
      " - 21s - loss: 2.4510 - acc: 0.3353\n",
      "Testing loss: 2.6163, acc: 0.3180\n",
      "Epoch 135/300\n",
      " - 21s - loss: 2.4510 - acc: 0.3353\n",
      "Testing loss: 2.6132, acc: 0.3184\n",
      "Epoch 136/300\n",
      " - 21s - loss: 2.4509 - acc: 0.3354\n",
      "Testing loss: 2.6118, acc: 0.3178\n",
      "Epoch 137/300\n",
      " - 21s - loss: 2.4512 - acc: 0.3351\n",
      "Testing loss: 2.6121, acc: 0.3176\n",
      "Epoch 138/300\n",
      " - 21s - loss: 2.4509 - acc: 0.3352\n",
      "Testing loss: 2.6148, acc: 0.3177\n",
      "Epoch 139/300\n",
      " - 21s - loss: 2.4506 - acc: 0.3354\n",
      "Testing loss: 2.6169, acc: 0.3180\n",
      "Epoch 140/300\n",
      " - 21s - loss: 2.4506 - acc: 0.3352\n",
      "Testing loss: 2.6217, acc: 0.3178\n",
      "Epoch 141/300\n",
      " - 21s - loss: 2.4509 - acc: 0.3352\n",
      "Testing loss: 2.6183, acc: 0.3175\n",
      "Epoch 142/300\n",
      " - 21s - loss: 2.4502 - acc: 0.3352\n",
      "Testing loss: 2.6158, acc: 0.3181\n",
      "Epoch 143/300\n",
      " - 21s - loss: 2.4502 - acc: 0.3355\n",
      "Testing loss: 2.6212, acc: 0.3179\n",
      "Epoch 144/300\n",
      " - 21s - loss: 2.4505 - acc: 0.3352\n",
      "Testing loss: 2.6133, acc: 0.3187\n",
      "Epoch 145/300\n",
      " - 21s - loss: 2.4508 - acc: 0.3351\n",
      "Testing loss: 2.6133, acc: 0.3185\n",
      "Epoch 146/300\n",
      " - 21s - loss: 2.4503 - acc: 0.3353\n",
      "Testing loss: 2.6155, acc: 0.3191\n",
      "Epoch 147/300\n",
      " - 21s - loss: 2.4502 - acc: 0.3352\n",
      "Testing loss: 2.6167, acc: 0.3177\n",
      "Epoch 148/300\n",
      " - 21s - loss: 2.4507 - acc: 0.3354\n",
      "Testing loss: 2.6144, acc: 0.3189\n",
      "Epoch 149/300\n",
      " - 21s - loss: 2.4496 - acc: 0.3354\n",
      "Testing loss: 2.6121, acc: 0.3190\n",
      "Epoch 150/300\n",
      " - 21s - loss: 2.4500 - acc: 0.3353\n",
      "Testing loss: 2.6118, acc: 0.3186\n",
      "Epoch 151/300\n",
      " - 21s - loss: 2.4496 - acc: 0.3355\n",
      "Testing loss: 2.6192, acc: 0.3179\n",
      "Epoch 152/300\n",
      " - 21s - loss: 2.4500 - acc: 0.3354\n",
      "Testing loss: 2.6149, acc: 0.3176\n",
      "Epoch 153/300\n",
      " - 21s - loss: 2.4502 - acc: 0.3356\n",
      "Testing loss: 2.6173, acc: 0.3178\n",
      "Epoch 154/300\n",
      " - 21s - loss: 2.4497 - acc: 0.3354\n",
      "Testing loss: 2.6155, acc: 0.3185\n",
      "Epoch 155/300\n",
      " - 21s - loss: 2.4495 - acc: 0.3354\n",
      "Testing loss: 2.6173, acc: 0.3191\n",
      "Epoch 156/300\n",
      " - 21s - loss: 2.4492 - acc: 0.3353\n",
      "Testing loss: 2.6195, acc: 0.3182\n",
      "Epoch 157/300\n",
      " - 21s - loss: 2.4494 - acc: 0.3354\n",
      "Testing loss: 2.6200, acc: 0.3175\n",
      "Epoch 158/300\n",
      " - 21s - loss: 2.4497 - acc: 0.3355\n",
      "Testing loss: 2.6144, acc: 0.3190\n",
      "Epoch 159/300\n",
      " - 21s - loss: 2.4491 - acc: 0.3356\n",
      "Testing loss: 2.6177, acc: 0.3191\n",
      "Epoch 160/300\n",
      " - 21s - loss: 2.4489 - acc: 0.3355\n",
      "Testing loss: 2.6161, acc: 0.3169\n",
      "Epoch 161/300\n",
      " - 21s - loss: 2.4492 - acc: 0.3355\n",
      "Testing loss: 2.6158, acc: 0.3179\n",
      "Epoch 162/300\n",
      " - 21s - loss: 2.4493 - acc: 0.3355\n",
      "Testing loss: 2.6179, acc: 0.3181\n",
      "Epoch 163/300\n",
      " - 21s - loss: 2.4495 - acc: 0.3356\n",
      "Testing loss: 2.6112, acc: 0.3188\n",
      "Epoch 164/300\n",
      " - 21s - loss: 2.4485 - acc: 0.3355\n",
      "Testing loss: 2.6175, acc: 0.3181\n",
      "Epoch 165/300\n",
      " - 21s - loss: 2.4495 - acc: 0.3355\n",
      "Testing loss: 2.6154, acc: 0.3185\n",
      "Epoch 166/300\n",
      " - 21s - loss: 2.4489 - acc: 0.3352\n",
      "Testing loss: 2.6108, acc: 0.3184\n",
      "Epoch 167/300\n",
      " - 21s - loss: 2.4490 - acc: 0.3355\n",
      "Testing loss: 2.6152, acc: 0.3176\n",
      "Epoch 168/300\n",
      " - 21s - loss: 2.4488 - acc: 0.3354\n",
      "Testing loss: 2.6123, acc: 0.3194\n",
      "Epoch 169/300\n",
      " - 21s - loss: 2.4484 - acc: 0.3355\n",
      "Testing loss: 2.6175, acc: 0.3174\n",
      "Epoch 170/300\n",
      " - 21s - loss: 2.4490 - acc: 0.3355\n",
      "Testing loss: 2.6195, acc: 0.3164\n",
      "Epoch 171/300\n",
      " - 21s - loss: 2.4489 - acc: 0.3355\n",
      "Testing loss: 2.6149, acc: 0.3176\n",
      "Epoch 172/300\n",
      " - 21s - loss: 2.4493 - acc: 0.3356\n",
      "Testing loss: 2.6188, acc: 0.3175\n",
      "Epoch 173/300\n",
      " - 21s - loss: 2.4490 - acc: 0.3354\n",
      "Testing loss: 2.6149, acc: 0.3186\n",
      "Epoch 174/300\n",
      " - 21s - loss: 2.4486 - acc: 0.3356\n",
      "Testing loss: 2.6155, acc: 0.3191\n",
      "Epoch 175/300\n",
      " - 21s - loss: 2.4485 - acc: 0.3353\n",
      "Testing loss: 2.6175, acc: 0.3185\n",
      "Epoch 176/300\n",
      " - 21s - loss: 2.4486 - acc: 0.3356\n",
      "Testing loss: 2.6151, acc: 0.3183\n",
      "Epoch 177/300\n",
      " - 21s - loss: 2.4486 - acc: 0.3356\n",
      "Testing loss: 2.6154, acc: 0.3178\n",
      "Epoch 178/300\n",
      " - 21s - loss: 2.4486 - acc: 0.3356\n",
      "Testing loss: 2.6171, acc: 0.3169\n",
      "Epoch 179/300\n",
      " - 21s - loss: 2.4486 - acc: 0.3356\n",
      "Testing loss: 2.6167, acc: 0.3162\n",
      "Epoch 180/300\n",
      " - 21s - loss: 2.4484 - acc: 0.3355\n",
      "Testing loss: 2.6176, acc: 0.3176\n",
      "Epoch 181/300\n",
      " - 21s - loss: 2.4480 - acc: 0.3356\n",
      "Testing loss: 2.6172, acc: 0.3187\n",
      "Epoch 182/300\n",
      " - 21s - loss: 2.4486 - acc: 0.3359\n",
      "Testing loss: 2.6188, acc: 0.3176\n",
      "Epoch 183/300\n",
      " - 21s - loss: 2.4478 - acc: 0.3354\n",
      "Testing loss: 2.6180, acc: 0.3173\n",
      "Epoch 184/300\n",
      " - 21s - loss: 2.4487 - acc: 0.3356\n",
      "Testing loss: 2.6149, acc: 0.3189\n",
      "Epoch 185/300\n",
      " - 21s - loss: 2.4480 - acc: 0.3356\n",
      "Testing loss: 2.6130, acc: 0.3175\n",
      "Epoch 186/300\n",
      " - 21s - loss: 2.4483 - acc: 0.3357\n",
      "Testing loss: 2.6158, acc: 0.3178\n",
      "Epoch 187/300\n",
      " - 21s - loss: 2.4479 - acc: 0.3354\n",
      "Testing loss: 2.6179, acc: 0.3182\n",
      "Epoch 188/300\n",
      " - 21s - loss: 2.4480 - acc: 0.3355\n",
      "Testing loss: 2.6191, acc: 0.3175\n",
      "Epoch 189/300\n",
      " - 21s - loss: 2.4478 - acc: 0.3358\n",
      "Testing loss: 2.6171, acc: 0.3184\n",
      "Epoch 190/300\n",
      " - 21s - loss: 2.4477 - acc: 0.3357\n",
      "Testing loss: 2.6164, acc: 0.3172\n",
      "Epoch 191/300\n",
      " - 21s - loss: 2.4475 - acc: 0.3357\n",
      "Testing loss: 2.6104, acc: 0.3192\n",
      "Epoch 192/300\n",
      " - 21s - loss: 2.4476 - acc: 0.3356\n",
      "Testing loss: 2.6215, acc: 0.3184\n",
      "Epoch 193/300\n",
      " - 21s - loss: 2.4474 - acc: 0.3357\n",
      "Testing loss: 2.6224, acc: 0.3175\n",
      "Epoch 194/300\n",
      " - 21s - loss: 2.4472 - acc: 0.3354\n",
      "Testing loss: 2.6173, acc: 0.3172\n",
      "Epoch 195/300\n",
      " - 21s - loss: 2.4476 - acc: 0.3356\n",
      "Testing loss: 2.6182, acc: 0.3176\n",
      "Epoch 196/300\n",
      " - 21s - loss: 2.4474 - acc: 0.3356\n",
      "Testing loss: 2.6182, acc: 0.3172\n",
      "Epoch 197/300\n",
      " - 21s - loss: 2.4473 - acc: 0.3357\n",
      "Testing loss: 2.6173, acc: 0.3185\n",
      "Epoch 198/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 21s - loss: 2.4471 - acc: 0.3358\n",
      "Testing loss: 2.6181, acc: 0.3176\n",
      "Epoch 199/300\n",
      " - 21s - loss: 2.4471 - acc: 0.3358\n",
      "Testing loss: 2.6194, acc: 0.3185\n",
      "Epoch 200/300\n",
      " - 21s - loss: 2.4470 - acc: 0.3357\n",
      "Testing loss: 2.6142, acc: 0.3184\n",
      "Epoch 201/300\n",
      " - 21s - loss: 2.4477 - acc: 0.3357\n",
      "Testing loss: 2.6183, acc: 0.3173\n",
      "Epoch 202/300\n",
      " - 21s - loss: 2.4472 - acc: 0.3357\n",
      "Testing loss: 2.6145, acc: 0.3196\n",
      "Epoch 203/300\n",
      " - 21s - loss: 2.4471 - acc: 0.3357\n",
      "Testing loss: 2.6170, acc: 0.3172\n",
      "Epoch 204/300\n",
      " - 21s - loss: 2.4469 - acc: 0.3357\n",
      "Testing loss: 2.6175, acc: 0.3177\n",
      "Epoch 205/300\n",
      " - 21s - loss: 2.4470 - acc: 0.3358\n",
      "Testing loss: 2.6225, acc: 0.3163\n",
      "Epoch 206/300\n",
      " - 21s - loss: 2.4471 - acc: 0.3355\n",
      "Testing loss: 2.6143, acc: 0.3181\n",
      "Epoch 207/300\n",
      " - 21s - loss: 2.4469 - acc: 0.3358\n",
      "Testing loss: 2.6153, acc: 0.3183\n",
      "Epoch 208/300\n",
      " - 21s - loss: 2.4465 - acc: 0.3357\n",
      "Testing loss: 2.6180, acc: 0.3177\n",
      "Epoch 209/300\n",
      " - 21s - loss: 2.4472 - acc: 0.3356\n",
      "Testing loss: 2.6181, acc: 0.3167\n",
      "Epoch 210/300\n",
      " - 21s - loss: 2.4467 - acc: 0.3356\n",
      "Testing loss: 2.6155, acc: 0.3179\n",
      "Epoch 211/300\n",
      " - 21s - loss: 2.4473 - acc: 0.3357\n",
      "Testing loss: 2.6183, acc: 0.3189\n",
      "Epoch 212/300\n",
      " - 21s - loss: 2.4465 - acc: 0.3357\n",
      "Testing loss: 2.6218, acc: 0.3165\n",
      "Epoch 213/300\n",
      " - 21s - loss: 2.4465 - acc: 0.3357\n",
      "Testing loss: 2.6162, acc: 0.3184\n",
      "Epoch 214/300\n",
      " - 21s - loss: 2.4468 - acc: 0.3357\n",
      "Testing loss: 2.6205, acc: 0.3186\n",
      "Epoch 215/300\n",
      " - 21s - loss: 2.4462 - acc: 0.3357\n",
      "Testing loss: 2.6213, acc: 0.3181\n",
      "Epoch 216/300\n",
      " - 21s - loss: 2.4468 - acc: 0.3356\n",
      "Testing loss: 2.6176, acc: 0.3178\n",
      "Epoch 217/300\n",
      " - 21s - loss: 2.4467 - acc: 0.3357\n",
      "Testing loss: 2.6187, acc: 0.3180\n",
      "Epoch 218/300\n",
      " - 21s - loss: 2.4468 - acc: 0.3357\n",
      "Testing loss: 2.6149, acc: 0.3176\n",
      "Epoch 219/300\n",
      " - 21s - loss: 2.4468 - acc: 0.3356\n",
      "Testing loss: 2.6199, acc: 0.3180\n",
      "Epoch 220/300\n",
      " - 21s - loss: 2.4461 - acc: 0.3358\n",
      "Testing loss: 2.6193, acc: 0.3185\n",
      "Epoch 221/300\n",
      " - 21s - loss: 2.4460 - acc: 0.3361\n",
      "Testing loss: 2.6193, acc: 0.3184\n",
      "Epoch 222/300\n",
      " - 21s - loss: 2.4462 - acc: 0.3360\n",
      "Testing loss: 2.6236, acc: 0.3167\n",
      "Epoch 223/300\n",
      " - 21s - loss: 2.4461 - acc: 0.3360\n",
      "Testing loss: 2.6219, acc: 0.3165\n",
      "Epoch 224/300\n",
      " - 21s - loss: 2.4463 - acc: 0.3357\n",
      "Testing loss: 2.6225, acc: 0.3177\n",
      "Epoch 225/300\n",
      " - 21s - loss: 2.4462 - acc: 0.3358\n",
      "Testing loss: 2.6194, acc: 0.3181\n",
      "Epoch 226/300\n",
      " - 21s - loss: 2.4461 - acc: 0.3357\n",
      "Testing loss: 2.6209, acc: 0.3180\n",
      "Epoch 227/300\n",
      " - 21s - loss: 2.4461 - acc: 0.3359\n",
      "Testing loss: 2.6195, acc: 0.3191\n",
      "Epoch 228/300\n",
      " - 21s - loss: 2.4459 - acc: 0.3357\n",
      "Testing loss: 2.6205, acc: 0.3175\n",
      "Epoch 229/300\n",
      " - 21s - loss: 2.4460 - acc: 0.3356\n",
      "Testing loss: 2.6214, acc: 0.3177\n",
      "Epoch 230/300\n",
      " - 21s - loss: 2.4461 - acc: 0.3359\n",
      "Testing loss: 2.6193, acc: 0.3180\n",
      "Epoch 231/300\n",
      " - 21s - loss: 2.4458 - acc: 0.3357\n",
      "Testing loss: 2.6189, acc: 0.3179\n",
      "Epoch 232/300\n",
      " - 21s - loss: 2.4459 - acc: 0.3360\n",
      "Testing loss: 2.6142, acc: 0.3184\n",
      "Epoch 233/300\n",
      " - 21s - loss: 2.4456 - acc: 0.3361\n",
      "Testing loss: 2.6212, acc: 0.3176\n",
      "Epoch 234/300\n",
      " - 21s - loss: 2.4466 - acc: 0.3355\n",
      "Testing loss: 2.6167, acc: 0.3177\n",
      "Epoch 235/300\n",
      " - 21s - loss: 2.4461 - acc: 0.3358\n",
      "Testing loss: 2.6178, acc: 0.3185\n",
      "Epoch 236/300\n",
      " - 21s - loss: 2.4460 - acc: 0.3356\n",
      "Testing loss: 2.6212, acc: 0.3174\n",
      "Epoch 237/300\n",
      " - 21s - loss: 2.4456 - acc: 0.3357\n",
      "Testing loss: 2.6200, acc: 0.3187\n",
      "Epoch 238/300\n",
      " - 21s - loss: 2.4457 - acc: 0.3360\n",
      "Testing loss: 2.6175, acc: 0.3178\n",
      "Epoch 239/300\n",
      " - 21s - loss: 2.4459 - acc: 0.3361\n",
      "Testing loss: 2.6179, acc: 0.3172\n",
      "Epoch 240/300\n",
      " - 21s - loss: 2.4459 - acc: 0.3358\n",
      "Testing loss: 2.6203, acc: 0.3177\n",
      "Epoch 241/300\n",
      " - 21s - loss: 2.4457 - acc: 0.3359\n",
      "Testing loss: 2.6162, acc: 0.3178\n",
      "Epoch 242/300\n",
      " - 21s - loss: 2.4456 - acc: 0.3356\n",
      "Testing loss: 2.6222, acc: 0.3177\n",
      "Epoch 243/300\n",
      " - 21s - loss: 2.4454 - acc: 0.3358\n",
      "Testing loss: 2.6164, acc: 0.3179\n",
      "Epoch 244/300\n",
      " - 21s - loss: 2.4457 - acc: 0.3359\n",
      "Testing loss: 2.6171, acc: 0.3185\n",
      "Epoch 245/300\n",
      " - 21s - loss: 2.4459 - acc: 0.3359\n",
      "Testing loss: 2.6169, acc: 0.3174\n",
      "Epoch 246/300\n",
      " - 21s - loss: 2.4456 - acc: 0.3360\n",
      "Testing loss: 2.6211, acc: 0.3168\n",
      "Epoch 247/300\n",
      " - 21s - loss: 2.4456 - acc: 0.3358\n",
      "Testing loss: 2.6218, acc: 0.3174\n",
      "Epoch 248/300\n",
      " - 21s - loss: 2.4452 - acc: 0.3360\n",
      "Testing loss: 2.6211, acc: 0.3178\n",
      "Epoch 249/300\n",
      " - 21s - loss: 2.4448 - acc: 0.3358\n",
      "Testing loss: 2.6208, acc: 0.3182\n",
      "Epoch 250/300\n",
      " - 21s - loss: 2.4455 - acc: 0.3359\n",
      "Testing loss: 2.6232, acc: 0.3174\n",
      "Epoch 251/300\n",
      " - 21s - loss: 2.4455 - acc: 0.3358\n",
      "Testing loss: 2.6215, acc: 0.3179\n",
      "Epoch 252/300\n",
      " - 21s - loss: 2.4451 - acc: 0.3361\n",
      "Testing loss: 2.6210, acc: 0.3171\n",
      "Epoch 253/300\n",
      " - 21s - loss: 2.4449 - acc: 0.3360\n",
      "Testing loss: 2.6210, acc: 0.3175\n",
      "Epoch 254/300\n",
      " - 21s - loss: 2.4453 - acc: 0.3359\n",
      "Testing loss: 2.6193, acc: 0.3181\n",
      "Epoch 255/300\n",
      " - 21s - loss: 2.4452 - acc: 0.3360\n",
      "Testing loss: 2.6199, acc: 0.3178\n",
      "Epoch 256/300\n",
      " - 21s - loss: 2.4449 - acc: 0.3361\n",
      "Testing loss: 2.6239, acc: 0.3183\n",
      "Epoch 257/300\n",
      " - 21s - loss: 2.4449 - acc: 0.3361\n",
      "Testing loss: 2.6213, acc: 0.3173\n",
      "Epoch 258/300\n",
      " - 21s - loss: 2.4457 - acc: 0.3358\n",
      "Testing loss: 2.6192, acc: 0.3175\n",
      "Epoch 259/300\n",
      " - 21s - loss: 2.4451 - acc: 0.3360\n",
      "Testing loss: 2.6223, acc: 0.3189\n",
      "Epoch 260/300\n",
      " - 21s - loss: 2.4448 - acc: 0.3359\n",
      "Testing loss: 2.6233, acc: 0.3169\n",
      "Epoch 261/300\n",
      " - 21s - loss: 2.4452 - acc: 0.3358\n",
      "Testing loss: 2.6177, acc: 0.3181\n",
      "Epoch 262/300\n",
      " - 21s - loss: 2.4450 - acc: 0.3361\n",
      "Testing loss: 2.6224, acc: 0.3180\n",
      "Epoch 263/300\n",
      " - 21s - loss: 2.4456 - acc: 0.3359\n",
      "Testing loss: 2.6176, acc: 0.3173\n",
      "Epoch 264/300\n",
      " - 21s - loss: 2.4450 - acc: 0.3357\n",
      "Testing loss: 2.6187, acc: 0.3177\n",
      "Epoch 265/300\n",
      " - 21s - loss: 2.4447 - acc: 0.3361\n",
      "Testing loss: 2.6212, acc: 0.3179\n",
      "Epoch 266/300\n",
      " - 21s - loss: 2.4450 - acc: 0.3359\n",
      "Testing loss: 2.6198, acc: 0.3182\n",
      "Epoch 267/300\n",
      " - 21s - loss: 2.4449 - acc: 0.3356\n",
      "Testing loss: 2.6199, acc: 0.3171\n",
      "Epoch 268/300\n",
      " - 21s - loss: 2.4446 - acc: 0.3359\n",
      "Testing loss: 2.6145, acc: 0.3190\n",
      "Epoch 269/300\n",
      " - 21s - loss: 2.4446 - acc: 0.3360\n",
      "Testing loss: 2.6225, acc: 0.3181\n",
      "Epoch 270/300\n",
      " - 21s - loss: 2.4451 - acc: 0.3360\n",
      "Testing loss: 2.6225, acc: 0.3175\n",
      "Epoch 271/300\n",
      " - 21s - loss: 2.4444 - acc: 0.3361\n",
      "Testing loss: 2.6230, acc: 0.3178\n",
      "Epoch 272/300\n",
      " - 21s - loss: 2.4443 - acc: 0.3359\n",
      "Testing loss: 2.6220, acc: 0.3181\n",
      "Epoch 273/300\n",
      " - 21s - loss: 2.4441 - acc: 0.3361\n",
      "Testing loss: 2.6160, acc: 0.3179\n",
      "Epoch 274/300\n",
      " - 21s - loss: 2.4443 - acc: 0.3358\n",
      "Testing loss: 2.6192, acc: 0.3175\n",
      "Epoch 275/300\n",
      " - 21s - loss: 2.4448 - acc: 0.3358\n",
      "Testing loss: 2.6218, acc: 0.3176\n",
      "Epoch 276/300\n",
      " - 21s - loss: 2.4440 - acc: 0.3359\n",
      "Testing loss: 2.6229, acc: 0.3176\n",
      "Epoch 277/300\n",
      " - 21s - loss: 2.4447 - acc: 0.3360\n",
      "Testing loss: 2.6201, acc: 0.3180\n",
      "Epoch 278/300\n",
      " - 21s - loss: 2.4447 - acc: 0.3359\n",
      "Testing loss: 2.6199, acc: 0.3177\n",
      "Epoch 279/300\n",
      " - 21s - loss: 2.4445 - acc: 0.3358\n",
      "Testing loss: 2.6221, acc: 0.3174\n",
      "Epoch 280/300\n",
      " - 21s - loss: 2.4444 - acc: 0.3360\n",
      "Testing loss: 2.6201, acc: 0.3185\n",
      "Epoch 281/300\n",
      " - 21s - loss: 2.4448 - acc: 0.3358\n",
      "Testing loss: 2.6190, acc: 0.3171\n",
      "Epoch 282/300\n",
      " - 21s - loss: 2.4445 - acc: 0.3361\n",
      "Testing loss: 2.6234, acc: 0.3166\n",
      "Epoch 283/300\n",
      " - 21s - loss: 2.4438 - acc: 0.3359\n",
      "Testing loss: 2.6200, acc: 0.3176\n",
      "Epoch 284/300\n",
      " - 21s - loss: 2.4445 - acc: 0.3360\n",
      "Testing loss: 2.6218, acc: 0.3165\n",
      "Epoch 285/300\n",
      " - 21s - loss: 2.4438 - acc: 0.3361\n",
      "Testing loss: 2.6189, acc: 0.3189\n",
      "Epoch 286/300\n",
      " - 21s - loss: 2.4441 - acc: 0.3357\n",
      "Testing loss: 2.6163, acc: 0.3182\n",
      "Epoch 287/300\n",
      " - 21s - loss: 2.4442 - acc: 0.3358\n",
      "Testing loss: 2.6200, acc: 0.3173\n",
      "Epoch 288/300\n",
      " - 21s - loss: 2.4442 - acc: 0.3362\n",
      "Testing loss: 2.6209, acc: 0.3179\n",
      "Epoch 289/300\n",
      " - 21s - loss: 2.4441 - acc: 0.3360\n",
      "Testing loss: 2.6238, acc: 0.3170\n",
      "Epoch 290/300\n",
      " - 21s - loss: 2.4443 - acc: 0.3359\n",
      "Testing loss: 2.6249, acc: 0.3167\n",
      "Epoch 291/300\n",
      " - 21s - loss: 2.4443 - acc: 0.3361\n",
      "Testing loss: 2.6202, acc: 0.3176\n",
      "Epoch 292/300\n",
      " - 21s - loss: 2.4443 - acc: 0.3360\n",
      "Testing loss: 2.6227, acc: 0.3170\n",
      "Epoch 293/300\n",
      " - 21s - loss: 2.4438 - acc: 0.3360\n",
      "Testing loss: 2.6185, acc: 0.3181\n",
      "Epoch 294/300\n",
      " - 21s - loss: 2.4441 - acc: 0.3360\n",
      "Testing loss: 2.6250, acc: 0.3163\n",
      "Epoch 295/300\n",
      " - 21s - loss: 2.4440 - acc: 0.3361\n",
      "Testing loss: 2.6231, acc: 0.3179\n",
      "Epoch 296/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 21s - loss: 2.4439 - acc: 0.3360\n",
      "Testing loss: 2.6237, acc: 0.3181\n",
      "Epoch 297/300\n",
      " - 21s - loss: 2.4440 - acc: 0.3361\n",
      "Testing loss: 2.6182, acc: 0.3176\n",
      "Epoch 298/300\n",
      " - 21s - loss: 2.4438 - acc: 0.3359\n",
      "Testing loss: 2.6268, acc: 0.3169\n",
      "Epoch 299/300\n",
      " - 21s - loss: 2.4443 - acc: 0.3363\n",
      "Testing loss: 2.6215, acc: 0.3181\n",
      "Epoch 300/300\n",
      " - 21s - loss: 2.4440 - acc: 0.3359\n",
      "Testing loss: 2.6190, acc: 0.3183\n",
      "train mrr: 0.48 | val mrr: 0.45\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "from keras.layers import concatenate, Dense, Dropout, Embedding, Input, Flatten, Conv1D, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import Callback\n",
    "# split_per = 0.1\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras import backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "for trn_ind, val_ind in skf.split(targets, targets):\n",
    "    trn_imp, val_imp = impressions[trn_ind], impressions[val_ind]\n",
    "    trn_price, val_price = prices[trn_ind], prices[val_ind]\n",
    "    trn_city, val_city = cities[trn_ind], cities[val_ind]\n",
    "    trn_plat, val_plat = platforms[trn_ind], platforms[val_ind]\n",
    "    trn_dev, val_dev = devices[trn_ind], devices[val_ind]\n",
    "    \n",
    "    # TEMP\n",
    "#     del impressions, prices, cities, platforms, devices\n",
    "#     gc.collect()\n",
    "    \n",
    "    y_trn, y_val = targets[trn_ind], targets[val_ind]\n",
    "    \n",
    "    # build model\n",
    "    # impressions\n",
    "    imp_input = Input(shape=(25, 100))\n",
    "    imp_conv = Conv1D(16, kernel_size=5, activation='relu')(imp_input)\n",
    "    imp_conv = BatchNormalization()(imp_conv)\n",
    "    imp_flatten = Flatten()(imp_conv)\n",
    "    # city embeddings\n",
    "    city_input = Input(shape = (1, ), dtype = 'int32')\n",
    "    city_embedding = Embedding(ncity, 20, input_length=1)(city_input)\n",
    "    city_embedding = Flatten()(city_embedding)\n",
    "    \n",
    "    # platform input \n",
    "    plat_input = Input(shape = (1, ), dtype = 'int32')\n",
    "    plat_embedding = Embedding(nplat, 10, input_length=1)(plat_input)\n",
    "    plat_embedding = Flatten()(plat_embedding)\n",
    "                       \n",
    "    # device\n",
    "    device_input =  Input(shape = (2, ))\n",
    "    \n",
    "    # price input\n",
    "    price_input =  Input(shape = (25, ))\n",
    "    \n",
    "    # concatenate\n",
    "    concat1 = concatenate([imp_flatten, price_input])\n",
    "    concat1 = BatchNormalization()(concat1)\n",
    "    concat1 = Dense(units=30, activation='relu')(concat1)\n",
    "    concat1 = Dropout(0.2)(concat1)\n",
    "    concat2 = concatenate([concat1, city_embedding, plat_embedding, device_input])\n",
    "    concat2 = BatchNormalization()(concat2)\n",
    "    concat2 = Dropout(0.2)(concat2)\n",
    "    \n",
    "    h = Dense(units=30, activation='relu')(concat2)\n",
    "    output_layer = Dense(25, activation='softmax')(h)\n",
    "\n",
    "\n",
    "    model = Model(inputs=[imp_input, city_input, plat_input, device_input, price_input], \n",
    "                  outputs=output_layer)\n",
    "\n",
    "    opt = optimizers.Adam(lr=0.001)\n",
    "    model.compile(optimizer = opt, loss = \"categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "#     model.compile(optimizer = opt, loss = \"categorical_crossentropy\", metrics=['accuracy', mrr])\n",
    "# model.compile(optimizer = opt, loss = \"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    # from clr import CyclicLR\n",
    "    from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "    from datetime import datetime as dt\n",
    "    model_file = 'test.model'\n",
    "\n",
    "    callbacks = [ModelCheckpoint(model_file, save_best_only=True, verbose=1)]\n",
    "    # callbacks.append(EarlyStopping(patience=150, verbose=1))\n",
    "    # callbacks.append(ReduceLROnPlateau(factor=0.5, patience=20, min_lr=5e-4, verbose=1))\n",
    "    log_dir = \"logs/{}\".format(dt.now().strftime('%m-%d-%H-%M'))\n",
    "    # tb = TensorBoard(log_dir=log_dir, histogram_freq=2, write_graph=True, write_grads=True, write_images=True,\n",
    "    #                  embeddings_freq=10, embeddings_layer_names=['embedding_1'], embeddings_data=next(val_gen))\n",
    "    tb = TensorBoard(log_dir=log_dir, write_graph=True, write_grads=True)\n",
    "    callbacks.append(tb)\n",
    "\n",
    "    \n",
    "    \n",
    "    batch_size = 256\n",
    "    n_epochs = 300\n",
    "    # keras requires 0, 1 binary label input\n",
    "    from keras.utils import to_categorical\n",
    "    train_y_binary = to_categorical(y_trn)\n",
    "    val_y_binary = to_categorical(y_val)\n",
    "\n",
    "    history = model.fit([trn_imp, trn_city, trn_plat, trn_dev, trn_price], \n",
    "                        train_y_binary, \n",
    "                        epochs=n_epochs, \n",
    "                        batch_size=batch_size,\n",
    "                        verbose = 2, \n",
    "                        shuffle = True,\n",
    "                        callbacks=callbacks+[TestCallback(([val_imp, val_city, val_plat, val_dev, val_price],\n",
    "                                                           val_y_binary))])\n",
    "    \n",
    "    # make predictions\n",
    "    trn_pred = model.predict([trn_imp, trn_city, trn_plat, trn_dev, trn_price])\n",
    "    trn_mrr = np.mean(1/(np.where(np.argsort(trn_pred)[:, ::-1] == y_trn.reshape(-1, 1))[1]+1))\n",
    "    \n",
    "    val_pred = model.predict([val_imp, val_city, val_plat, val_dev, val_price])\n",
    "    val_mrr = np.mean(1/(np.where(np.argsort(val_pred)[:, ::-1] == y_val.reshape(-1, 1))[1]+1))\n",
    "    print(f'train mrr: {trn_mrr:.2f} | val mrr: {val_mrr:.2f}')\n",
    "    \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train mrr: 0.47 | val mrr: 0.45"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
