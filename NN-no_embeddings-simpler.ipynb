{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1) Here we try to train nn with only meta ohe for impressions, prices and current_filters ohe,\\nwe run TCN on impressions, concatenate or separately run tcn with prices, \\nand concatenate with dense layer on top of ohe current_filters. We for now leave out, cities, countries, platforms\\n2) and possibly, do a bit preprocessing on the current filters, \\nuse the last longest current filters before clickouts (or the latest non-null current_filters), \\nwith indication of session size\\n3) whether there is a previous interacted action, if so and the index of previously interacted item \\nin current listings\\n4) the time spent before the click\\nWhat if we just submit the impression list as the predicitons\\n[impressions ind compare to previous ind]\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1) Here we try to train nn with only meta ohe for impressions, prices and current_filters ohe,\n",
    "we run TCN on impressions, concatenate or separately run tcn with prices, \n",
    "and concatenate with dense layer on top of ohe current_filters. We for now leave out, cities, countries, platforms\n",
    "2) and possibly, do a bit preprocessing on the current filters, \n",
    "use the last longest current filters before clickouts (or the latest non-null current_filters), \n",
    "with indication of session size\n",
    "3) whether there is a previous interacted action, if so and the index of previously interacted item \n",
    "in current listings\n",
    "4) the time spent before the click\n",
    "What if we just submit the impression list as the predicitons\n",
    "[impressions ind compare to previous ind]\n",
    "\"\"\"\n",
    "\n",
    "# 1) plot price location of target\n",
    "# 2) map interaction, image interaction etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime, time, os, gc, re, sys\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import ignore_warnings, load_data\n",
    "from clean_session import preprocess_sessions\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[05-05 19:12:20 - utils - load_data - INFO] Loading train using 1,000,000 rows which is 0.06% out of total train data\n"
     ]
    }
   ],
   "source": [
    "train = load_data('train', nrows=1000000)\n",
    "def fprint(df, name):\n",
    "    print(f'{name} shape: ({df.shape[0]:,}, {df.shape[1]})')\n",
    "    \n",
    "# fprint(train, 'raw train')\n",
    "# train = (train[(train['action_type'] == 'clickout item') & \n",
    "#                (train['impressions'].notna()) & \n",
    "#                (train['reference'].notna())]\n",
    "#          .reset_index(drop=True))\n",
    "\n",
    "# fprint(train, 'train after filtering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[05-05 19:12:22 - utils - preprocess_sessions - INFO] Dropping duplicates\n",
      "[05-05 19:12:22 - utils - remove_duplicates - INFO] Before dropping duplicates df shape: (1,000,000, 12)\n",
      "[05-05 19:12:23 - utils - remove_duplicates - INFO] After dropping duplicates df shape: (421,120, 12)\n",
      "[05-05 19:12:23 - utils - preprocess_sessions - INFO] Cliping session dataframe up to last click out (if there is clickout)\n",
      "[05-05 19:12:47 - utils - preprocess_sessions - INFO] filtering out sessions without clickouts, reference, or clickout is nan\n",
      "[05-05 19:12:47 - utils - preprocess_sessions - INFO] train length before filtering: 362,675\n"
     ]
    }
   ],
   "source": [
    "train = preprocess_sessions(train, recompute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['timestamp'] = train['timestamp'].apply(lambda t: datetime.datetime.utcfromtimestamp(t))\n",
    "usecols =['user_id', 'session_id', 'timestamp', 'step', 'action_type', 'current_filters', 'reference',\n",
    "          'impressions', 'prices']\n",
    "train = train[usecols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train.timestamp[1] - train.timestamp[0]).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now we only deal with the last row of each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def session_duration(ts):\n",
    "    if len(ts) == 1:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return (ts.max() - ts.min()).total_seconds()\n",
    "    \n",
    "def dwell_time_prior_clickout(ts):\n",
    "    if len(ts) == 1:\n",
    "        return np.nan\n",
    "    else:\n",
    "        ts_sorted = ts.sort_values()\n",
    "        return (ts_sorted.iloc[-1] - ts_sorted.iloc[-2]).total_seconds()\n",
    "\n",
    "def last_filters(cf):\n",
    "    mask = cf.notna()\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return cf[mask].iloc[-1]\n",
    "    \n",
    "def last_reference_id(rids):\n",
    "    mask = rids.notna()\n",
    "    if mask.sum() <= 1:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return rids[mask].iloc[-2] # the second last i.e. the one before click out\n",
    "\n",
    "\n",
    "    \n",
    "aggs = {'timestamp': [session_duration, dwell_time_prior_clickout],\n",
    "        'current_filters': [last_filters],\n",
    "        'session_id': 'size',\n",
    "        'reference': [last_reference_id]}\n",
    "session_grp = train.groupby('session_id')\n",
    "session_fts = session_grp.agg(aggs)\n",
    "session_fts.columns = ['_'.join(col).strip() for col in session_fts.columns.values]\n",
    "session_fts.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_fts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_fts.current_filters_last_filters.notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.current_filters.notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from clean_session import preprocess_sessions\n",
    "# # train = preprocess_sessions(train,data_source='data')\n",
    "# train = preprocess_sessions(None,data_source='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_last = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train, session_fts, on='session_id')\n",
    "fprint(train, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "train = train.groupby('session_id').last().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.session_id_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current filters\n",
    "train['cfs'] = train['current_filters_last_filters'].str.lower().str.split('|')\n",
    "train['ncfs'] = train['cfs'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add prices split now after shrink (otherwise prices being list cannot get shrinked)\n",
    "# prices\n",
    "train['prices'] = train['prices'].str.split('|')\n",
    "train['prices'] = train['prices'].apply(lambda x: [int(p) for p in x])\n",
    "# # pad it\n",
    "train['prices'] = train.prices.apply(lambda x: np.pad(x, (0, 25-len(x)), mode='constant'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of impressions\n",
    "train['nimps'] = train['impressions'].str.split('|').str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# impressions\n",
    "train['impressions'] = train['impressions'].str.split('|')\n",
    "# convert impression id to int\n",
    "train['impressions'] = train['impressions'].apply(lambda x: [int(i) for i in x])\n",
    "train['reference'] = train['reference'].astype(int)\n",
    "# train['reference_last_reference_id'] = train['reference_last_reference_id'].astype(int)\n",
    "# pad to 25 len\n",
    "train['impressions'] = train['impressions'].apply(lambda x: np.pad(x, (0, 25-len(x)), mode='constant'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_item_ids = len(set(np.concatenate(train['impressions'].values)))\n",
    "# n_item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out nan rows with reference_id not in impressions list, since if the true target in test\n",
    "# is not in the impression list then it would not get evaluated\n",
    "def assign_target(row):\n",
    "    ref = row['reference']\n",
    "    imp = list(row['impressions'])\n",
    "    if ref in imp:\n",
    "        return imp.index(ref)\n",
    "    else:\n",
    "        return np.nan\n",
    "def assign_last_ref_id(row):\n",
    "    ref = row['reference_last_reference_id']\n",
    "    imp = [str(i) for i in row['impressions']]\n",
    "    if pd.isna(ref):\n",
    "        return np.nan\n",
    "    else:\n",
    "        if ref in imp:\n",
    "            return imp.index(ref)\n",
    "        else:\n",
    "            return np.nan \n",
    "    \n",
    "train['target'] = train.apply(assign_target, axis=1)\n",
    "fprint(train, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train.session_id_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['last_ref_ind'] = train.apply(assign_last_ref_id, axis=1)\n",
    "fprint(train, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['last_ref_ind'].notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'drop number of rows reference id not in impressions list: {train.target.isna().sum()}')\n",
    "# drop the ones whose reference is not in the impression list\n",
    "train = train[train['target'].notna()].reset_index(drop=True)\n",
    "train['target'] = train['target'].astype(int)\n",
    "fprint(train, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the target distribution\n",
    "pd.value_counts(train['target']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.session_id_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = load_data('item_metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['properties'] = meta_df['properties'].str.lower().str.split('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_properties = np.concatenate(meta_df['properties'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(all_properties).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_properties = list(set(all_properties))\n",
    "property2natural = {v: k for k, v in enumerate(unique_properties)}\n",
    "del all_properties\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_properties = len(unique_properties)\n",
    "n_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['properties'] = meta_df['properties'].apply(lambda ps: [property2natural[p] for p in ps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['properties'] = meta_df['properties'].apply(lambda ps: np.sum(np.eye(n_properties, dtype=int)[ps], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_mapping = dict(meta_df[['item_id', 'properties']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a mapping for the padded values\n",
    "meta_mapping[0] = np.zeros(n_properties, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del meta_df, unique_properties, property2natural\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train['impressions'] = (train['impressions']\n",
    "                        .apply(lambda imps: np.vstack([meta_mapping[i] \n",
    "                                                      if i in meta_mapping.keys()\n",
    "                                                      else np.zeros(n_properties, dtype=int) \n",
    "                                                      for i in imps])))\n",
    "del meta_mapping\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_filters\n",
    "all_cfs = np.concatenate(train['cfs'].dropna().values)\n",
    "pd.value_counts(all_cfs).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(all_cfs, normalize=True).cumsum().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cfs = list(set(all_cfs))\n",
    "cfs_mapping = {v: k for k, v in enumerate(unique_cfs)}\n",
    "n_cfs = len(unique_cfs)\n",
    "n_cfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['cfs'].notna(), 'cfs'] = (train.loc[train['cfs'].notna(), 'cfs']\n",
    "                                          .apply(lambda cfs: [cfs_mapping[cf] for cf in cfs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train['cfs'] = (train['cfs'].apply(lambda cfs: np.sum(np.eye(n_cfs, dtype=int)[cfs], axis=0) \n",
    "                                   if type(cfs) ==list else np.zeros(n_cfs, dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cfs_mapping\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.session_id_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe normalize to percentage within each records, check does each item_id have the same price over all records\n",
    "def normalize(ps):\n",
    "    p_arr = np.array(ps)\n",
    "    return p_arr/(p_arr.max())\n",
    "\n",
    "train['prices'] = train['prices'].apply(normalize)\n",
    "# PRICES\n",
    "prices = np.array(list(train['prices'].values))\n",
    "del train['prices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPRESSIONS\n",
    "impressions = np.array(list(train['impressions'].values))\n",
    "del train['impressions']\n",
    "\n",
    "# CURRENT_FILTERS\n",
    "cfilters = np.array(list(train['cfs'].values))\n",
    "del train['cfs']\n",
    "\n",
    "# numerics\n",
    "num_cols = ['session_id_size', 'timestamp_dwell_time_prior_clickout', 'last_ref_ind']\n",
    "for c in num_cols:\n",
    "    train[c] = train[c].fillna(-1)\n",
    "    \n",
    "numerics = train[num_cols].values\n",
    "# train = train.drop(num_cols, axis=1)\n",
    "\n",
    "# TARGETS\n",
    "targets = train['target'].values\n",
    "del train['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impressions = impressions.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.session_id_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.hstack(impressions).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.isnan(impressions).sum()\n",
    "for i in impressions:\n",
    "    n = np.isneginf(i).sum()\n",
    "    if n!=0:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(numerics, impressions, prices, cfilters, targets,\n",
    "                        batch_size, shuffle=True):\n",
    "    # default we will shuffle\n",
    "    indices = np.arange(len(targets))\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        \n",
    "        remainder = len(targets) % batch_size\n",
    "        for start_idx in range(0, len(targets), batch_size):\n",
    "            if remainder !=0 and start_idx + batch_size >= len(targets):\n",
    "                excerpt = indices[len(targets)-batch_size:len(targets)]\n",
    "            else:\n",
    "                excerpt = indices[start_idx:start_idx+batch_size]\n",
    "\n",
    "            numerics_batch = numerics[excerpt]\n",
    "            impressions_batch = impressions[excerpt]#.reshape(batch_size, -1, 157)\n",
    "            prices_batch = prices[excerpt]#.reshape(batch_size, -1, 1)\n",
    "            cfilters_batch = cfilters[excerpt]\n",
    "            targets_batch = targets[excerpt]\n",
    "#             print('\\n>>>>>>>>>>>>>>', impressions_batch[0].shape, '<<<<<<<<<<<<<<<<<<\\n')\n",
    "#             impressions_batch = np.array([i.reshape(-1, 1) for i in impressions_batch])\n",
    "            prices_batch = np.array([i.reshape(-1, 1) for i in prices_batch])\n",
    "            yield ([numerics_batch, impressions_batch, prices_batch, \n",
    "                    cfilters_batch], targets_batch)\n",
    "#             yield ([numerics_batch, \n",
    "#                     cfilters_batch], targets_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # return [numerics_batch, impressions_batch, prices_batch[:, :, None], cfilters_batch]\n",
    "# train_gen = iterate_minibatches(trn_numerics, trn_imp, trn_price, trn_cfilter, y_trn, \n",
    "#                                 batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in train_gen:\n",
    "# #     i.reshape(-1, 1)\n",
    "# #     for j in i[0]:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_gen:\n",
    "#     for j in i[0]:\n",
    "# #         print(j[0].shape)\n",
    "#         print(j.shape, j[0].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_gen:\n",
    "#     pass\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in [trn_imp, trn_price, trn_cfilter, trn_city, trn_country, trn_plat, trn_dev]:\n",
    "#     print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trn_cfilter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in impressions:\n",
    "# #     print(i.shape)\n",
    "#     if i.shape[1]!=157 or i.shape[0]<2:\n",
    "#         print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from datetime import datetime as dt\n",
    "from nn_model_simple import build_model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "batch_size = 128\n",
    "n_epochs = 50\n",
    "\n",
    "skf = StratifiedKFold(n_splits=6)\n",
    "\n",
    "\n",
    "for trn_ind, val_ind in skf.split(targets, targets):\n",
    "    trn_numerics, val_numerics = numerics[trn_ind], numerics[val_ind]\n",
    "    trn_imp, val_imp = impressions[trn_ind], impressions[val_ind]\n",
    "    trn_price, val_price = prices[trn_ind], prices[val_ind]\n",
    "    trn_cfilter, val_cfilter = cfilters[trn_ind], cfilters[val_ind]\n",
    "    y_trn, y_val = targets[trn_ind], targets[val_ind]\n",
    "    \n",
    "    # create data generator numerics, impressions, prices, cfilters, targets, batchsize\n",
    "    # return [numerics_batch, impressions_batch, prices_batch[:, :, None], cfilters_batch]\n",
    "    train_gen = iterate_minibatches(trn_numerics, trn_imp, trn_price, trn_cfilter, y_trn, \n",
    "                                    batch_size, shuffle=True)\n",
    "    \n",
    "    val_gen = iterate_minibatches(val_numerics, val_imp, val_price, val_cfilter, y_val, \n",
    "                                  batch_size, shuffle=False)\n",
    "#     TEMP\n",
    "#     del impressions, prices, cities, platforms, devices\n",
    "#     gc.collect()\n",
    "    \n",
    "    # =====================================================================================\n",
    "    # create model\n",
    "    model = build_model(n_cfs, batch_size)\n",
    "    \n",
    "    # print out model info\n",
    "    nparams = model.count_params()\n",
    "    print((f'train len: {len(y_trn):,} | val len: {len(y_val):,} '\n",
    "           f'| numer of parameters: {nparams:,} | train_len/nparams={len(y_trn)/nparams:.5f}'))\n",
    "#     print(model.summary())\n",
    "#     plot_model(model, to_file='model.png')\n",
    "    # add some callbacks\n",
    "    callbacks = []\n",
    "    model_file = 'test.model'\n",
    "    callbacks = [ModelCheckpoint(model_file, save_best_only=True, verbose=1)]\n",
    "    log_dir = \"logs/{}\".format(dt.now().strftime('%m-%d-%H-%M'))\n",
    "    tb = TensorBoard(log_dir=log_dir, write_graph=True, write_grads=True)\n",
    "    callbacks.append(tb)\n",
    "    # simple early stopping\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', patience=100, verbose=1)\n",
    "    callbacks.append(es)\n",
    "    # rp\n",
    "    rp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=100, verbose=1)\n",
    "    callbacks.append(rp)\n",
    "    # add mrr callback\n",
    "#     callbacks.append(IntervalEvaluation())\n",
    "    \n",
    "    history = model.fit_generator(train_gen, \n",
    "                                  steps_per_epoch=len(y_trn)//batch_size, \n",
    "                                  epochs=n_epochs, \n",
    "                                  verbose=1,\n",
    "                                  callbacks=callbacks, \n",
    "                                  validation_data=val_gen, \n",
    "                                  validation_steps=len(y_val)//batch_size)\n",
    "\n",
    "    # make prediction\n",
    "#      [numerics_batch, impressions_batch, prices_batch[:, :, None], cfilters_batch]\n",
    "    trn_pred = model.predict(x=[trn_numerics, trn_imp, trn_price[:, :, None], trn_cfilter], batch_size=1024)\n",
    "    trn_pred_label = np.where(np.argsort(trn_pred)[:, ::-1] == y_trn.reshape(-1, 1))[1]\n",
    "    trn_mrr = np.mean(1/(trn_pred_label+1))\n",
    "\n",
    "    val_pred = model.predict(x=[val_numerics, val_imp, val_price[:, :, None], val_cfilter], batch_size=1024)\n",
    "    val_pred_label = np.where(np.argsort(val_pred)[:, ::-1] == y_val.reshape(-1, 1))[1]\n",
    "    val_mrr = np.mean(1/(val_pred_label+1))\n",
    "    print(f'train mrr: {trn_mrr:.2f} | val mrr: {val_mrr:.2f}')\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(trn_pred_label, bins=50, label='train_pred', alpha=0.7)\n",
    "_ = plt.hist(y_trn, bins=50, label = 'train label', alpha=0.7)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(val_pred_label, bins=50, label='val_pred', alpha=0.7)\n",
    "_ = plt.hist(y_val, bins=50, label = 'val label', alpha=0.7)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train mrr: 0.47 | val mrr: 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add mrr at per epochs probably\n",
    "# look at no embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_pred, y_true, normalize='row', level=0, log_scale=False):\n",
    "    compare = pd.DataFrame({'prediction': y_pred, 'y_true': y_true})\n",
    "    counts = compare.groupby('y_true')['prediction'].value_counts()\n",
    "    mat = counts.unstack(level=0)\n",
    "    mat.fillna(0, inplace=True)\n",
    "    \n",
    "    if normalize == 'row':\n",
    "        row_sum = mat.sum(axis=1)\n",
    "        mat = mat.div(row_sum, axis=0)\n",
    "        log_scale = False\n",
    "    elif normalize == 'column':\n",
    "        col_sum = mat.sum(axis=0)\n",
    "        mat = mat.div(col_sum, axis=1)\n",
    "        log_scale = False\n",
    "    # plot\n",
    "    fig = plt.figure(figsize=(35,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    if log_scale:\n",
    "        cax = ax.matshow(np.log1p(mat), interpolation='nearest')#, cmap='coolwarm')#, aspect='auto')\n",
    "    else:\n",
    "        cax = ax.matshow(mat, interpolation='nearest')#, cmap='coolwarm')#, aspect='auto')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xlabel(f'{mat.columns.name}')\n",
    "    ax.xaxis.set_label_position('top') \n",
    "    ax.set_ylabel(f'{mat.index.name}')\n",
    "    \n",
    "    ax.set_xticks(np.arange(mat.shape[1]))\n",
    "    ax.set_xticklabels(list(mat.columns.astype(str)), rotation=90)\n",
    "    ax.set_yticks(np.arange(mat.shape[0]))\n",
    "    _ = ax.set_yticklabels(list(mat.index.astype(str)))\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(trn_pred_label, y_trn, normalize=False, level=0, log_scale=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(trn_pred_label, y_trn, normalize='row')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(trn_pred_label, y_trn, normalize='column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(val_pred_label, y_val, level=0, normalize=None, log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
