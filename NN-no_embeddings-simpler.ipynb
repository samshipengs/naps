{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# 1) Here we try to train nn with only meta ohe for impressions, prices and current_filters ohe,\n",
    "# we run TCN on impressions, concatenate or separately run tcn with prices, \n",
    "# and concatenate with dense layer on top of ohe current_filters. We for now leave out, cities, countries, platforms\n",
    "# 2) and possibly, do a bit preprocessing on the current filters, \n",
    "# use the last longest current filters before clickouts (or the latest non-null current_filters), \n",
    "# with indication of session size\n",
    "# 3) whether there is a previous interacted action, if so and the index of previously interacted item \n",
    "# in current listings\n",
    "# 4) the time spent before the click\n",
    "# What if we just submit the impression list as the predicitons\n",
    "# [impressions ind compare to previous ind]\n",
    "# \"\"\"\n",
    "\n",
    "# \"\"\"\n",
    "# 1) look at distribution of prior time, and price\n",
    "# 2) tcn receptive field\n",
    "# \"\"\"\n",
    "\n",
    "# 1) plot price location of target\n",
    "# 2) map interaction, image interaction etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime, time, os, gc, re, sys\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import ignore_warnings, load_data\n",
    "from clean_session import preprocess_sessions\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[05-06 07:40:27 - utils - load_data - INFO] Loading train using 1,000,000 rows which is 6.28% out of total train data\n"
     ]
    }
   ],
   "source": [
    "train = load_data('train', nrows=1000000)\n",
    "def fprint(df, name):\n",
    "    print(f'{name} shape: ({df.shape[0]:,}, {df.shape[1]})')\n",
    "    \n",
    "# fprint(train, 'raw train')\n",
    "# train = (train[(train['action_type'] == 'clickout item') & \n",
    "#                (train['impressions'].notna()) & \n",
    "#                (train['reference'].notna())]\n",
    "#          .reset_index(drop=True))\n",
    "\n",
    "# fprint(train, 'train after filtering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[05-06 07:40:29 - utils - preprocess_sessions - INFO] Dropping duplicates\n",
      "[05-06 07:40:29 - utils - remove_duplicates - INFO] Before dropping duplicates df shape: (1,000,000, 12)\n",
      "[05-06 07:40:30 - utils - remove_duplicates - INFO] After dropping duplicates df shape: (421,120, 12)\n",
      "[05-06 07:40:30 - utils - preprocess_sessions - INFO] Cliping session dataframe up to last click out (if there is clickout)\n",
      "[05-06 07:40:53 - utils - preprocess_sessions - INFO] filtering out sessions without clickouts, reference, or clickout is nan\n",
      "[05-06 07:40:53 - utils - preprocess_sessions - INFO] train length before filtering: 362,675\n",
      "[05-06 07:41:41 - utils - preprocess_sessions - INFO] train length after filtering: 326,695\n",
      "[05-06 07:41:41 - utils - preprocess_sessions - INFO] Saving ./cache/preprocessed_train.snappy\n"
     ]
    }
   ],
   "source": [
    "train = preprocess_sessions(train, recompute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['timestamp'] = train['timestamp'].apply(lambda t: datetime.datetime.utcfromtimestamp(t))\n",
    "usecols =['user_id', 'session_id', 'timestamp', 'step', 'action_type', 'current_filters', 'reference',\n",
    "          'impressions', 'prices']\n",
    "train = train[usecols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train.timestamp[1] - train.timestamp[0]).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now we only deal with the last row of each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def session_duration(ts):\n",
    "    if len(ts) == 1:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return (ts.max() - ts.min()).total_seconds()\n",
    "    \n",
    "def dwell_time_prior_clickout(ts):\n",
    "    if len(ts) == 1:\n",
    "        return np.nan\n",
    "    else:\n",
    "        ts_sorted = ts.sort_values()\n",
    "        return (ts_sorted.iloc[-1] - ts_sorted.iloc[-2]).total_seconds()\n",
    "\n",
    "def last_filters(cf):\n",
    "    mask = cf.notna()\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return cf[mask].iloc[-1]\n",
    "    \n",
    "def last_reference_id(rids):\n",
    "    mask = rids.notna()\n",
    "    if mask.sum() <= 1:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return rids[mask].iloc[-2] # the second last i.e. the one before click out\n",
    "\n",
    "\n",
    "    \n",
    "aggs = {'timestamp': [session_duration, dwell_time_prior_clickout],\n",
    "        'current_filters': [last_filters],\n",
    "        'session_id': 'size',\n",
    "        'reference': [last_reference_id]}\n",
    "session_grp = train.groupby('session_id')\n",
    "session_fts = session_grp.agg(aggs)\n",
    "session_fts.columns = ['_'.join(col).strip() for col in session_fts.columns.values]\n",
    "session_fts.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp_session_duration</th>\n",
       "      <th>timestamp_dwell_time_prior_clickout</th>\n",
       "      <th>current_filters_last_filters</th>\n",
       "      <th>session_id_size</th>\n",
       "      <th>reference_last_reference_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000263df674fa</td>\n",
       "      <td>293.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>8249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002ccee5a980</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>109489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00033ccbbcdd4</td>\n",
       "      <td>197.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>3854580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0005174c23dcb</td>\n",
       "      <td>149.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>5132346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0005f5625af6a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      session_id  timestamp_session_duration  \\\n",
       "0  000263df674fa                       293.0   \n",
       "1  0002ccee5a980                         1.0   \n",
       "2  00033ccbbcdd4                       197.0   \n",
       "3  0005174c23dcb                       149.0   \n",
       "4  0005f5625af6a                         NaN   \n",
       "\n",
       "   timestamp_dwell_time_prior_clickout current_filters_last_filters  \\\n",
       "0                                236.0                          NaN   \n",
       "1                                  1.0                          NaN   \n",
       "2                                 45.0                          NaN   \n",
       "3                                 20.0                          NaN   \n",
       "4                                  NaN                          NaN   \n",
       "\n",
       "   session_id_size reference_last_reference_id  \n",
       "0                5                        8249  \n",
       "1                2                      109489  \n",
       "2                7                     3854580  \n",
       "3                4                     5132346  \n",
       "4                1                         NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_fts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11508"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_fts.current_filters_last_filters.notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61886"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.current_filters.notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(326695, 9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from clean_session import preprocess_sessions\n",
    "# # train = preprocess_sessions(train,data_source='data')\n",
    "# train = preprocess_sessions(None,data_source='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_last = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (326,695, 14)\n"
     ]
    }
   ],
   "source": [
    "train = pd.merge(train, session_fts, on='session_id')\n",
    "fprint(train, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>step</th>\n",
       "      <th>action_type</th>\n",
       "      <th>current_filters</th>\n",
       "      <th>reference</th>\n",
       "      <th>impressions</th>\n",
       "      <th>prices</th>\n",
       "      <th>timestamp_session_duration</th>\n",
       "      <th>timestamp_dwell_time_prior_clickout</th>\n",
       "      <th>current_filters_last_filters</th>\n",
       "      <th>session_id_size</th>\n",
       "      <th>reference_last_reference_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C2D58CUH9AOT</td>\n",
       "      <td>000263df674fa</td>\n",
       "      <td>2018-11-05 20:31:45</td>\n",
       "      <td>1</td>\n",
       "      <td>search for destination</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Orlando, USA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>293.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>8249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C2D58CUH9AOT</td>\n",
       "      <td>000263df674fa</td>\n",
       "      <td>2018-11-05 20:31:47</td>\n",
       "      <td>2</td>\n",
       "      <td>search for destination</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Orlando, USA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>293.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>8249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C2D58CUH9AOT</td>\n",
       "      <td>000263df674fa</td>\n",
       "      <td>2018-11-05 20:32:08</td>\n",
       "      <td>3</td>\n",
       "      <td>search for item</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>293.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>8249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C2D58CUH9AOT</td>\n",
       "      <td>000263df674fa</td>\n",
       "      <td>2018-11-05 20:32:42</td>\n",
       "      <td>4</td>\n",
       "      <td>clickout item</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8249</td>\n",
       "      <td>8249|8250|14378|67361|8284|8257|1955219|8254|6...</td>\n",
       "      <td>74|88|87|72|56|87|244|168|123|50|129|72|151|83...</td>\n",
       "      <td>293.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>8249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C2D58CUH9AOT</td>\n",
       "      <td>000263df674fa</td>\n",
       "      <td>2018-11-05 20:36:38</td>\n",
       "      <td>5</td>\n",
       "      <td>clickout item</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8249</td>\n",
       "      <td>8249|8250|14378|67361|8284|8257|1955219|8254|6...</td>\n",
       "      <td>74|88|87|72|56|87|244|168|123|50|129|72|151|83...</td>\n",
       "      <td>293.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>8249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id     session_id           timestamp  step  \\\n",
       "0  C2D58CUH9AOT  000263df674fa 2018-11-05 20:31:45     1   \n",
       "1  C2D58CUH9AOT  000263df674fa 2018-11-05 20:31:47     2   \n",
       "2  C2D58CUH9AOT  000263df674fa 2018-11-05 20:32:08     3   \n",
       "3  C2D58CUH9AOT  000263df674fa 2018-11-05 20:32:42     4   \n",
       "4  C2D58CUH9AOT  000263df674fa 2018-11-05 20:36:38     5   \n",
       "\n",
       "              action_type current_filters     reference  \\\n",
       "0  search for destination             NaN  Orlando, USA   \n",
       "1  search for destination             NaN  Orlando, USA   \n",
       "2         search for item             NaN          8249   \n",
       "3           clickout item             NaN          8249   \n",
       "4           clickout item             NaN          8249   \n",
       "\n",
       "                                         impressions  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  8249|8250|14378|67361|8284|8257|1955219|8254|6...   \n",
       "4  8249|8250|14378|67361|8284|8257|1955219|8254|6...   \n",
       "\n",
       "                                              prices  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  74|88|87|72|56|87|244|168|123|50|129|72|151|83...   \n",
       "4  74|88|87|72|56|87|244|168|123|50|129|72|151|83...   \n",
       "\n",
       "   timestamp_session_duration  timestamp_dwell_time_prior_clickout  \\\n",
       "0                       293.0                                236.0   \n",
       "1                       293.0                                236.0   \n",
       "2                       293.0                                236.0   \n",
       "3                       293.0                                236.0   \n",
       "4                       293.0                                236.0   \n",
       "\n",
       "  current_filters_last_filters  session_id_size reference_last_reference_id  \n",
       "0                          NaN                5                        8249  \n",
       "1                          NaN                5                        8249  \n",
       "2                          NaN                5                        8249  \n",
       "3                          NaN                5                        8249  \n",
       "4                          NaN                5                        8249  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "train = train.groupby('session_id').last().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.session_id_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current filters\n",
    "train['cfs'] = train['current_filters_last_filters'].str.lower().str.split('|')\n",
    "train['ncfs'] = train['cfs'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add prices split now after shrink (otherwise prices being list cannot get shrinked)\n",
    "# prices\n",
    "train['prices'] = train['prices'].str.split('|')\n",
    "train['prices'] = train['prices'].apply(lambda x: [int(p) for p in x])\n",
    "# # pad it\n",
    "# train['prices'] = train.prices.apply(lambda x: np.pad(x, (0, 25-len(x)), mode='constant'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of impressions\n",
    "train['nimps'] = train['impressions'].str.split('|').str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 275 ms, sys: 36 ms, total: 311 ms\n",
      "Wall time: 311 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# impressions\n",
    "train['impressions'] = train['impressions'].str.split('|')\n",
    "# convert impression id to int\n",
    "train['impressions'] = train['impressions'].apply(lambda x: [int(i) for i in x])\n",
    "train['reference'] = train['reference'].astype(int)\n",
    "# train['reference_last_reference_id'] = train['reference_last_reference_id'].astype(int)\n",
    "# pad to 25 len\n",
    "# train['impressions'] = train['impressions'].apply(lambda x: np.pad(x, (0, 25-len(x)), mode='constant'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_item_ids = len(set(np.concatenate(train['impressions'].values)))\n",
    "# n_item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (51,962, 18)\n"
     ]
    }
   ],
   "source": [
    "# filter out nan rows with reference_id not in impressions list, since if the true target in test\n",
    "# is not in the impression list then it would not get evaluated\n",
    "def assign_target(row):\n",
    "    ref = row['reference']\n",
    "    imp = list(row['impressions'])\n",
    "    if ref in imp:\n",
    "        return imp.index(ref)\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def assign_last_ref_id(row):\n",
    "    ref = row['reference_last_reference_id']\n",
    "    imp = [str(i) for i in row['impressions']]\n",
    "    if pd.isna(ref):\n",
    "        return np.nan\n",
    "    else:\n",
    "        if ref in imp:\n",
    "            return imp.index(ref)\n",
    "        else:\n",
    "            return np.nan \n",
    "    \n",
    "train['target'] = train.apply(assign_target, axis=1)\n",
    "fprint(train, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train.session_id_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (51,962, 19)\n"
     ]
    }
   ],
   "source": [
    "train['last_ref_ind'] = train.apply(assign_last_ref_id, axis=1)\n",
    "fprint(train, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27926"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['last_ref_ind'].notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51962, 19)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop number of rows reference id not in impressions list: 17\n",
      "train shape: (51,945, 19)\n"
     ]
    }
   ],
   "source": [
    "print(f'drop number of rows reference id not in impressions list: {train.target.isna().sum()}')\n",
    "# drop the ones whose reference is not in the impression list\n",
    "train = train[train['target'].notna()].reset_index(drop=True)\n",
    "train['target'] = train['target'].astype(int)\n",
    "fprint(train, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    19603\n",
       "1     5302\n",
       "2     3729\n",
       "3     2880\n",
       "4     2447\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the target distribution\n",
    "pd.value_counts(train['target']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.session_id_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = load_data('item_metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>properties</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5101</td>\n",
       "      <td>Satellite TV|Golf Course|Airport Shuttle|Cosme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5416</td>\n",
       "      <td>Satellite TV|Cosmetic Mirror|Safe (Hotel)|Tele...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id                                         properties\n",
       "0     5101  Satellite TV|Golf Course|Airport Shuttle|Cosme...\n",
       "1     5416  Satellite TV|Cosmetic Mirror|Safe (Hotel)|Tele..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['properties'] = meta_df['properties'].str.lower().str.split('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_properties = np.concatenate(meta_df['properties'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hostal (es)     3282\n",
       "camping site    2526\n",
       "szep kartya     1533\n",
       "kosher food     1248\n",
       "water slide      349\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(all_properties).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_properties = list(set(all_properties))\n",
    "property2natural = {v: k for k, v in enumerate(unique_properties)}\n",
    "del all_properties\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_properties = len(unique_properties)\n",
    "n_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['properties'] = meta_df['properties'].apply(lambda ps: [property2natural[p] for p in ps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['properties'] = meta_df['properties'].apply(lambda ps: np.sum(np.eye(n_properties, dtype=int)[ps], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_mapping = dict(meta_df[['item_id', 'properties']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a mapping for the padded values\n",
    "meta_mapping[0] = np.zeros(n_properties, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del meta_df, unique_properties, property2natural\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.15 s, sys: 140 ms, total: 2.29 s\n",
      "Wall time: 2.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train['impressions'] = (train['impressions']\n",
    "                        .apply(lambda imps: np.vstack([meta_mapping[i] \n",
    "                                                      if i in meta_mapping.keys()\n",
    "                                                      else np.zeros(n_properties, dtype=int) \n",
    "                                                      for i in imps])))\n",
    "del meta_mapping\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sort by price        3026\n",
       "focus on distance    2037\n",
       "hotel                1827\n",
       "5 star               1401\n",
       "best value           1371\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# current_filters\n",
    "all_cfs = np.concatenate(train['cfs'].dropna().values)\n",
    "pd.value_counts(all_cfs).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sort by price        0.133634\n",
       "focus on distance    0.223591\n",
       "hotel                0.304275\n",
       "5 star               0.366146\n",
       "best value           0.426691\n",
       "dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(all_cfs, normalize=True).cumsum().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_cfs = list(set(all_cfs))\n",
    "cfs_mapping = {v: k for k, v in enumerate(unique_cfs)}\n",
    "n_cfs = len(unique_cfs)\n",
    "n_cfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['cfs'].notna(), 'cfs'] = (train.loc[train['cfs'].notna(), 'cfs']\n",
    "                                          .apply(lambda cfs: [cfs_mapping[cf] for cf in cfs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 171 ms, sys: 3.97 ms, total: 175 ms\n",
      "Wall time: 174 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train['cfs'] = (train['cfs'].apply(lambda cfs: np.sum(np.eye(n_cfs, dtype=int)[cfs], axis=0) \n",
    "                                   if type(cfs) ==list else np.zeros(n_cfs, dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del cfs_mapping\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.session_id_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe normalize to percentage within each records, check does each item_id have the same price over all records\n",
    "def normalize(ps):\n",
    "    p_arr = np.array(ps)\n",
    "    return p_arr/(p_arr.max())\n",
    "\n",
    "train['prices'] = train['prices'].apply(normalize)\n",
    "# PRICES\n",
    "prices = np.array(list(train['prices'].values))\n",
    "del train['prices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPRESSIONS\n",
    "impressions = np.array(list(train['impressions'].values))\n",
    "del train['impressions']\n",
    "\n",
    "# CURRENT_FILTERS\n",
    "cfilters = np.array(list(train['cfs'].values))\n",
    "del train['cfs']\n",
    "\n",
    "# numerics\n",
    "num_cols = ['session_id_size', 'timestamp_dwell_time_prior_clickout', 'last_ref_ind']\n",
    "for c in num_cols:\n",
    "    train[c] = train[c].fillna(-1)\n",
    "    \n",
    "numerics = train[num_cols].values\n",
    "# train = train.drop(num_cols, axis=1)\n",
    "\n",
    "# TARGETS\n",
    "targets = train['target'].values\n",
    "del train['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impressions = impressions.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.session_id_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.hstack(impressions).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.isnan(impressions).sum()\n",
    "for i in impressions:\n",
    "    n = np.isneginf(i).sum()\n",
    "    if n!=0:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(numerics, impressions, prices, cfilters, targets,\n",
    "                        batch_size, shuffle=True):\n",
    "    # default we will shuffle\n",
    "    indices = np.arange(len(targets))\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        \n",
    "        remainder = len(targets) % batch_size\n",
    "        for start_idx in range(0, len(targets), batch_size):\n",
    "            if remainder !=0 and start_idx + batch_size >= len(targets):\n",
    "                excerpt = indices[len(targets)-batch_size:len(targets)]\n",
    "            else:\n",
    "                excerpt = indices[start_idx:start_idx+batch_size]\n",
    "\n",
    "            numerics_batch = numerics[excerpt]\n",
    "            impressions_batch = impressions[excerpt]#.reshape(batch_size, -1, 157)\n",
    "            prices_batch = prices[excerpt]#.reshape(batch_size, -1, 1)\n",
    "            cfilters_batch = cfilters[excerpt]\n",
    "            targets_batch = targets[excerpt]\n",
    "\n",
    "            prices_batch = np.array([i.reshape(-1, 1) for i in prices_batch])\n",
    "            yield ([numerics_batch, impressions_batch, prices_batch, \n",
    "                    cfilters_batch], targets_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # return [numerics_batch, impressions_batch, prices_batch[:, :, None], cfilters_batch]\n",
    "# train_gen = iterate_minibatches(trn_numerics, trn_imp, trn_price, trn_cfilter, y_trn, \n",
    "#                                 batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in train_gen:\n",
    "# #     i.reshape(-1, 1)\n",
    "# #     for j in i[0]:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_gen:\n",
    "#     for j in i[0]:\n",
    "# #         print(j[0].shape)\n",
    "#         print(j.shape, j[0].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_gen:\n",
    "#     pass\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in [trn_imp, trn_price, trn_cfilter, trn_city, trn_country, trn_plat, trn_dev]:\n",
    "#     print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trn_cfilter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in impressions:\n",
    "# #     print(i.shape)\n",
    "#     if i.shape[1]!=157 or i.shape[0]<2:\n",
    "#         print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHANGED ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~CHANGED ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~CHANGED ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~CHANGED ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~CHANGED ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~CHANGED ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~CHANGED ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~CHANGED ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~CHANGED ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~CHANGED ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "train len: 43,277 | val len: 8,668 | numer of parameters: 98,121 | train_len/nparams=0.44106\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected impression_input to have 3 dimensions, but got array with shape (128, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-208ab489d621>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m                                   \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                                   validation_steps=len(y_val)//batch_size)\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# make prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected impression_input to have 3 dimensions, but got array with shape (128, 1)"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from datetime import datetime as dt\n",
    "from nn_model_simple import build_model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "batch_size = 128\n",
    "n_epochs = 10\n",
    "\n",
    "skf = StratifiedKFold(n_splits=6)\n",
    "\n",
    "\n",
    "for trn_ind, val_ind in skf.split(targets, targets):\n",
    "    trn_numerics, val_numerics = numerics[trn_ind], numerics[val_ind]\n",
    "    trn_imp, val_imp = impressions[trn_ind], impressions[val_ind]\n",
    "    trn_price, val_price = prices[trn_ind], prices[val_ind]\n",
    "    trn_cfilter, val_cfilter = cfilters[trn_ind], cfilters[val_ind]\n",
    "    y_trn, y_val = targets[trn_ind], targets[val_ind]\n",
    "    \n",
    "    # create data generator numerics, impressions, prices, cfilters, targets, batchsize\n",
    "    # return [numerics_batch, impressions_batch, prices_batch[:, :, None], cfilters_batch]\n",
    "    train_gen = iterate_minibatches(trn_numerics, trn_imp, trn_price, trn_cfilter, y_trn, \n",
    "                                    batch_size, shuffle=True)\n",
    "    \n",
    "    val_gen = iterate_minibatches(val_numerics, val_imp, val_price, val_cfilter, y_val, \n",
    "                                  batch_size, shuffle=False)\n",
    "#     TEMP\n",
    "#     del impressions, prices, cities, platforms, devices\n",
    "#     gc.collect()\n",
    "    \n",
    "    # =====================================================================================\n",
    "    # create model\n",
    "    model = build_model(n_cfs, batch_size)\n",
    "    \n",
    "    # print out model info\n",
    "    nparams = model.count_params()\n",
    "    print((f'train len: {len(y_trn):,} | val len: {len(y_val):,} '\n",
    "           f'| numer of parameters: {nparams:,} | train_len/nparams={len(y_trn)/nparams:.5f}'))\n",
    "#     print(model.summary())\n",
    "#     plot_model(model, to_file='model.png')\n",
    "    # add some callbacks\n",
    "    callbacks = []\n",
    "    model_file = 'test.model'\n",
    "    callbacks = [ModelCheckpoint(model_file, save_best_only=True, verbose=1)]\n",
    "    log_dir = \"logs/{}\".format(dt.now().strftime('%m-%d-%H-%M'))\n",
    "    tb = TensorBoard(log_dir=log_dir, write_graph=True, write_grads=True)\n",
    "    callbacks.append(tb)\n",
    "    # simple early stopping\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', patience=100, verbose=1)\n",
    "    callbacks.append(es)\n",
    "    # rp\n",
    "    rp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=100, verbose=1)\n",
    "    callbacks.append(rp)\n",
    "    # add mrr callback\n",
    "#     callbacks.append(IntervalEvaluation())\n",
    "    \n",
    "    history = model.fit_generator(train_gen, \n",
    "                                  steps_per_epoch=len(y_trn)//batch_size, \n",
    "                                  epochs=n_epochs, \n",
    "                                  verbose=1,\n",
    "                                  callbacks=callbacks, \n",
    "                                  validation_data=val_gen, \n",
    "                                  validation_steps=len(y_val)//batch_size)\n",
    "\n",
    "    # make prediction\n",
    "#      [numerics_batch, impressions_batch, prices_batch[:, :, None], cfilters_batch]\n",
    "    trn_pred = model.predict(x=[trn_numerics, trn_imp, trn_price[:, :, None], trn_cfilter], batch_size=1024)\n",
    "    trn_pred_label = np.where(np.argsort(trn_pred)[:, ::-1] == y_trn.reshape(-1, 1))[1]\n",
    "    trn_mrr = np.mean(1/(trn_pred_label+1))\n",
    "\n",
    "    val_pred = model.predict(x=[val_numerics, val_imp, val_price[:, :, None], val_cfilter], batch_size=1024)\n",
    "    val_pred_label = np.where(np.argsort(val_pred)[:, ::-1] == y_val.reshape(-1, 1))[1]\n",
    "    val_mrr = np.mean(1/(val_pred_label+1))\n",
    "    print(f'train mrr: {trn_mrr:.2f} | val mrr: {val_mrr:.2f}')\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.7'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tcn\n",
    "tcn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(trn_pred_label, bins=50, label='train_pred', alpha=0.7)\n",
    "_ = plt.hist(y_trn, bins=50, label = 'train label', alpha=0.7)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(val_pred_label, bins=50, label='val_pred', alpha=0.7)\n",
    "_ = plt.hist(y_val, bins=50, label = 'val label', alpha=0.7)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train mrr: 0.47 | val mrr: 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add mrr at per epochs probably\n",
    "# look at no embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_pred, y_true, normalize='row', level=0, log_scale=False):\n",
    "    compare = pd.DataFrame({'prediction': y_pred, 'y_true': y_true})\n",
    "    counts = compare.groupby('y_true')['prediction'].value_counts()\n",
    "    mat = counts.unstack(level=0)\n",
    "    mat.fillna(0, inplace=True)\n",
    "    \n",
    "    if normalize == 'row':\n",
    "        row_sum = mat.sum(axis=1)\n",
    "        mat = mat.div(row_sum, axis=0)\n",
    "        log_scale = False\n",
    "    elif normalize == 'column':\n",
    "        col_sum = mat.sum(axis=0)\n",
    "        mat = mat.div(col_sum, axis=1)\n",
    "        log_scale = False\n",
    "    # plot\n",
    "    fig = plt.figure(figsize=(35,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    if log_scale:\n",
    "        cax = ax.matshow(np.log1p(mat), interpolation='nearest')#, cmap='coolwarm')#, aspect='auto')\n",
    "    else:\n",
    "        cax = ax.matshow(mat, interpolation='nearest')#, cmap='coolwarm')#, aspect='auto')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xlabel(f'{mat.columns.name}')\n",
    "    ax.xaxis.set_label_position('top') \n",
    "    ax.set_ylabel(f'{mat.index.name}')\n",
    "    \n",
    "    ax.set_xticks(np.arange(mat.shape[1]))\n",
    "    ax.set_xticklabels(list(mat.columns.astype(str)), rotation=90)\n",
    "    ax.set_yticks(np.arange(mat.shape[0]))\n",
    "    _ = ax.set_yticklabels(list(mat.index.astype(str)))\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(trn_pred_label, y_trn, normalize=False, level=0, log_scale=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(trn_pred_label, y_trn, normalize='row')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(trn_pred_label, y_trn, normalize='column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(val_pred_label, y_val, level=0, normalize=None, log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
