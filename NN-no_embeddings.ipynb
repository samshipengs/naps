{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime, time, os, gc, re, sys\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import ignore_warnings, load_data\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[05-04 08:00:33 - utils - load_data - INFO] Loading train using 5,000,000 rows which is 0.31% out of total train data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw train shape: (5,000,000, 12)\n",
      "train after filtering shape: (496,444, 12)\n"
     ]
    }
   ],
   "source": [
    "train = load_data('train', nrows=5000000)\n",
    "def fprint(df, name):\n",
    "    print(f'{name} shape: ({df.shape[0]:,}, {df.shape[1]})')\n",
    "    \n",
    "fprint(train, 'raw train')\n",
    "train = (train[(train['action_type'] == 'clickout item') & \n",
    "               (train['impressions'].notna()) & \n",
    "               (train['reference'].notna())]\n",
    "         .reset_index(drop=True))\n",
    "\n",
    "fprint(train, 'train after filtering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # also get test\n",
    "# test = load_data('test')#, nrows=1000000)\n",
    "# fprint(test, 'raw test')\n",
    "# test = test[(test.action_type == 'clickout item') & (test.impressions.notna()) & (test.reference.notna())].reset_index(drop=True)\n",
    "# fprint(test, 'test after')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.concat([train, test], ignore_index=True)\n",
    "# del test\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping duplicates shape: (494,139, 12)\n"
     ]
    }
   ],
   "source": [
    "# drop duplciates of rows with all same info except step\n",
    "cols = [c for c in train.columns if c != 'step']\n",
    "train = train.drop_duplicates(subset=cols, keep='last').reset_index(drop=True)\n",
    "fprint(train, 'after dropping duplicates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# def session_duration(x):\n",
    "#     return x.max() - x.min()\n",
    "# # session_fts = train.groupby('session_id').agg({'session_id': 'size', 'timestamp': session_duration})\n",
    "# session_fts = train.groupby('session_id').size().reset_index(name='session_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from clean_session import preprocess_sessions\n",
    "# # train = preprocess_sessions(train,data_source='data')\n",
    "# train = preprocess_sessions(None,data_source='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# train_last = train.groupby('session_id').last().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_last = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_last = pd.merge(train_last, session_fts, on='session_id')\n",
    "# fprint(train_last, 'train_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fprint(train_last, 'train_last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "London, United Kingdom    0.017784\n",
       "Tokyo, Japan              0.032007\n",
       "Paris, France             0.044649\n",
       "New York, USA             0.056122\n",
       "Istanbul, Turkey          0.065795\n",
       "Name: city, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get only common cities\n",
    "city_counts = train['city'].value_counts()\n",
    "city_counts_cs = city_counts.cumsum()/(city_counts.sum())\n",
    "city_counts_cs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (444,716, 12)\n"
     ]
    }
   ],
   "source": [
    "# set the threshold we select cities\n",
    "th = 0.9\n",
    "above_th = city_counts_cs[city_counts_cs<th]\n",
    "common_cities = above_th.index.values\n",
    "train = train[train['city'].isin(common_cities)].reset_index(drop=True)\n",
    "fprint(train, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# create a label see if it contains any sortings\n",
    "train['no_reorder'] = train['current_filters'].str.contains(r'\\b(sort|focus)', case=False)\n",
    "train['no_reorder'].fillna(False, inplace=True)\n",
    "# train['current_filters'].dropna()[train['current_filters'].dropna().str.contains(r'\\bsort', case=False)].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current filters\n",
    "train['cfs'] = train['current_filters'].str.lower().str.split('|')\n",
    "train['ncfs'] = train['cfs'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add country infomation\n",
    "train['country'] = train.city.str.split(',').str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add prices split now after shrink (otherwise prices being list cannot get shrinked)\n",
    "# prices\n",
    "train['prices'] = train.prices.str.split('|')\n",
    "train['prices'] = train.prices.apply(lambda x: [int(p) for p in x])\n",
    "# pad it\n",
    "train['prices'] = train.prices.apply(lambda x: np.pad(x, (0, 25-len(x)), mode='constant'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of impressions\n",
    "train['nimps'] = train.impressions.str.split('|').str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.73 s, sys: 361 ms, total: 9.09 s\n",
      "Wall time: 8.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# impressions\n",
    "train['impressions'] = train['impressions'].str.split('|')\n",
    "# convert impression id to int\n",
    "train['impressions'] = train['impressions'].apply(lambda x: [int(i) for i in x])\n",
    "train['reference'] = train['reference'].astype(int)\n",
    "# pad to 25 len\n",
    "train['impressions'] = train['impressions'].apply(lambda x: np.pad(x, (0, 25-len(x)), mode='constant'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "459928"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_item_ids = len(set(np.concatenate(train['impressions'].values)))\n",
    "n_item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (444,716, 18)\n"
     ]
    }
   ],
   "source": [
    "# filter out nan rows with reference_id not in impressions list, since if the true target in test\n",
    "# is not in the impression list then it would not get evaluated\n",
    "def assign_target(row):\n",
    "    ref = row['reference']\n",
    "    imp = list(row['impressions'])\n",
    "    if ref in imp:\n",
    "        return imp.index(ref)\n",
    "    else:\n",
    "        return np.nan\n",
    "train['target'] = train.apply(assign_target, axis=1)\n",
    "fprint(train, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop number of rows reference id not in impressions list: 244\n",
      "train shape: (444,472, 18)\n"
     ]
    }
   ],
   "source": [
    "print(f'drop number of rows reference id not in impressions list: {train.target.isna().sum()}')\n",
    "# drop the ones whose reference is not in the impression list\n",
    "train = train[train['target'].notna()].reset_index(drop=True)\n",
    "fprint(train, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    138404\n",
       "1.0     46348\n",
       "2.0     33107\n",
       "3.0     26737\n",
       "4.0     22873\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the target distribution\n",
    "pd.value_counts(train['target']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = load_data('item_metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>properties</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5101</td>\n",
       "      <td>Satellite TV|Golf Course|Airport Shuttle|Cosme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5416</td>\n",
       "      <td>Satellite TV|Cosmetic Mirror|Safe (Hotel)|Tele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5834</td>\n",
       "      <td>Satellite TV|Cosmetic Mirror|Safe (Hotel)|Tele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5910</td>\n",
       "      <td>Satellite TV|Sailing|Cosmetic Mirror|Telephone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6066</td>\n",
       "      <td>Satellite TV|Sailing|Diving|Cosmetic Mirror|Sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id                                         properties\n",
       "0     5101  Satellite TV|Golf Course|Airport Shuttle|Cosme...\n",
       "1     5416  Satellite TV|Cosmetic Mirror|Safe (Hotel)|Tele...\n",
       "2     5834  Satellite TV|Cosmetic Mirror|Safe (Hotel)|Tele...\n",
       "3     5910  Satellite TV|Sailing|Cosmetic Mirror|Telephone...\n",
       "4     6066  Satellite TV|Sailing|Diving|Cosmetic Mirror|Sa..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['properties'] = meta_df['properties'].str.lower().str.split('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_properties = np.concatenate(meta_df['properties'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = pd.value_counts(all_properties).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hostal (es)     3282\n",
       "camping site    2526\n",
       "szep kartya     1533\n",
       "kosher food     1248\n",
       "water slide      349\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(all_properties).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_properties = list(set(all_properties))\n",
    "property2natural = {v: k for k, v in enumerate(unique_properties)}\n",
    "del all_properties\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_properties = len(unique_properties)\n",
    "n_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['properties'] = meta_df['properties'].apply(lambda ps: [property2natural[p] for p in ps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['properties'] = meta_df['properties'].apply(lambda ps: np.sum(np.eye(n_properties, dtype=int)[ps], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_mapping = dict(meta_df[['item_id', 'properties']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a mapping for the padded values\n",
    "# meta_mapping[0] = np.zeros(n_properties, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del meta_df, unique_properties, property2natural\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.6 s, sys: 2.33 s, total: 24 s\n",
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train['impressions'] = (train['impressions']\n",
    "                        .apply(lambda imps: np.vstack([meta_mapping[i] \n",
    "                                                      if i in meta_mapping.keys()\n",
    "                                                      else np.zeros(n_properties, dtype=int) \n",
    "                                                      for i in imps])))\n",
    "del meta_mapping\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sort by price        32722\n",
       "focus on distance    22641\n",
       "hotel                18475\n",
       "4 star               14174\n",
       "5 star               13873\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# current_filters\n",
    "all_cfs = np.concatenate(train['cfs'].dropna().values)\n",
    "pd.value_counts(all_cfs).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sort by price        0.142395\n",
       "focus on distance    0.240921\n",
       "hotel                0.321318\n",
       "4 star               0.382999\n",
       "5 star               0.443370\n",
       "dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(all_cfs, normalize=True).cumsum().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_cfs = list(set(all_cfs))\n",
    "cfs_mapping = {v: k for k, v in enumerate(unique_cfs)}\n",
    "n_cfs = len(unique_cfs)\n",
    "n_cfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['cfs'].notna(), 'cfs'] = (train.loc[train['cfs'].notna(), 'cfs']\n",
    "                                          .apply(lambda cfs: [cfs_mapping[cf] for cf in cfs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.83 s, sys: 87.9 ms, total: 1.92 s\n",
      "Wall time: 1.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train['cfs'] = (train['cfs'].apply(lambda cfs: np.sum(np.eye(n_cfs, dtype=int)[cfs], axis=0) \n",
    "                                   if type(cfs) ==list else np.zeros(n_cfs, dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del cfs_mapping\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting city\n",
      "converting platform\n",
      "converting device\n",
      "converting country\n"
     ]
    }
   ],
   "source": [
    "# encode city, platform and device\n",
    "def categorize(df, cols):\n",
    "    for col in cols:\n",
    "        print('converting', col)\n",
    "        unique_values = df[col].unique()\n",
    "        mapping = {v: k for k, v in enumerate(unique_values)}\n",
    "        df[col] = df[col].map(mapping)\n",
    "        \n",
    "categorize(train, ['city', 'platform', 'device', 'country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['device'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train, columns=['device'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # look at price distribution\n",
    "# all_prices = np.concatenate(train.prices.values)\n",
    "# _= plt.hist(all_prices, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.max(all_prices), np.min(all_prices[all_prices!=0]), np.mean(all_prices), np.median(all_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _= plt.hist(np.log1p(all_prices), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = np.mean(np.log1p(all_prices))\n",
    "# sd = np.std(np.log1p(all_prices))\n",
    "# _ = plt.hist((np.log1p(all_prices)-m)/sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe normalize to percentage within each records, check does each item_id have the same price over all records\n",
    "def normalize(ps):\n",
    "    p_arr = np.array(ps)\n",
    "    return p_arr/(p_arr.max())\n",
    "\n",
    "train['prices'] = train['prices'].apply(normalize)\n",
    "prices = np.array(list(train['prices'].values))\n",
    "del train['prices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # IMPRESSIONS\n",
    "# impressions = np.array(list(train['impressions'].values))\n",
    "# # map each item_id to integer value\n",
    "# # impressions = np.array([[imps_mapping[j] for j in i] for i in impressions])\n",
    "# del train['impressions']\n",
    "# CURRENT_FILTERS\n",
    "cfilters = np.array(list(train['cfs'].values))\n",
    "del train['cfs']\n",
    "# record no_reorder info\n",
    "no_reorders = train['no_reorder'].values\n",
    "del train['no_reorder']\n",
    "# CITY\n",
    "cities = train['city'].values\n",
    "n_city = train['city'].nunique()\n",
    "del train['city']\n",
    "# COUNTRY\n",
    "countries = train['country'].values\n",
    "n_country = train['country'].nunique()\n",
    "del train['country']\n",
    "# PLATFORM\n",
    "platforms = train['platform'].values\n",
    "n_plat = train['platform'].nunique()\n",
    "del train['platform']\n",
    "# DEVICES\n",
    "devices = train[['device_1', 'device_2']].values\n",
    "del train['device_1'], train['device_2']\n",
    "\n",
    "# SESSION_ID\n",
    "sids = train['session_id'].values\n",
    "del train['session_id']\n",
    "# TARGETS\n",
    "targets = train['target'].values\n",
    "del train['target']\n",
    "\n",
    "# IMPRESSIONS\n",
    "impressions = np.array(list(train['impressions'].values))\n",
    "del train['impressions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(no_reorders, impressions, prices, cfilters, cities, countries, platforms, devices, \n",
    "                        batchsize, targets, shuffle=True, reorder=True):\n",
    "    # default we will shuffle\n",
    "    indices = np.arange(len(targets))\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        for start_idx in range(0, len(targets), batchsize):\n",
    "            excerpt = indices[start_idx:start_idx+batchsize]\n",
    "            nos = ~no_reorders[excerpt]\n",
    "#             print(f'allowed reorder:{nos.sum()}')\n",
    "            imps = impressions[excerpt]\n",
    "            ps = prices[excerpt]\n",
    "            cfs = cfilters[excerpt]\n",
    "            cis = cities[excerpt]\n",
    "            cos = countries[excerpt]\n",
    "            plats = platforms[excerpt]\n",
    "            ds = devices[excerpt]\n",
    "            ys = targets[excerpt]\n",
    "            \n",
    "            if reorder:\n",
    "                re_imps = imps[nos]\n",
    "                re_ps = ps[nos]\n",
    "                re_cfs = cfs[nos]\n",
    "                re_cis = cis[nos]\n",
    "                re_cos = cos[nos]\n",
    "                re_plats = plats[nos]\n",
    "                re_ds = ds[nos]\n",
    "                re_ys = ys[nos]\n",
    "                # randomly shuffle the order of the impressions and prices\n",
    "                reorder_ind = ([np.random.choice(np.arange(25, dtype='int'), 25, replace=False) \n",
    "                                for _ in range(len(re_imps))])\n",
    "                reorder_imp = re_imps[np.arange(re_imps.shape[0])[:, None], reorder_ind]\n",
    "                reorder_price = re_ps[np.arange(re_ps.shape[0])[:, None], reorder_ind]\n",
    "                reorder_ys = re_ys[np.arange(re_ys.shape[0])[:, None], reorder_ind]\n",
    "        \n",
    "                # concatenate them back\n",
    "                yield ([np.vstack((imps, reorder_imp)),\n",
    "                        np.vstack((ps, reorder_price))[:, :, None],\n",
    "                        np.vstack((cfs, re_cfs)),\n",
    "                        np.hstack((cis, re_cis)), \n",
    "                        np.hstack((cos, re_cos)), \n",
    "                        np.hstack((plats, re_plats)),\n",
    "                        np.vstack((ds, re_ds))],\n",
    "                       np.vstack((ys, reorder_ys)))\n",
    "            else:\n",
    "                yield ([imps, ps[:, :, None], cfs, cis, cos, plats, ds], ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_auc_score\n",
    "# from keras.callbacks import Callback\n",
    "\n",
    "# class Mrr(Callback):\n",
    "#     def __init__(self, validation_data=validation_data, interval=2):\n",
    "#         super(Callback, self).__init__()\n",
    "#         self.interval = interval\n",
    "#         self.X_val, self.y_val = validation_data\n",
    "#         self.y_val = y_val\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         if epoch % self.interval == 0:\n",
    "#             y_pred = self.model.predict_generator(self.val_gen, verbose=0)\n",
    "#             val_mrr = np.mean(1/(np.where(np.argsort(y_pred)[:, ::-1] == y_val[val_pred.shape[0]].reshape(-1, 1))[1]+1))\n",
    "#             print(\"interval evaluation - epoch: {:d} - score: {:.6f}\".format(epoch, val_mrr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nemd(n):\n",
    "    return int(n**(1/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(459928, 26)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_item_ids, nemd(n_item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4419, 143)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_city, n_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_cfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nemd(n_city), nemd(n_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nemd(n_plat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create data generator [imps, ps, cis, cos, plats, ds], ys\n",
    "# train_gen = iterate_minibatches(trn_no, trn_imp, trn_price, trn_cfilter, trn_city, trn_country, \n",
    "#                                 trn_plat, trn_dev, batch_size, train_y_binary, shuffle=True, reorder=True)\n",
    "\n",
    "# val_gen = iterate_minibatches(val_no, val_imp, val_price, val_cfilter, val_city, val_country, \n",
    "#                               val_plat, val_dev, batch_size, val_y_binary, shuffle=False, reorder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_gen:\n",
    "#     for j in i[0]:\n",
    "#         print(j.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in val_gen:\n",
    "#     for j in i[0]:\n",
    "#         print(j.shape)\n",
    "#     print('===', i[1].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in [trn_imp, trn_price, trn_cfilter, trn_city, trn_country, trn_plat, trn_dev]:\n",
    "#     print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'city': 4419, 'country': 143, 'cfilter': 149, 'platform': 55}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_uniques = {'city': n_city, 'country': n_country, 'cfilter': n_cfs, 'platform': n_plat}\n",
    "n_uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len: 370,383 | val len: 74,089 | numer of parameters: 70,481 | train_len/nparams=5.25508\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "impression_input (InputLayer)   (None, 25, 157)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 21, 16)       12576       impression_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 21, 16)       64          conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 10, 16)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "price_input (InputLayer)        (None, 25, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 10, 16)       0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 21, 8)        48          price_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 6, 32)        2592        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 21, 8)        32          conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 6, 32)        128         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 10, 8)        0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 3, 32)        0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 6, 16)        656         max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 3, 32)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 6, 16)        64          conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 96)           0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 3, 16)        0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "cfilter_input (InputLayer)      (None, 149)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 96)           0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 48)           0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 30)           4500        cfilter_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 174)          0           dropout_3[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "city_input (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "country_input (InputLayer)      (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "platform (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 174)          696         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 9)         39771       city_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 4)         572         country_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 3)         165         platform[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 30)           5250        batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 1, 9)         0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 1, 4)         0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_3 (SpatialDro (None, 1, 3)         0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 30)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 9)            0           spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 4)            0           spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 3)            0           spatial_dropout1d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "device_input (InputLayer)       (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 48)           0           dropout_4[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 device_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 48)           192         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 48)           0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 30)           1470        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 30)           930         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 25)           775         dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 70,481\n",
      "Trainable params: 69,893\n",
      "Non-trainable params: 588\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2893/2893 [==============================] - 44s 15ms/step - loss: 2.9775 - acc: 0.1848 - val_loss: 2.6585 - val_acc: 0.3118\n",
      "Epoch 2/10\n",
      "2893/2893 [==============================] - 39s 13ms/step - loss: 2.9401 - acc: 0.1864 - val_loss: 2.6922 - val_acc: 0.3119\n",
      "Epoch 3/10\n",
      "2893/2893 [==============================] - 39s 13ms/step - loss: 2.9291 - acc: 0.1864 - val_loss: 2.6745 - val_acc: 0.3107\n",
      "Epoch 4/10\n",
      "2893/2893 [==============================] - 39s 13ms/step - loss: 2.9210 - acc: 0.1865 - val_loss: 2.6496 - val_acc: 0.3107\n",
      "Epoch 5/10\n",
      "2893/2893 [==============================] - 39s 13ms/step - loss: 2.9160 - acc: 0.1871 - val_loss: 2.6281 - val_acc: 0.3108\n",
      "Epoch 6/10\n",
      "2893/2893 [==============================] - 39s 13ms/step - loss: 2.9131 - acc: 0.1869 - val_loss: 2.6554 - val_acc: 0.3101\n",
      "Epoch 7/10\n",
      "2893/2893 [==============================] - 39s 13ms/step - loss: 2.9095 - acc: 0.1870 - val_loss: 2.6476 - val_acc: 0.3110\n",
      "Epoch 8/10\n",
      "2893/2893 [==============================] - 39s 13ms/step - loss: 2.9065 - acc: 0.1873 - val_loss: 2.6251 - val_acc: 0.3105\n",
      "Epoch 9/10\n",
      "2893/2893 [==============================] - 39s 13ms/step - loss: 2.9042 - acc: 0.1874 - val_loss: 2.6498 - val_acc: 0.3096\n",
      "Epoch 10/10\n",
      "2893/2893 [==============================] - 39s 14ms/step - loss: 2.9026 - acc: 0.1877 - val_loss: 2.6581 - val_acc: 0.3101\n",
      "train mrr: 0.45 | val mrr: 0.45\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from datetime import datetime as dt\n",
    "from nn_model import build_model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "batch_size = 128\n",
    "# n_repeat = 2\n",
    "n_epochs = 300\n",
    "\n",
    "skf = StratifiedKFold(n_splits=6)\n",
    "\n",
    "for trn_ind, val_ind in skf.split(targets, targets):\n",
    "    trn_no, val_no = no_reorders[trn_ind], no_reorders[val_ind]\n",
    "    trn_imp, val_imp = impressions[trn_ind], impressions[val_ind]\n",
    "    trn_price, val_price = prices[trn_ind], prices[val_ind]\n",
    "    trn_cfilter, val_cfilter = cfilters[trn_ind], cfilters[val_ind]\n",
    "    trn_city, val_city = cities[trn_ind], cities[val_ind]\n",
    "    trn_country, val_country = countries[trn_ind], countries[val_ind]\n",
    "    trn_plat, val_plat = platforms[trn_ind], platforms[val_ind]\n",
    "    trn_dev, val_dev = devices[trn_ind], devices[val_ind]\n",
    "    \n",
    "    y_trn, y_val = targets[trn_ind], targets[val_ind]\n",
    "    # keras requires 0, 1 binary label input\n",
    "    from keras.utils import to_categorical\n",
    "    train_y_binary = to_categorical(y_trn)\n",
    "    val_y_binary = to_categorical(y_val)\n",
    "    \n",
    "    \n",
    "    # create data generator [imps, ps, cis, cos, plats, ds], ys\n",
    "    train_gen = iterate_minibatches(trn_no, trn_imp, trn_price, trn_cfilter, trn_city, trn_country, \n",
    "                                    trn_plat, trn_dev, batch_size, train_y_binary, shuffle=True, reorder=True)\n",
    "    \n",
    "    val_gen = iterate_minibatches(val_no, val_imp, val_price, val_cfilter, val_city, val_country, \n",
    "                                  val_plat, val_dev, batch_size, val_y_binary, shuffle=False, reorder=False)\n",
    "#     TEMP\n",
    "    del impressions, prices, cities, platforms, devices\n",
    "    gc.collect()\n",
    "    \n",
    "    # =====================================================================================\n",
    "    # create model\n",
    "    model = build_model(n_uniques, conv1d_filter_size=5)\n",
    "    \n",
    "    # print out model info\n",
    "    nparams = model.count_params()\n",
    "    print((f'train len: {len(y_trn):,} | val len: {len(y_val):,} '\n",
    "           f'| numer of parameters: {nparams:,} | train_len/nparams={len(y_trn)/nparams:.5f}'))\n",
    "    print(model.summary())\n",
    "#     plot_model(model, to_file='model.png')\n",
    "    # add some callbacks\n",
    "    callbacks = []\n",
    "    model_file = 'test.model'\n",
    "    callbacks = [ModelCheckpoint(model_file, save_best_only=True, verbose=1)]\n",
    "    log_dir = \"logs/{}\".format(dt.now().strftime('%m-%d-%H-%M'))\n",
    "    tb = TensorBoard(log_dir=log_dir, write_graph=True, write_grads=True)\n",
    "    callbacks.append(tb)\n",
    "    # add mrr callback\n",
    "#     callbacks.append(IntervalEvaluation())\n",
    "    \n",
    "    history = model.fit_generator(train_gen, \n",
    "                                  steps_per_epoch=len(y_trn)//batch_size, \n",
    "                                  epochs=10, \n",
    "                                  verbose=1,\n",
    "                                  callbacks=callbacks, \n",
    "                                  validation_data=val_gen, \n",
    "                                  validation_steps=len(y_val)//batch_size)\n",
    "\n",
    "    # make prediction\n",
    "    trn_pred = model.predict(x=[trn_imp, trn_price[:, :, None], trn_cfilter, \n",
    "                                trn_city, trn_country, trn_plat, trn_dev], \n",
    "                             batch_size=1024)\n",
    "    trn_pred_label = np.where(np.argsort(trn_pred)[:, ::-1] == y_trn.reshape(-1, 1))[1]+1\n",
    "    trn_mrr = np.mean(1/trn_pred_label)\n",
    "\n",
    "    val_pred = model.predict(x=[val_imp, val_price[:, :, None], val_cfilter, \n",
    "                                val_city, val_country, val_plat, val_dev], \n",
    "                             batch_size=1024)\n",
    "    val_pred_label = np.where(np.argsort(val_pred)[:, ::-1] == y_val.reshape(-1, 1))[1]+1\n",
    "    val_mrr = np.mean(1/val_pred_label)\n",
    "    print(f'train mrr: {trn_mrr:.2f} | val mrr: {val_mrr:.2f}')\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction\n",
    "trn_pred = model.predict(x=[trn_imp, trn_price[:, :, None], trn_cfilter, \n",
    "                            trn_city, trn_country, trn_plat, trn_dev], \n",
    "                         batch_size=1024)\n",
    "trn_pred_label = np.where(np.argsort(trn_pred)[:, ::-1] == y_trn.reshape(-1, 1))[1]+1\n",
    "trn_mrr = np.mean(1/trn_pred_label)\n",
    "\n",
    "val_pred = model.predict(x=[val_imp, val_price[:, :, None], val_cfilter, \n",
    "                            val_city, val_country, val_plat, val_dev], \n",
    "                         batch_size=1024)\n",
    "val_pred_label = np.where(np.argsort(val_pred)[:, ::-1] == y_val.reshape(-1, 1))[1]+1\n",
    "val_mrr = np.mean(1/val_pred_label)\n",
    "print(f'train mrr: {trn_mrr:.2f} | val mrr: {val_mrr:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(trn_pred_label, bins=50, label='train_pred', alpha=0.7)\n",
    "_ = plt.hist(y_trn+1, bins=50, label = 'train label', alpha=0.7)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(val_pred_label, bins=50, label='val_pred', alpha=0.7)\n",
    "_ = plt.hist(y_val+1, bins=50, label = 'val label', alpha=0.7)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train mrr: 0.47 | val mrr: 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add mrr at per epochs probably\n",
    "# look at no embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
